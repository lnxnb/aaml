{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc02c0a",
   "metadata": {},
   "source": [
    "## Coding problem: \n",
    "\n",
    "Implement a gradient descent method for Ridge Regression by using the PyTorch library. Your implementation should be a class that has the required methods “.fit” and “.predict”. You should include an application of your code to a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7c5681e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0366381a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8732/639760672.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.cat((ones_column, torch.tensor(self.X)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, the gradient is [[-46.891836719387754, -19.052142880612237, -11.927325599707315, -15.02158424758712]] \n",
      " the MSE is 610.4738251295921, and the loss function is 610.4738251295921\n",
      "At iteration 100, the gradient is [[-3.2701493142622247, 2.4959221730476187, 2.2975709819214107, 1.8475508740965552]] \n",
      " the MSE is 63.148331680965676, and the loss function is 63.153079446202824\n",
      "At iteration 200, the gradient is [[-1.7679976402980992, 1.330005324460747, 1.2709145049220247, 1.1177791322376072]] \n",
      " the MSE is 33.17090350017052, and the loss function is 33.17836665652273\n",
      "At iteration 300, the gradient is [[-0.9750708985105735, 0.6735153218173656, 0.6945420057932299, 0.7003622420653045]] \n",
      " the MSE is 24.05559335849761, and the loss function is 24.06504525245937\n",
      "At iteration 400, the gradient is [[-0.5397116029800763, 0.31502192677335267, 0.37788260542397367, 0.4686398536413705]] \n",
      " the MSE is 21.232529732328388, and the loss function is 21.243232037601146\n",
      "At iteration 500, the gradient is [[-0.3006037184912177, 0.12008298376999593, 0.20382676873357358, 0.3389561433355248]] \n",
      " the MSE is 20.310580279728146, and the loss function is 20.32202505279625\n",
      "At iteration 600, the gradient is [[-0.16923470005415678, 0.01486990245127866, 0.10806690870370805, 0.2653642338877923]] \n",
      " the MSE is 19.965995415829894, and the loss function is 19.977873753612247\n",
      "At iteration 700, the gradient is [[-0.09701390943466473, -0.04114500803080042, 0.05529975109342625, 0.22263834540760652]] \n",
      " the MSE is 19.79950793952037, and the loss function is 19.81164198288583\n",
      "At iteration 800, the gradient is [[-0.05726652192257556, -0.07020831345486707, 0.026144517853230277, 0.1969280326461159]] \n",
      " the MSE is 19.69050704867539, and the loss function is 19.702797353171984\n",
      "At iteration 900, the gradient is [[-0.03534880954728004, -0.08453087441718667, 0.009961394746504473, 0.18062762381967384]] \n",
      " the MSE is 19.602392484299475, and the loss function is 19.614784523628455\n",
      "At iteration 1000, the gradient is [[-0.0232217416649118, -0.0908143213682414, 0.0009090949191983095, 0.16955970118735592]] \n",
      " the MSE is 19.523905879065985, and the loss function is 19.53637027499349\n",
      "At iteration 1100, the gradient is [[-0.01647206183249524, -0.09273594560209991, -0.004219415421698037, 0.1614289180670977]] \n",
      " the MSE is 19.45144473660104, and the loss function is 19.46396599518794\n",
      "At iteration 1200, the gradient is [[-0.012676952384612705, -0.09231282470775745, -0.007185036102435689, 0.15497319077325827]] \n",
      " the MSE is 19.383729230871268, and the loss function is 19.39629944062078\n",
      "At iteration 1300, the gradient is [[-0.010506286511007677, -0.09065068295508937, -0.008954773468028473, 0.14949772757486457]] \n",
      " the MSE is 19.320192865769695, and the loss function is 19.33280820741045\n",
      "At iteration 1400, the gradient is [[-0.009229792736107077, -0.08835499051539221, -0.010059800311153546, 0.14461922323211093]] \n",
      " the MSE is 19.260496023709273, and the loss function is 19.273154849565294\n",
      "At iteration 1500, the gradient is [[-0.008446503605877526, -0.08575664244928, -0.010792019892418868, 0.1401254279048112]] \n",
      " the MSE is 19.204378286571067, and the loss function is 19.217080069127654\n",
      "At iteration 1600, the gradient is [[-0.007936231434467431, -0.08303584785432523, -0.011311973762301211, 0.13589805303153135]] \n",
      " the MSE is 19.15161307258545, and the loss function is 19.16435783219029\n",
      "At iteration 1700, the gradient is [[-0.007578033079965008, -0.08029013963175108, -0.011708080100363684, 0.1318704472774011]] \n",
      " the MSE is 19.10199324572011, and the loss function is 19.114781241630016\n",
      "At iteration 1800, the gradient is [[-0.007305449598051947, -0.07757170827309391, -0.012029154024902771, 0.12800436021529413]] \n",
      " the MSE is 19.055326150266875, and the loss function is 19.068157716096845\n",
      "At iteration 1900, the gradient is [[-0.007081934152046643, -0.07490789554166057, -0.012302260532398622, 0.12427718462863079]] \n",
      " the MSE is 19.011431550350295, and the loss function is 19.024307008500788\n",
      "At iteration 2000, the gradient is [[-0.006887362644195612, -0.07231244350842403, -0.012542515500927866, 0.12067495130187927]] \n",
      " the MSE is 18.97014049384446, and the loss function is 18.98306011296857\n",
      "At iteration 2100, the gradient is [[-0.00671062841529763, -0.06979166861023185, -0.012758466423820168, 0.1171884817977228]] \n",
      " the MSE is 18.93129449447698, and the loss function is 18.944258470266657\n",
      "At iteration 2200, the gradient is [[-0.006545576909065258, -0.06734784974583365, -0.012955046544687546, 0.11381127491955231]] \n",
      " the MSE is 18.8947448434982, and the loss function is 18.90775329197997\n",
      "At iteration 2300, the gradient is [[-0.006388773873737296, -0.0649810870058803, -0.013135196856884194, 0.11053834496121033]] \n",
      " the MSE is 18.860351990078026, and the loss function is 18.87340494750217\n",
      "At iteration 2400, the gradient is [[-0.006238280120240143, -0.06269032086780141, -0.013300756793749806, 0.10736558250465689]] \n",
      " the MSE is 18.827984969209915, and the loss function is 18.841082395303218\n",
      "At iteration 2500, the gradient is [[-0.006092978849993988, -0.06047389054979561, -0.013452953444671336, 0.10428940212529685]] \n",
      " the MSE is 18.79752086849196, and the loss function is 18.81066265131776\n",
      "At iteration 2600, the gradient is [[-0.005952206327497149, -0.05832983941374443, -0.013592670366170671, 0.10130654764456239]] \n",
      " the MSE is 18.76884432934686, and the loss function is 18.78203029082413\n",
      "At iteration 2700, the gradient is [[-0.005815549081758987, -0.05625608154139493, -0.013720595389897361, 0.09841398391320957]] \n",
      " the MSE is 18.74184707976209, and the loss function is 18.7550769813379\n",
      "At iteration 2800, the gradient is [[-0.005682732527913223, -0.05425049213280505, -0.013837301995907464, 0.09560883613860247]] \n",
      " the MSE is 18.71642749626571, and the loss function is 18.729701044477316\n",
      "At iteration 2900, the gradient is [[-0.005553559777475117, -0.052310956118831436, -0.013943294208165976, 0.09288835535176432]] \n",
      " the MSE is 18.692490193173985, and the loss function is 18.70580704496887\n",
      "At iteration 3000, the gradient is [[-0.005427878001967489, -0.05043539386622454, -0.014039031457611444, 0.09024989826304142]] \n",
      " the MSE is 18.669945637346665, and the loss function is 18.683305405100405\n",
      "At iteration 3100, the gradient is [[-0.0053055599238269885, -0.04862177433837138, -0.014124942441251395, 0.0876909150538837]] \n",
      " the MSE is 18.6487097868313, and the loss function is 18.66211204304164\n",
      "At iteration 3200, the gradient is [[-0.00518649361309868, -0.046868121399438946, -0.014201432933624962, 0.08520894156157884]] \n",
      " the MSE is 18.6287037518962, and the loss function is 18.642148033552157\n",
      "At iteration 3300, the gradient is [[-0.005070576845239575, -0.045172516383599724, -0.014268890271894178, 0.08280159391050831]] \n",
      " the MSE is 18.60985347705423, and the loss function is 18.62333928968957\n",
      "At iteration 3400, the gradient is [[-0.004957713963860903, -0.04353309864193974, -0.014327686008541073, 0.08046656452045436]] \n",
      " the MSE is 18.592089442771005, and the loss function is 18.60561626421654\n",
      "At iteration 3500, the gradient is [[-0.004847814119990127, -0.04194806500643239, -0.014378177552063791, 0.07820161890371584]] \n",
      " the MSE is 18.575346385634823, and the loss function is 18.58891366948617\n",
      "At iteration 3600, the gradient is [[-0.004740790268226662, -0.04041566868583492, -0.014420709246270584, 0.07600459292701438]] \n",
      " the MSE is 18.559563035842864, and the loss function is 18.573170214661033\n",
      "At iteration 3700, the gradient is [[-0.00463655857955974, -0.03893421787520804, -0.014455613135624547, 0.07387339035945106]] \n",
      " the MSE is 18.544681870929892, and the loss function is 18.55832835919179\n",
      "At iteration 3800, the gradient is [[-0.0045350380838633094, -0.03750207423293904, -0.014483209552716387, 0.07180598060733925]] \n",
      " the MSE is 18.53064888473288, and the loss function is 18.544334081548296\n",
      "At iteration 3900, the gradient is [[-0.004436150439907238, -0.03611765130909551, -0.014503807602820374, 0.0698003965804643]] \n",
      " the MSE is 18.51741337064735, and the loss function is 18.531136662258177\n",
      "At iteration 4000, the gradient is [[-0.004339819776229264, -0.03477941297018125, -0.014517705586716152, 0.06785473265854094]] \n",
      " the MSE is 18.504927718290084, and the loss function is 18.518688480366645\n",
      "At iteration 4100, the gradient is [[-0.004245972571902467, -0.033485871844379636, -0.014525191384600504, 0.06596714273982489]] \n",
      " the MSE is 18.493147222737715, and the loss function is 18.506944822486194\n",
      "At iteration 4200, the gradient is [[-0.0041545375602936, -0.03223558779984853, -0.014526542813821647, 0.06413583836102277]] \n",
      " the MSE is 18.48202990556215, and the loss function is 18.49586370365627\n",
      "At iteration 4300, the gradient is [[-0.004065445646282719, -0.031027166462169357, -0.014522027967495015, 0.06235908688186507]] \n",
      " the MSE is 18.471536346932247, and the loss function is 18.485405699281465\n",
      "At iteration 4400, the gradient is [[-0.003978629832116769, -0.02985925777384761, -0.014511905538162754, 0.06063520972967778]] \n",
      " the MSE is 18.4616295280963, and the loss function is 18.475533787462034\n",
      "At iteration 4500, the gradient is [[-0.0038940251485831442, -0.02873055459653837, -0.014496425128759198, 0.05896258070086227]] \n",
      " the MSE is 18.452274683602464, and the loss function is 18.466213201073092\n",
      "At iteration 4600, the gradient is [[-0.003811568589878734, -0.027639791355795258, -0.014475827552288923, 0.057339624316789434]] \n",
      " the MSE is 18.443439162654066, and the loss function is 18.45741128898875\n",
      "At iteration 4700, the gradient is [[-0.0037311990519330604, -0.026585742728017895, -0.014450345121375524, 0.05576481423169898]] \n",
      " the MSE is 18.43509229903409, and the loss function is 18.449097385884823\n",
      "At iteration 4800, the gradient is [[-0.0036528572728265376, -0.02556722236814746, -0.014420201928007713, 0.05423667169112259]] \n",
      " the MSE is 18.427205289068255, and the loss function is 18.441242690088853\n",
      "At iteration 4900, the gradient is [[-0.0035764857754821645, -0.02458308167740816, -0.014385614114214776, 0.05275376403881998]] \n",
      " the MSE is 18.419751077128975, and the loss function is 18.433820148979244\n",
      "At iteration 5000, the gradient is [[-0.0035020288122349632, -0.023632208609669123, -0.014346790133879211, 0.05131470327082819]] \n",
      " the MSE is 18.412704248213203, and the loss function is 18.42680435146589\n",
      "At iteration 5100, the gradient is [[-0.003429432311494832, -0.02271352651558414, -0.01430393100621727, 0.04991814463489563]] \n",
      " the MSE is 18.406040927156297, and the loss function is 18.42017142711397\n",
      "At iteration 5200, the gradient is [[-0.0033586438256137637, -0.0218259930229092, -0.01425723056093931, 0.048562785274154564]] \n",
      " the MSE is 18.399738684070996, and the loss function is 18.4138989514995\n",
      "At iteration 5300, the gradient is [[-0.0032896124809326773, -0.020968598952305552, -0.014206875675646805, 0.047247362913314495]] \n",
      " the MSE is 18.39377644562632, and the loss function is 18.407965857410996\n",
      "At iteration 5400, the gradient is [[-0.0032222889291470703, -0.02014036726731226, -0.014153046505582134, 0.04597065458619932]] \n",
      " the MSE is 18.38813441180458, and the loss function is 18.40235235153508\n",
      "At iteration 5500, the gradient is [[-0.003156625300110136, -0.019340352057355483, -0.014095916705953197, 0.044731475403353343]] \n",
      " the MSE is 18.382793977797746, and the loss function is 18.39703983628679\n",
      "At iteration 5600, the gradient is [[-0.0030925751563884137, -0.018567637552960398, -0.014035653647176462, 0.04352867735831099]] \n",
      " the MSE is 18.3777376607249, and the loss function is 18.39201083646605\n",
      "At iteration 5700, the gradient is [[-0.0030300934489668188, -0.01782133717193442, -0.013972418623158721, 0.04236114817147002]] \n",
      " the MSE is 18.372949030872448, and the loss function is 18.387248930441558\n",
      "At iteration 5800, the gradient is [[-0.0029691364745141418, -0.017100592595709597, -0.013906367052906625, 0.04122781017026542]] \n",
      " the MSE is 18.368412647177152, and the loss function is 18.382738685581895\n",
      "At iteration 5900, the gradient is [[-0.0029096618339667923, -0.016404572874854133, -0.013837648675689266, 0.04012761920452723]] \n",
      " the MSE is 18.364113996689568, and the loss function is 18.37846559767108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 6000, the gradient is [[-0.0028516283922191796, -0.01573247356268585, -0.013766407739827619, 0.03905956359602634]] \n",
      " the MSE is 18.36003943777146, and the loss function is 18.37441603406192\n",
      "At iteration 6100, the gradient is [[-0.002794996239477423, -0.015083515876452713, -0.013692783185567556, 0.03802266312087491]] \n",
      " the MSE is 18.356176146796216, and the loss function is 18.370577180335875\n",
      "At iteration 6200, the gradient is [[-0.0027397266531762453, -0.014456945884660045, -0.013616908821848064, 0.037015968024157776]] \n",
      " the MSE is 18.35251206813563, and the loss function is 18.366936990252594\n",
      "At iteration 6300, the gradient is [[-0.0026857820618843777, -0.013852033720446622, -0.013538913497643955, 0.03603855806522732]] \n",
      " the MSE is 18.3490358672296, and the loss function is 18.363484138785402\n",
      "At iteration 6400, the gradient is [[-0.0026331260096638933, -0.013268072819438774, -0.013458921267559374, 0.03508954159326688]] \n",
      " the MSE is 18.345736886548213, and the loss function is 18.36020797805205\n",
      "At iteration 6500, the gradient is [[-0.0025817231217477666, -0.01270437918181146, -0.013377051552182509, 0.03416805465181347]] \n",
      " the MSE is 18.342605104267204, and the loss function is 18.357098495961445\n",
      "At iteration 6600, the gradient is [[-0.0025315390716567038, -0.012160290657763036, -0.013293419293351856, 0.033273260111297824]] \n",
      " the MSE is 18.339631095489064, and the loss function is 18.35414627740854\n",
      "At iteration 6700, the gradient is [[-0.0024825405486344166, -0.011635166255217396, -0.0132081351041961, 0.032404346829121834]] \n",
      " the MSE is 18.33680599585236, and the loss function is 18.351342467859663\n",
      "At iteration 6800, the gradient is [[-0.002434695226666892, -0.011128385469729178, -0.013121305414569405, 0.03156052883585333]] \n",
      " the MSE is 18.33412146738157, and the loss function is 18.348678739180546\n",
      "At iteration 6900, the gradient is [[-0.0023879717343966845, -0.010639347635388957, -0.01303303261167978, 0.030741044547105287]] \n",
      " the MSE is 18.331569666438966, and the loss function is 18.3461472575683\n",
      "At iteration 7000, the gradient is [[-0.0023423396256596024, -0.01016747129615167, -0.012943415176136137, 0.029945156000238678]] \n",
      " the MSE is 18.329143213648603, and the loss function is 18.343740653457413\n",
      "At iteration 7100, the gradient is [[-0.0022977693511756726, -0.00971219359710336, -0.012852547813652467, 0.02917214811498229]] \n",
      " the MSE is 18.32683516567046, and the loss function is 18.341451993277516\n",
      "At iteration 7200, the gradient is [[-0.0022542322311974563, -0.009272969694999413, -0.01276052158253864, 0.02842132797722962]] \n",
      " the MSE is 18.324638988710483, and the loss function is 18.339274752948672\n",
      "At iteration 7300, the gradient is [[-0.0022117004288714605, -0.008849272187330488, -0.012667424016997648, 0.027692024145415026]] \n",
      " the MSE is 18.322548533659173, and the loss function is 18.33720279300665\n",
      "At iteration 7400, the gradient is [[-0.0021701469243306263, -0.00844059055936984, -0.01257333924641065, 0.0269835859787069]] \n",
      " the MSE is 18.32055801275814, and the loss function is 18.33523033525753\n",
      "At iteration 7500, the gradient is [[-0.0021295454897928647, -0.008046430648782238, -0.012478348110817602, 0.026295382986230607]] \n",
      " the MSE is 18.318661977700263, and the loss function is 18.333351940867146\n",
      "At iteration 7600, the gradient is [[-0.002089870665553063, -0.007666314127222246, -0.01238252827272039, 0.025626804196672032]] \n",
      " the MSE is 18.31685529907483, and the loss function is 18.33156248979668\n",
      "At iteration 7700, the gradient is [[-0.002051097736137487, -0.007299777997982446, -0.012285954325037922, 0.02497725754792637]] \n",
      " the MSE is 18.31513314707463, and the loss function is 18.329857161501263\n",
      "At iteration 7800, the gradient is [[-0.0020132027079245887, -0.006946374109870007, -0.012188697895804046, 0.024346169295639264]] \n",
      " the MSE is 18.31349097338709, and the loss function is 18.328231416813622\n",
      "At iteration 7900, the gradient is [[-0.001976162286961492, -0.006605668686231113, -0.012090827749331089, 0.023732983440483475]] \n",
      " the MSE is 18.311924494196354, and the loss function is 18.32668098093959\n",
      "At iteration 8000, the gradient is [[-0.0019399538577812982, -0.0062772418689438034, -0.011992409884147643, 0.023137161173349154]] \n",
      " the MSE is 18.31042967422764, and the loss function is 18.325201827496688\n",
      "At iteration 8100, the gradient is [[-0.001904555462469328, -0.005960687276660988, -0.011893507627646637, 0.022558180338077737]] \n",
      " the MSE is 18.309002711769658, and the loss function is 18.323790163531616\n",
      "At iteration 8200, the gradient is [[-0.0018699457810077862, -0.005655611577299078, -0.01179418172786124, 0.021995534910836265]] \n",
      " the MSE is 18.307640024614717, and the loss function is 18.32244241545608\n",
      "At iteration 8300, the gradient is [[-0.0018361041115748772, -0.005361634073726687, -0.011694490442033164, 0.021448734496088768]] \n",
      " the MSE is 18.306338236859794, and the loss function is 18.321155215844275\n",
      "At iteration 8400, the gradient is [[-0.0018030103517989123, -0.005078386302676745, -0.011594489622402443, 0.02091730383827316]] \n",
      " the MSE is 18.305094166515573, and the loss function is 18.319925391038936\n",
      "At iteration 8500, the gradient is [[-0.001770644980710623, -0.004805511646423375, -0.011494232799254068, 0.020400782348748488]] \n",
      " the MSE is 18.30390481387363, and the loss function is 18.318749949516075\n",
      "At iteration 8600, the gradient is [[-0.0017389890409121342, -0.004542664956544862, -0.011393771261134686, 0.019898723647730345]] \n",
      " the MSE is 18.302767350584872, and the loss function is 18.317626070961506\n",
      "At iteration 8700, the gradient is [[-0.0017080241215262355, -0.0042895121896872265, -0.011293154132529435, 0.01941069512052636]] \n",
      " the MSE is 18.301679109405562, and the loss function is 18.316551096015395\n",
      "At iteration 8800, the gradient is [[-0.0016777323416725485, -0.004045730054843128, -0.011192428449007155, 0.018936277487684616]] \n",
      " the MSE is 18.300637574569684, and the loss function is 18.315522516643586\n",
      "At iteration 8900, the gradient is [[-0.0016480963343521358, -0.0038110056717395127, -0.011091639229876025, 0.018475064388675425]] \n",
      " the MSE is 18.299640372749042, and the loss function is 18.31453796709706\n",
      "At iteration 9000, the gradient is [[-0.0016190992308815846, -0.00358503623998958, -0.010990829548457039, 0.01802666197865937]] \n",
      " the MSE is 18.29868526456494, and the loss function is 18.31359521542331\n",
      "At iteration 9100, the gradient is [[-0.0015907246459028659, -0.0033675287188375718, -0.010890040600127576, 0.01759068853780855]] \n",
      " the MSE is 18.29777013661745, and the loss function is 18.312692155495636\n",
      "At iteration 9200, the gradient is [[-0.0015629566626394822, -0.003158199516851869, -0.010789311768038703, 0.017166774093015306]] \n",
      " the MSE is 18.296892994000366, and the loss function is 18.311826799528433\n",
      "At iteration 9300, the gradient is [[-0.0015357798188757835, -0.0029567741915668432, -0.010688680686760136, 0.01675456005137108]] \n",
      " the MSE is 18.296051953272027, and the loss function is 18.310997271048585\n",
      "At iteration 9400, the gradient is [[-0.0015091790930679468, -0.002762987158512347, -0.010588183303774102, 0.016353698845229473]] \n",
      " the MSE is 18.29524523585384, and the loss function is 18.31020179829481\n",
      "At iteration 9500, the gradient is [[-0.001483139891232374, -0.002576581409548109, -0.010487853939007826, 0.015963853588309828]] \n",
      " the MSE is 18.29447116183028, and the loss function is 18.309438708018714\n",
      "At iteration 9600, the gradient is [[-0.0014576480339503286, -0.00239730824004971, -0.010387725342386613, 0.015584697742659092]] \n",
      " the MSE is 18.293728144125655, and the loss function is 18.30870641966278\n",
      "At iteration 9700, the gradient is [[-0.0014326897439285633, -0.0022249269846991958, -0.010287828749475711, 0.015215914796041062]] \n",
      " the MSE is 18.293014683034464, and the loss function is 18.30800343989215\n",
      "At iteration 9800, the gradient is [[-0.00140825163378956, -0.002059204761639606, -0.01018819393528947, 0.014857197949475336]] \n",
      " the MSE is 18.292329361083564, and the loss function is 18.307328357458335\n",
      "At iteration 9900, the gradient is [[-0.0013843206945624784, -0.001899916224810598, -0.010088849266398307, 0.014508249814477832]] \n",
      " the MSE is 18.29167083820588, and the loss function is 18.30667983837466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8732/639760672.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_x = torch.cat((ones_column, torch.tensor(new_x)), dim=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([16.4219, 14.2659, 16.7077, 16.9884, 16.9103,  8.1404,  7.5841,  8.1455,\n",
       "         7.0547, 12.3957, 14.5907, 15.0555, 12.8511, 16.6907, 29.1498, 23.7636,\n",
       "        24.1697, 25.4969, 31.1875, 33.3104, 27.0464, 28.8434, 29.2948, 29.9948,\n",
       "        25.0764], dtype=torch.float64)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code from class Intro_To_Locally_Weighted_Regression\n",
    "class CustomMinMaxScaler:\n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Compute the minimum and maximum value of the data for scaling.\n",
    "\n",
    "        Args:\n",
    "        - data (torch.Tensor): Input data tensor.\n",
    "        \"\"\"\n",
    "        self.min = torch.min(data, dim=0, keepdim=True).values\n",
    "        self.max = torch.max(data, dim=0, keepdim=True).values\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Scale the data based on the computed minimum and maximum values.\n",
    "\n",
    "        Args:\n",
    "        - data (torch.Tensor): Input data tensor.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Scaled data tensor.\n",
    "        \"\"\"\n",
    "        if self.min is None or self.max is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet. Please call 'fit' with appropriate data.\")\n",
    "        \n",
    "        #I discovered that sometimes in this step you might get self.max-self.min = 0, which leads to a division by 0\n",
    "        #error, leading to NaNs in the output tensor. For instance, if one column of the tensor has a max value of 8\n",
    "        #and a min value of 8, 8-8 = 0 (this happened in the cylinders column in the cars dataset) which leads to \n",
    "        #division by zero. I have added an extra step which replaces any NaNs in the scaled_data with 1.\n",
    "        scaled_data = (data - self.min) / (self.max - self.min)\n",
    "        scaled_data = torch.where(torch.isnan(scaled_data), torch.tensor(1.0), scaled_data)\n",
    "        return scaled_data\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"\n",
    "        Fit to data, then transform it.\n",
    "\n",
    "        Args:\n",
    "        - data (torch.Tensor): Input data tensor.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Scaled data tensor.\n",
    "        \"\"\"\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "#Getting gradient explosion with MSE increasing for some reason. Ask prof for advice.\n",
    "class CustomRidgeModel:\n",
    "    \n",
    "    def __init__(self, alpha=0.00001, max_iter=10000, lr=0.02):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.lr = lr\n",
    "        \n",
    "    #User specifies if they would like to scale data here, if they would like to add an intercept column.\n",
    "    def scale(self,X,y,scale=True,intercept=True):\n",
    "        #X is of shape (n,p) where n is number of samples and p is number of features\n",
    "        #y is of shape (n,1)\n",
    "        self.X = torch.tensor(X)\n",
    "        \n",
    "        \n",
    "        #Scaling\n",
    "        if scale == True:\n",
    "            scaler = CustomMinMaxScaler()\n",
    "            self.X = scaler.fit_transform(self.X)\n",
    "        self.y = torch.tensor(y)\n",
    "        self.n = self.X.shape[0]\n",
    "        \n",
    "        #Add intercept\n",
    "        if intercept:\n",
    "            ones_column = torch.ones(self.n, 1)\n",
    "            self.X = torch.cat((ones_column, torch.tensor(self.X)), dim=1)\n",
    "        self.p = self.X.shape[1]\n",
    "        self.w=torch.tensor(np.ones((1,self.p))*0)\n",
    "        \n",
    "    #Fit the data: perform gradient descent until weights are optimized\n",
    "    def fit(self,X,y,tolerance = 1e-6,scale=True,intercept=True):\n",
    "        self.scale(X,y,scale,intercept)\n",
    "        w = self.w\n",
    "        for i in range(self.max_iter):\n",
    "            X = self.X\n",
    "            y = self.y\n",
    "            yPred = X@w.flatten()\n",
    "            error = y-yPred\n",
    "            opt = (1/self.n)*torch.sum(error**2)+self.alpha*torch.sum(w**2)\n",
    "            grad = -(2.0/self.n) * error@X + self.alpha*2*w\n",
    "            mse = (1.0/self.n) * torch.sum(error**2)\n",
    "            newW = w - self.lr*grad\n",
    "            if torch.sum(abs(newW - w)) < tolerance:\n",
    "                print(\"Gradient Descent has converged\")\n",
    "                break\n",
    "            w = newW\n",
    "            if i % 100 == 0:\n",
    "                print(\"At iteration %s, the gradient is %s \\n the MSE is %s, and the loss function is %s\" % (str(i),str(grad.tolist()), mse.item(),opt.item()))\n",
    "        self.w = newW\n",
    "    #Predict new value of y for a row\n",
    "    def predict(self,new_x,intercept=True,scale=True):\n",
    "        w = self.w\n",
    "        if scale:\n",
    "            scaler = CustomMinMaxScaler()\n",
    "            new_x = scaler.fit_transform(new_x)\n",
    "        \n",
    "        if intercept:\n",
    "            ones_column = torch.ones(new_x.shape[0],1)\n",
    "            new_x = torch.cat((ones_column, torch.tensor(new_x)), dim=1)            \n",
    "        yPred = new_x@w.flatten()\n",
    "        return yPred\n",
    "\n",
    "model = CustomRidgeModel()\n",
    "data = pd.read_csv('../content/01intro/cars.csv')\n",
    "y = data['MPG'].values\n",
    "X = data.drop(['MPG'],axis=1).values\n",
    "#This will print the gradient descent at 100-iteration intervals\n",
    "model.fit(X,y)\n",
    "#This will print expected return mpg for a given set of inputs. This input can be of arbitrary length n as long as\n",
    "#it has the same number of features as the data the model was fitted with.\n",
    "new_X = torch.tensor(X)[0:25]\n",
    "model.predict(new_X)\n",
    "#We could use this with k-fold for further optimization of model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa726f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2636a39e",
   "metadata": {},
   "source": [
    "# Complete the exercise provided in the Application to Locally Weighted Regression notebook and test the method on a data set, for example, the one provided in class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60908f7",
   "metadata": {},
   "source": [
    "Adjust the code below and make it work without errors. Compare the results with the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "98947f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Kernel\n",
    "def Gaussian(x):\n",
    "  return np.where(np.abs(x)>4,0,1/(np.sqrt(2*np.pi))*np.exp(-1/2*x**2))\n",
    "# this is the correct vectorized version\n",
    "def tricubic(x):\n",
    "  return np.where(np.abs(x)>1,0,(1-np.abs(x)**3)**3)\n",
    "# Epanechnikov Kernel\n",
    "def Epanechnikov(x):\n",
    "  return np.where(np.abs(x)>1,0,3/4*(1-np.abs(x)**2))\n",
    "# Quartic Kernel\n",
    "def Quartic(x):\n",
    "  return np.where(np.abs(x)>1,0,15/16*(1-np.abs(x)**2)**2)\n",
    "\n",
    "def weight_function(u,v,kern=Gaussian,tau=0.5):\n",
    "    return kern(dist(u,v)/(2*tau))\n",
    "\n",
    "def dist(u,v):\n",
    "  D = []\n",
    "  if len(v.shape)==1:\n",
    "    v = v.reshape(1,-1)\n",
    "  # we would like all the pairwise combinations if u and v are matrices\n",
    "  # we could avoid two for loops if we consider broadcasting\n",
    "  for rowj in v:\n",
    "    D.append(np.sqrt(np.sum((u-rowj)**2,axis=1)))\n",
    "  return np.array(D).T\n",
    "\n",
    "def kernel_function(xi,x0,kern, tau):\n",
    "    return kern((xi - x0)/(2*tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e3d097cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.12717127, 16.35627513, 16.6662146 , 17.26997367, 17.40461308,\n",
       "       14.37842598, 14.09099113, 14.42087629, 13.74169599, 15.41415626,\n",
       "       15.16359016, 16.47623847, 15.06659002, 13.99996086, 28.04194919,\n",
       "       21.77241641, 21.78087761, 21.77325133, 31.23932603, 32.71997936,\n",
       "       25.72652373, 28.06934906, 28.69713971, 28.32976296, 21.86163991,\n",
       "       12.99761072, 13.59975579, 13.6746693 , 11.25667231, 31.23932603,\n",
       "       27.08072068, 28.99070598, 19.80860843, 19.12274268, 18.50875233,\n",
       "       18.67196718, 19.29718566, 14.65380186, 13.96137964, 14.89407973,\n",
       "       14.92000035, 12.089635  , 12.86433643, 11.97745926, 21.76825052,\n",
       "       26.68687283, 18.79800082, 19.78969263, 28.34130208, 29.42987767,\n",
       "       33.30931607, 32.58983282, 34.0829047 , 34.18797713, 32.7226503 ,\n",
       "       32.88433991, 28.6500694 , 31.21803773, 30.26129802, 26.68687283,\n",
       "       28.30553224, 14.38580341, 14.31411677, 14.75629636, 15.00197065,\n",
       "       16.80305546, 13.04183511, 13.4168358 , 13.62232151, 14.14762806,\n",
       "       19.93333871, 15.78686849, 14.88349315, 13.99913927, 14.99981415,\n",
       "       23.0512802 , 26.80174881, 22.54393696, 30.8823254 , 27.38127038,\n",
       "       29.99372252, 26.86568055, 30.87609859, 31.45909851, 15.12177205,\n",
       "       16.80305546, 15.61094134, 15.14202845, 16.2312404 , 12.03607326,\n",
       "       13.96137964, 14.02764111, 14.32443341, 12.58361411, 11.9670432 ,\n",
       "       16.10072159, 20.00475617, 18.82371601, 20.29368011, 20.58722883,\n",
       "       21.63730483, 32.32391237, 12.10458752, 12.3377621 , 12.82912327,\n",
       "       13.43040331, 20.27984704, 30.06407638, 26.70238639, 28.35233464,\n",
       "       14.54637195, 27.82489208, 23.32003555, 30.07471724, 15.20109785,\n",
       "       14.77129042, 34.05530209, 29.2083918 , 26.52229412, 23.68852807,\n",
       "       16.55344143, 25.74139225, 23.69319457, 16.35143897, 20.99159781,\n",
       "       20.33879732, 18.46777211, 33.7684509 , 27.09528899, 34.08074297,\n",
       "       26.37563687, 16.40663935, 16.51977601, 18.5691599 , 14.71727798,\n",
       "       12.50691834, 13.3087128 , 11.93260895, 14.18193302, 30.43994203,\n",
       "       33.73383248, 29.90047962, 34.16195376, 33.33695224, 31.99055103,\n",
       "       32.11229546, 28.62884923, 26.96640142, 28.26989858, 33.61858629,\n",
       "       19.59951537, 17.83090856, 17.95873576, 19.65329569, 13.14568975,\n",
       "       13.69243456, 13.09702504, 12.7115939 , 16.98883786, 15.83590268,\n",
       "       16.05440898, 17.83364559, 20.10382489, 20.29125547, 15.97710355,\n",
       "       30.92296968, 26.04664848, 20.32921364, 26.22259235, 25.59472743,\n",
       "       31.23584975, 26.64947108, 22.55854521, 33.04485223, 19.54548316,\n",
       "       25.42650064, 22.74387198, 22.94058912, 25.64654938, 33.35603124,\n",
       "       27.81948646, 28.7998815 , 26.28768075, 30.15360578, 30.27717462,\n",
       "       14.37773722, 14.52467637, 15.47224486, 14.63648428, 19.68666817,\n",
       "       18.37054157, 21.24727455, 19.95932371, 33.02740497, 30.87609859,\n",
       "       33.04485223, 33.35603124, 18.42406024, 17.32728274, 17.0237081 ,\n",
       "       19.5559404 , 32.74620384, 33.24039258, 31.04813903, 26.30919254,\n",
       "       23.06700622, 15.57756438, 20.63767136, 23.39929621, 16.48721531,\n",
       "       13.94906101, 15.32029318, 15.91580792, 16.30982277, 31.72197891,\n",
       "       29.67543657, 33.97971011, 27.87970047, 33.4130498 , 15.83242358,\n",
       "       16.91971326, 14.73529935, 13.99405634, 17.55838632, 18.9295958 ,\n",
       "       18.50547301, 17.53674306, 14.97500845, 14.83956074, 14.57911722,\n",
       "       14.14360012, 32.36650055, 26.01321645, 30.17421418, 25.48120349,\n",
       "       31.68468679, 31.52893556, 32.16045211, 30.77240932, 23.61801477,\n",
       "       26.21692247, 22.59848376, 32.84105531, 32.7061684 , 33.7241773 ,\n",
       "       32.8335639 , 33.34609189, 21.06770933, 16.37793025, 17.2462799 ,\n",
       "       18.59265936, 20.6998152 , 21.38250822, 25.6675164 , 19.14777605,\n",
       "       19.54879519, 19.06003194, 21.04853935, 18.5431743 , 17.7064088 ,\n",
       "       17.20069891, 18.87094029, 16.30427008, 14.98720614, 30.94611813,\n",
       "       26.32398692, 28.07540813, 29.67852502, 26.49466187, 26.22647756,\n",
       "       25.66785603, 27.4836685 , 25.4924783 , 21.88977302, 24.44418357,\n",
       "       19.32113781, 32.90685521, 31.09923549, 19.46448425, 21.31349636,\n",
       "       24.79597075, 19.3697737 , 19.3378483 , 16.01723848, 16.62621261,\n",
       "       15.74962579, 16.02938729, 14.03263083, 15.33051471, 19.68173784,\n",
       "       15.79386802, 33.17316383, 33.22484134, 32.36211217, 25.65526762,\n",
       "       25.23831731, 15.95437118, 25.19381945, 21.06628294, 29.9069728 ,\n",
       "       30.28299083, 33.10298236, 31.85278234, 26.22731202, 23.77706836,\n",
       "       23.62857448, 26.44545311, 31.03080165, 33.00602755, 31.21130711,\n",
       "       33.02469567, 26.20530182, 24.88192246, 25.708629  , 19.28134063,\n",
       "       30.78833006, 25.53818616, 26.64014883, 27.32289053, 29.15060319,\n",
       "       32.4892507 , 26.09239631, 32.58423253, 32.27000088, 30.36867159,\n",
       "       32.1886142 , 26.69933159, 33.23276815, 31.12534581, 33.40328799,\n",
       "       23.06212923, 22.22217216, 26.83200597, 29.05531783, 26.56501521,\n",
       "       26.31366615, 26.4864491 , 23.56655787, 26.88433631, 34.0404844 ,\n",
       "       33.54574248, 33.97783524, 31.69831869, 33.30230017, 32.58677104,\n",
       "       32.74958892, 29.79271293, 31.72197891, 29.19368108, 29.98299173,\n",
       "       29.64522681, 28.5514503 , 26.13131495, 25.95603905, 25.44855658,\n",
       "       23.46862903, 23.10886044, 23.74694415, 18.95870011, 16.34290984,\n",
       "       21.08523456, 19.0491007 , 26.37939601, 26.03389143, 27.96244296,\n",
       "       26.65250342, 26.44895617, 26.02969692, 24.90474404, 31.32928327,\n",
       "       32.54209734, 32.81914885, 30.46437151, 31.17424598, 28.86020559,\n",
       "       29.68255748, 29.29632717, 32.84135853, 32.84135853, 32.70073659,\n",
       "       22.36129106, 21.83411982, 26.52453549, 20.33533662, 26.03391349,\n",
       "       26.93136859, 25.59796868, 25.29002517, 31.23932603, 27.19180412,\n",
       "       26.03758926, 25.17220987])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code was copied and adjusted from Efficeint_Applications_with_Distances_and_Weights\n",
    "class Lowess:\n",
    "    def __init__(self, kernel = Gaussian, tau=0.05):\n",
    "        self.kernel = kernel\n",
    "        self.tau = tau\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        kernel = self.kernel\n",
    "        tau = self.tau\n",
    "        self.xtrain_ = x\n",
    "        self.yhat_ = y\n",
    "\n",
    "    def predict(self, x_new):\n",
    "        check_is_fitted(self)\n",
    "        x = self.xtrain_\n",
    "        y = self.yhat_\n",
    "        lm = LinearRegression()\n",
    "        w = weight_function(x,x_new,self.kernel,self.tau)\n",
    "\n",
    "        if np.isscalar(x_new):\n",
    "            lm.fit(np.diag(w)@(x.reshape(-1,1)),np.diag(w)@(y.reshape(-1,1)))\n",
    "            yest = lm.predict([[x_new]])[0][0]\n",
    "        else:\n",
    "          n = len(x_new)\n",
    "          yest_test = np.zeros(n)\n",
    "          #Looping through all x-points\n",
    "          for i in range(n):\n",
    "            lm.fit(np.diag(w[:,i])@x,np.diag(w[:,i])@y)\n",
    "            yest_test[i] = lm.predict(x_new[i].reshape(1,-1))\n",
    "        return yest_test\n",
    "\n",
    "#We'll use the same X and y as last time but we'll pre-scale them this time.\n",
    "scaler = MinMaxScaler()\n",
    "data = pd.read_csv('../content/01intro/cars.csv')\n",
    "y = data['MPG']\n",
    "X = data.drop(['MPG'],axis=1)\n",
    "Xscaled = scaler.fit_transform(torch.tensor(X.values))\n",
    "model = Lowess()\n",
    "model.fit(Xscaled,y)\n",
    "#This returns a tensor with y values for each x in the input\n",
    "yEst = model.predict(Xscaled)\n",
    "yEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a4e288ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.791404587907422"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see the MSE\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "mse(yEst,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "38ce3244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8732/639760672.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.cat((ones_column, torch.tensor(self.X)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, the gradient is [[-46.83579545454547, -19.079886394318176, -11.986209499853182, -15.802950244726192]] \n",
      " the MSE is 609.1130955517049, and the loss function is 609.1130955517049\n",
      "At iteration 100, the gradient is [[-3.314563973745384, 2.4989227051603233, 2.2478344983135545, 1.8892354701013572]] \n",
      " the MSE is 63.42874159310418, and the loss function is 63.43345613439054\n",
      "At iteration 200, the gradient is [[-1.7955507710815957, 1.3291254213002517, 1.2395644600071472, 1.1406659768800644]] \n",
      " the MSE is 33.17812726312421, and the loss function is 33.18558148829518\n",
      "At iteration 300, the gradient is [[-0.9907510229514866, 0.6723728469878286, 0.6745265550141913, 0.7134507148775177]] \n",
      " the MSE is 23.983167570653166, and the loss function is 23.992628425834184\n",
      "At iteration 400, the gradient is [[-0.5489485490723223, 0.31398812591421177, 0.3641907785451106, 0.47624660100195426]] \n",
      " the MSE is 21.13600582789536, and the loss function is 21.146728074285953\n",
      "At iteration 500, the gradient is [[-0.3063362909917718, 0.11930661843663248, 0.1936695388349414, 0.3434323136356916]] \n",
      " the MSE is 20.20608561362402, and the loss function is 20.217556743809023\n",
      "At iteration 600, the gradient is [[-0.17304959258497338, 0.014401006091858995, 0.09989391645940311, 0.26799587765277516]] \n",
      " the MSE is 19.858413222305273, and the loss function is 19.870321642171206\n",
      "At iteration 700, the gradient is [[-0.09976802528722085, -0.04129986632678724, 0.04824940048433515, 0.22413288972809095]] \n",
      " the MSE is 19.690500049126307, and the loss function is 19.70266637722721\n",
      "At iteration 800, the gradient is [[-0.05942296945573882, -0.0700603701887889, 0.019738122465231153, 0.19767839716064056]] \n",
      " the MSE is 19.58079256690223, and the loss function is 19.593116502457367\n",
      "At iteration 900, the gradient is [[-0.03715825065645889, -0.08409765649416853, 0.003933096825191245, 0.18085560836236927]] \n",
      " the MSE is 19.492394765074398, and the loss function is 19.50482128814229\n",
      "At iteration 1000, the gradient is [[-0.024820244337789836, -0.09011474239306576, -0.004888697781628849, 0.16939382911943746]] \n",
      " the MSE is 19.41393747675304, and the loss function is 19.42643690732205\n",
      "At iteration 1100, the gradient is [[-0.01793388463604368, -0.09178839865499669, -0.009868596534982321, 0.1609466991031462]] \n",
      " the MSE is 19.34176189315196, and the loss function is 19.354318534509268\n",
      "At iteration 1200, the gradient is [[-0.014043009786001613, -0.09113457240686425, -0.012731016931962486, 0.15422407113325065]] \n",
      " the MSE is 19.27454952937439, and the loss function is 19.28715531693102\n",
      "At iteration 1300, the gradient is [[-0.011799495206660161, -0.08925776860930364, -0.014422718104989478, 0.14851555846738387]] \n",
      " the MSE is 19.211702389509835, and the loss function is 19.22435337493769\n",
      "At iteration 1400, the gradient is [[-0.010463343078976473, -0.08676233022070805, -0.015463645387146051, 0.14342904100458886]] \n",
      " the MSE is 19.15285340596171, and the loss function is 19.165547823589524\n",
      "At iteration 1500, the gradient is [[-0.009628316821755642, -0.08397815858680094, -0.016139484445539108, 0.13874714729174795]] \n",
      " the MSE is 19.097717721736196, and the loss function is 19.11045493084048\n",
      "At iteration 1600, the gradient is [[-0.009071361181368698, -0.08108459329248116, -0.01660734100065741, 0.13434848517361037]] \n",
      " the MSE is 19.04604687601106, and the loss function is 19.058826785736656\n",
      "At iteration 1700, the gradient is [[-0.008669959476379815, -0.07817839902955255, -0.016953741323284116, 0.13016440846782681]] \n",
      " the MSE is 18.997614137196013, and the loss function is 19.010436896610383\n",
      "At iteration 1800, the gradient is [[-0.008356776735344398, -0.07531107683362166, -0.017226464972135484, 0.12615528692638892]] \n",
      " the MSE is 18.952209321511504, and the loss function is 18.965075154947204\n",
      "At iteration 1900, the gradient is [[-0.00809476642267186, -0.07250933970612182, -0.017452015890347443, 0.12229748024668236]] \n",
      " the MSE is 18.90963654444142, and the loss function is 18.922545666229077\n",
      "At iteration 2000, the gradient is [[-0.007863508181316974, -0.06978634842318397, -0.01764521140278358, 0.11857618685206921]] \n",
      " the MSE is 18.86971291483095, and the loss function is 18.882665487238803\n",
      "At iteration 2100, the gradient is [[-0.007651709488025524, -0.06714787632638525, -0.01781444527583248, 0.1149815169900115]] \n",
      " the MSE is 18.83226756193619, and the loss function is 18.84526367647038\n",
      "At iteration 2200, the gradient is [[-0.007453090246631502, -0.06459569104962308, -0.017964576565329182, 0.11150633549993827]] \n",
      " the MSE is 18.797140805088063, and the loss function is 18.810180476282653\n",
      "At iteration 2300, the gradient is [[-0.007264124039795897, -0.06212940887608149, -0.018098515416834566, 0.10814507589539195]] \n",
      " the MSE is 18.764183403952224, and the loss function is 18.777266569704768\n",
      "At iteration 2400, the gradient is [[-0.0070827983717557125, -0.059747510884688215, -0.018218093704685332, 0.10489308759761014]] \n",
      " the MSE is 18.733255867184365, and the loss function is 18.74638239239615\n",
      "At iteration 2500, the gradient is [[-0.006907934166290672, -0.057447899115211795, -0.01832454315951728, 0.1017462758415769]] \n",
      " the MSE is 18.704227809976384, and the loss function is 18.717397491738886\n",
      "At iteration 2600, the gradient is [[-0.0067388122029472305, -0.05522820033311717, -0.018418758063978056, 0.09870090227081668]] \n",
      " the MSE is 18.67697735525707, and the loss function is 18.690189928638457\n",
      "At iteration 2700, the gradient is [[-0.006574968012974977, -0.0530859313193735, -0.018501439703681264, 0.09575347378206675]] \n",
      " the MSE is 18.65139057489439, and the loss function is 18.664645718824463\n",
      "At iteration 2800, the gradient is [[-0.006416079233358763, -0.051018588208679896, -0.01857317591258481, 0.092900679861583]] \n",
      " the MSE is 18.627360967938078, and the loss function is 18.640658310932775\n",
      "At iteration 2900, the gradient is [[-0.0062619037073782255, -0.04902369418952555, -0.018634484987413477, 0.09013935659041339]] \n",
      " the MSE is 18.6047889733145, and the loss function is 18.618128098913285\n",
      "At iteration 3000, the gradient is [[-0.006112245438590924, -0.047098824396417605, -0.018685840037988132, 0.08746646534012324]] \n",
      " the MSE is 18.583581514633394, and the loss function is 18.596961966494217\n",
      "At iteration 3100, the gradient is [[-0.005966935834514421, -0.04524161832783614, -0.018727682591947008, 0.08487907958269515]] \n",
      " the MSE is 18.563651574956207, and the loss function is 18.577072861591017\n",
      "At iteration 3200, the gradient is [[-0.00582582334351653, -0.04344978545931245, -0.018760430293703566, 0.08237437620404682]] \n",
      " the MSE is 18.544917799535103, and the loss function is 18.55837939868899\n",
      "At iteration 3300, the gradient is [[-0.005688767700952393, -0.04172110716226169, -0.018784481354427323, 0.07994962933746935]] \n",
      " the MSE is 18.527304124672124, and the loss function is 18.540805487359275\n",
      "At iteration 3400, the gradient is [[-0.0055556367070669615, -0.04005343663420002, -0.01880021721131915, 0.07760220562690104]] \n",
      " the MSE is 18.510739430974585, and the loss function is 18.52427998518904\n",
      "At iteration 3500, the gradient is [[-0.0054263043962031994, -0.038444697774925846, -0.018808004196624054, 0.07532956032032877]] \n",
      " the MSE is 18.495157219398806, and the loss function is 18.50873637351991\n",
      "At iteration 3600, the gradient is [[-0.005300649972409219, -0.03689288352096673, -0.018808194656343275, 0.07312923386217035]] \n",
      " the MSE is 18.4804953085817, and the loss function is 18.494112454494452\n",
      "At iteration 3700, the gradient is [[-0.005178557166607404, -0.03539605391728898, -0.01880112775973366, 0.07099884880220884]] \n",
      " the MSE is 18.466695552059054, and the loss function is 18.480350068009145\n",
      "At iteration 3800, the gradient is [[-0.005059913828460422, -0.03395233407946137, -0.018787130132950355, 0.06893610691851702]] \n",
      " the MSE is 18.45370357406206, and the loss function is 18.467394827264446\n",
      "At iteration 3900, the gradient is [[-0.004944611647454212, -0.03255991212778623, -0.018766516389314182, 0.06693878649789793]] \n",
      " the MSE is 18.441468522669986, and the loss function is 18.455195871688925\n",
      "At iteration 4000, the gradient is [[-0.0048325459477891785, -0.03121703713827582, -0.01873958959699998, 0.06500473974048239]] \n",
      " the MSE is 18.429942839177308, and the loss function is 18.44370563609466\n",
      "At iteration 4100, the gradient is [[-0.004723615524933413, -0.02992201713330075, -0.018706641706270857, 0.06313189026956346]] \n",
      " the MSE is 18.419082042608977, and the loss function is 18.43287963499641\n",
      "At iteration 4200, the gradient is [[-0.004617722507179774, -0.0286732171237879, -0.018667953948840795, 0.06131823073466393]] \n",
      " the MSE is 18.40884452838753, and the loss function is 18.422676261097358\n",
      "At iteration 4300, the gradient is [[-0.0045147722319539, -0.027469057208084038, -0.018623797216188558, 0.059561820500467766]] \n",
      " the MSE is 18.399191380221414, and the loss function is 18.413056597009675\n",
      "At iteration 4400, the gradient is [[-0.004414673132560827, -0.026308010729998322, -0.018574432421209233, 0.05786078341582313]] \n",
      " the MSE is 18.390086194345113, and the loss function is 18.403984239339625\n",
      "At iteration 4500, the gradient is [[-0.004317336631430969, -0.02518860249564158, -0.018520110845218773, 0.0562133056592333]] \n",
      " the MSE is 18.38149491529884, and the loss function is 18.395425134324046\n",
      "At iteration 4600, the gradient is [[-0.004222677039105491, -0.02410940704879131, -0.01846107447216148, 0.0546176336571046]] \n",
      " the MSE is 18.373385682489058, and the loss function is 18.387347424258667\n",
      "At iteration 4700, the gradient is [[-0.0041306114570331814, -0.023069047003003836, -0.01839755631069593, 0.05307207207228918]] \n",
      " the MSE is 18.365728686820972, and the loss function is 18.37972130400862\n",
      "At iteration 4800, the gradient is [[-0.004041059684704118, -0.022066191429376773, -0.018329780705153762, 0.051574981859964675]] \n",
      " the MSE is 18.35849603674072, and the loss function is 18.372518886938124\n",
      "At iteration 4900, the gradient is [[-0.0039539441298220785, -0.021099554297948558, -0.018257963635690336, 0.0501247783887795]] \n",
      " the MSE is 18.351661633068694, and the loss function is 18.365714079640167\n",
      "At iteration 5000, the gradient is [[-0.003869189721619979, -0.02016789297113576, -0.018182313008145073, 0.04871992962491394]] \n",
      " the MSE is 18.345201052046, and the loss function is 18.359282464887492\n",
      "At iteration 5100, the gradient is [[-0.0037867238272795084, -0.019270006747676427, -0.01810302893410294, 0.047358954376822644]] \n",
      " the MSE is 18.339091436054115, and the loss function is 18.35320119226439\n",
      "At iteration 5200, the gradient is [[-0.0037064761712050512, -0.018404735455261414, -0.018020304001394854, 0.04604042059870568]] \n",
      " the MSE is 18.333311391503283, and the loss function is 18.34744887597431\n",
      "At iteration 5300, the gradient is [[-0.0036283787568991643, -0.017570958090239647, -0.01793432353540919, 0.04476294375074936]] \n",
      " the MSE is 18.327840893418625, and the loss function is 18.342005499351746\n",
      "At iteration 5400, the gradient is [[-0.0035523657915297936, -0.01676759150296791, -0.017845265851569054, 0.04352518521413295]] \n",
      " the MSE is 18.322661196283395, and the loss function is 18.336852325637313\n",
      "At iteration 5500, the gradient is [[-0.003478373612995171, -0.015993589127087798, -0.017753302499204104, 0.042325850759089546]] \n",
      " the MSE is 18.317754750728387, and the loss function is 18.33197181460467\n",
      "At iteration 5600, the gradient is [[-0.003406340619446, -0.015247939751476623, -0.01765859849720057, 0.0411636890641153]] \n",
      " the MSE is 18.313105125683215, and the loss function is 18.32734754465452\n",
      "At iteration 5700, the gradient is [[-0.0033362072011302755, -0.01452966633324747, -0.017561312561614768, 0.0400374902847255]] \n",
      " the MSE is 18.308696935630355, and the loss function is 18.322964140016197\n",
      "At iteration 5800, the gradient is [[-0.0032679156745418366, -0.013837824850583814, -0.017461597325602554, 0.038946084670029524]] \n",
      " the MSE is 18.304515772626846, and the loss function is 18.318807202721455\n",
      "At iteration 5900, the gradient is [[-0.003201410218707641, -0.013171503193923963, -0.01735959955183148, 0.037888341225572775]] \n",
      " the MSE is 18.300548142780215, and the loss function is 18.31486324903662\n",
      "At iteration 6000, the gradient is [[-0.003136636813762242, -0.012529820094428698, -0.01725546033776981, 0.03686316642081166]] \n",
      " the MSE is 18.296781406885955, and the loss function is 18.311119650060128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 6100, the gradient is [[-0.0030735431812554765, -0.011911924088155551, -0.017149315313882897, 0.0358695029399056]] \n",
      " the MSE is 18.293203724953244, and the loss function is 18.307564576211917\n",
      "At iteration 6200, the gradient is [[-0.003012078726801113, -0.0113169925151521, -0.017041294835232076, 0.03490632847412171]] \n",
      " the MSE is 18.289804004363322, and the loss function is 18.30418694535873\n",
      "At iteration 6300, the gradient is [[-0.0029521944844204603, -0.010744230551895446, -0.016931524166438915, 0.033972654554722065]] \n",
      " the MSE is 18.28657185142196, and the loss function is 18.30097637433661\n",
      "At iteration 6400, the gradient is [[-0.0028938430627959027, -0.010192870276303896, -0.0168201236604614, 0.03306752542476743]] \n",
      " the MSE is 18.28349752608311, and the loss function is 18.297923133647355\n",
      "At iteration 6500, the gradient is [[-0.0028369785932033124, -0.009662169763925994, -0.016707208931197263, 0.03219001694872677]] \n",
      " the MSE is 18.2805718996353, and the loss function is 18.295018105120352\n",
      "At iteration 6600, the gradient is [[-0.0027815566795305175, -0.009151412214609333, -0.016592891020335242, 0.03133923555837482]] \n",
      " the MSE is 18.277786415156402, and the loss function is 18.292252742345184\n",
      "At iteration 6700, the gradient is [[-0.0027275343494961002, -0.008659905108238247, -0.016477276558405612, 0.030514317234073673]] \n",
      " the MSE is 18.275133050554764, and the loss function is 18.28961903369283\n",
      "At iteration 6800, the gradient is [[-0.002674870007854319, -0.008186979388893963, -0.016360467920431023, 0.02971442652000365]] \n",
      " the MSE is 18.272604284027047, and the loss function is 18.287109467755617\n",
      "At iteration 6900, the gradient is [[-0.002623523390848571, -0.007731988676239508, -0.016242563376196142, 0.028938755572374842]] \n",
      " the MSE is 18.270193061773977, and the loss function is 18.284717001046964\n",
      "At iteration 7000, the gradient is [[-0.0025734555225808394, -0.007294308503540241, -0.01612365723554881, 0.028186523239293652]] \n",
      " the MSE is 18.267892767825824, and the loss function is 18.282435027812618\n",
      "At iteration 7100, the gradient is [[-0.002524628672405122, -0.006873335580938227, -0.016003839988543187, 0.02745697417159315]] \n",
      " the MSE is 18.26569719583912, and the loss function is 18.280257351814722\n",
      "At iteration 7200, the gradient is [[-0.0024770063140315607, -0.00646848708366466, -0.015883198440978477, 0.026749377963182053]] \n",
      " the MSE is 18.263600522735228, and the loss function is 18.278178159959207\n",
      "At iteration 7300, the gradient is [[-0.0024305530859167436, -0.006079199963978729, -0.01576181584522376, 0.0260630283202119]] \n",
      " the MSE is 18.26159728405986, and the loss function is 18.2761919976455\n",
      "At iteration 7400, the gradient is [[-0.0023852347528079175, -0.005704930286145302, -0.015639772026565673, 0.0253972422580283]] \n",
      " the MSE is 18.259682350950666, and the loss function is 18.274293745725537\n",
      "At iteration 7500, the gradient is [[-0.0023410181688338213, -0.005345152583849463, -0.015517143505339366, 0.02475135932483812]] \n",
      " the MSE is 18.25785090860735, and the loss function is 18.272478598966444\n",
      "At iteration 7600, the gradient is [[-0.0022978712417731735, -0.004999359239034417, -0.01539400361479983, 0.024124740851392462]] \n",
      " the MSE is 18.25609843616568, and the loss function is 18.270742045918194\n",
      "At iteration 7700, the gradient is [[-0.0022557628982005024, -0.004667059881511532, -0.015270422614948083, 0.023516769225787666]] \n",
      " the MSE is 18.2544206878835, and the loss function is 18.269079850094183\n",
      "At iteration 7800, the gradient is [[-0.002214663050228523, -0.004347780808846557, -0.015146467802553305, 0.022926847192364]] \n",
      " the MSE is 18.25281367555246, and the loss function is 18.267488032378424\n",
      "At iteration 7900, the gradient is [[-0.002174542563079259, -0.004041064425621538, -0.015022203617364636, 0.02235439717409883]] \n",
      " the MSE is 18.25127365205523, and the loss function is 18.265962854579037\n",
      "At iteration 8000, the gradient is [[-0.0021353732237863214, -0.0037464687014255515, -0.014897691744643856, 0.02179886061766473]] \n",
      " the MSE is 18.249797095993095, and the loss function is 18.264500804052847\n",
      "At iteration 8100, the gradient is [[-0.0020971277110305012, -0.0034635666471413214, -0.01477299121426856, 0.02125969736027588]] \n",
      " the MSE is 18.248380697313703, and the loss function is 18.263098579330777\n",
      "At iteration 8200, the gradient is [[-0.0020597795658133062, -0.003191945808714335, -0.01464815849638992, 0.02073638501775727]] \n",
      " the MSE is 18.247021343873435, and the loss function is 18.261753076678527\n",
      "At iteration 8300, the gradient is [[-0.0020233031632606977, -0.002931207777889846, -0.014523247593800439, 0.020228418393028313]] \n",
      " the MSE is 18.24571610887321, and the loss function is 18.2604613775312\n",
      "At iteration 8400, the gradient is [[-0.0019876736853145748, -0.0026809677194387558, -0.01439831013119585, 0.019735308904306203]] \n",
      " the MSE is 18.24446223911042, and the loss function is 18.259220736744556\n",
      "At iteration 8500, the gradient is [[-0.0019528670942746368, -0.0024408539141224264, -0.014273395441299158, 0.01925658403246454]] \n",
      " the MSE is 18.24325714399355, and the loss function is 18.25802857160944\n",
      "At iteration 8600, the gradient is [[-0.0019188601072356636, -0.0022105073170338883, -0.014148550648048335, 0.018791786786833263]] \n",
      " the MSE is 18.242098385269614, and the loss function is 18.25688245157941\n",
      "At iteration 8700, the gradient is [[-0.0018856301715327387, -0.0019895811308200755, -0.014023820746985993, 0.018340475188763347]] \n",
      " the MSE is 18.24098366741762, and the loss function is 18.255780088664757\n",
      "At iteration 8800, the gradient is [[-0.001853155440725192, -0.0017777403930888114, -0.013899248682787994, 0.017902221772557257]] \n",
      " the MSE is 18.239910828664552, and the loss function is 18.254719328449415\n",
      "At iteration 8900, the gradient is [[-0.0018214147514879447, -0.0015746615776892176, -0.01377487542416242, 0.017476613103009075]] \n",
      " the MSE is 18.238877832583082, and the loss function is 18.25369814168984\n",
      "At iteration 9000, the gradient is [[-0.0017903876015606796, -0.0013800322095386217, -0.013650740036262545, 0.017063249308935713]] \n",
      " the MSE is 18.23788276023298, and the loss function is 18.25271461645791\n",
      "At iteration 9100, the gradient is [[-0.0017600541278880392, -0.0011935504920501043, -0.013526879750418754, 0.016661743632530066]] \n",
      " the MSE is 18.23692380281063, and the loss function is 18.251766950792145\n",
      "At iteration 9200, the gradient is [[-0.0017303950858750562, -0.0010149249473194073, -0.013403330031621877, 0.01627172199354843]] \n",
      " the MSE is 18.23599925477342, and the loss function is 18.250853445823992\n",
      "At iteration 9300, the gradient is [[-0.001701391829289128, -0.0008438740682958851, -0.013280124643625168, 0.015892822568111937]] \n",
      " the MSE is 18.235107507408088, and the loss function is 18.249972499348274\n",
      "At iteration 9400, the gradient is [[-0.00167302629066129, -0.0006801259825987329, -0.013157295711793452, 0.015524695381596829]] \n",
      " the MSE is 18.2342470428137, and the loss function is 18.24912259980845\n",
      "At iteration 9500, the gradient is [[-0.0016452809623968912, -0.0005234181276001106, -0.01303487378375519, 0.015167001915115283]] \n",
      " the MSE is 18.23341642827258, and the loss function is 18.24830232066993\n",
      "At iteration 9600, the gradient is [[-0.0016181388788013423, -0.0003734969365506885, -0.012912887888066953, 0.014819414725014595]] \n",
      " the MSE is 18.23261431098349, and the loss function is 18.24751031515577\n",
      "At iteration 9700, the gradient is [[-0.001591583598186147, -0.00023011753500353307, -0.012791365590681428, 0.014481617075269942]] \n",
      " the MSE is 18.23183941313369, and the loss function is 18.246745311321387\n",
      "At iteration 9800, the gradient is [[-0.0015655991859171558, -9.304344762283564e-05, -0.012670333049602384, 0.014153302581991763]] \n",
      " the MSE is 18.231090527287485, and the loss function is 18.246006107445815\n",
      "At iteration 9900, the gradient is [[-0.0015401701980270203, 3.7953685246809607e-05, -0.012549815067581866, 0.013834174869841677]] \n",
      " the MSE is 18.23036651207092, and the loss function is 18.245291567719192\n",
      "At iteration 0, the gradient is [[-46.562500005681834, -18.92193183977271, -11.910877687646824, -15.04588791572802]] \n",
      " the MSE is 600.4105678471594, and the loss function is 600.4105678471594\n",
      "At iteration 100, the gradient is [[-3.2308231981434683, 2.4774866710796823, 2.2794107798092376, 1.7980076777489389]] \n",
      " the MSE is 61.176128716946884, and the loss function is 61.18078965786947\n",
      "At iteration 200, the gradient is [[-1.7510928463030528, 1.3259452904647342, 1.2636727528441134, 1.0893115046822384]] \n",
      " the MSE is 31.860963608393128, and the loss function is 31.8682758404407\n",
      "At iteration 300, the gradient is [[-0.9682885663596015, 0.6759732958467638, 0.6920492974146321, 0.6826315835789588]] \n",
      " the MSE is 22.90951911623248, and the loss function is 22.918778656014172\n",
      "At iteration 400, the gradient is [[-0.5374510356732163, 0.32016991895706126, 0.3772663317040011, 0.4562929036560051]] \n",
      " the MSE is 20.129126619403994, and the loss function is 20.139613546248157\n",
      "At iteration 500, the gradient is [[-0.30025132574852104, 0.1262030113436298, 0.2038450869621939, 0.3292916071110093]] \n",
      " the MSE is 19.22194789318852, and the loss function is 19.23316494062805\n",
      "At iteration 600, the gradient is [[-0.169609252331443, 0.021229782914187974, 0.10822245429882656, 0.25703083523080195]] \n",
      " the MSE is 18.886183733519694, and the loss function is 18.8978274326758\n",
      "At iteration 700, the gradient is [[-0.09760666791065141, -0.034831407088782886, 0.055420782542630755, 0.2149675038453517]] \n",
      " the MSE is 18.727319577531578, and the loss function is 18.739214597945512\n",
      "At iteration 800, the gradient is [[-0.05787527891674406, -0.06403456760877796, 0.026192338691621287, 0.18959374018508884]] \n",
      " the MSE is 18.625617209806318, and the loss function is 18.63766516025889\n",
      "At iteration 900, the gradient is [[-0.03590515383300354, -0.07851360500068355, 0.009945193051255944, 0.17347415711346623]] \n",
      " the MSE is 18.544574074805404, and the loss function is 18.55672077322826\n",
      "At iteration 1000, the gradient is [[-0.0237117316886181, -0.08494369650527812, 0.0008505059848048651, 0.16251498367114653]] \n",
      " the MSE is 18.472915409778135, and the loss function is 18.485131498914193\n",
      "At iteration 1100, the gradient is [[-0.016901206867980325, -0.08699627320745663, -0.0042995376159105266, 0.15446133180582003]] \n",
      " the MSE is 18.40702409687974, and the loss function is 18.419293967086897\n",
      "At iteration 1200, the gradient is [[-0.01305568188138357, -0.08668993531950986, -0.007270455126092112, 0.14807027655483]] \n",
      " the MSE is 18.345617164585164, and the loss function is 18.357932739161253\n",
      "At iteration 1300, the gradient is [[-0.0108445562431119, -0.08513382602881193, -0.009034091173812476, 0.14265581995545332]] \n",
      " the MSE is 18.28813172456198, and the loss function is 18.300489008641712\n",
      "At iteration 1400, the gradient is [[-0.009535545359200493, -0.08293669017939234, -0.010125540522670826, 0.13783849357043185]] \n",
      " the MSE is 18.2342347696762, and the loss function is 18.246631946116104\n",
      "At iteration 1500, the gradient is [[-0.008725606061382346, -0.08043196753516052, -0.010839554393366648, 0.1334075687212802]] \n",
      " the MSE is 18.18367364828327, and the loss function is 18.196110032453223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8732/639760672.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.cat((ones_column, torch.tensor(self.X)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 1600, the gradient is [[-0.00819287140436872, -0.07780165441916716, -0.011338618426427872, 0.1292452320652306]] \n",
      " the MSE is 18.136229917048787, and the loss function is 18.148705386277257\n",
      "At iteration 1700, the gradient is [[-0.007815173911094278, -0.07514446011569988, -0.011712425692960333, 0.12528485925594998]] \n",
      " the MSE is 18.091704635785046, and the loss function is 18.104219320331577\n",
      "At iteration 1800, the gradient is [[-0.007525210818653482, -0.07251331009291993, -0.012010603575139398, 0.12148805201290026]] \n",
      " the MSE is 18.04991327547739, and the loss function is 18.062467393263425\n",
      "At iteration 1900, the gradient is [[-0.007285872525606117, -0.0699359815802675, -0.012260722892879715, 0.1178320002998047]] \n",
      " the MSE is 18.010683603197272, and the loss function is 18.023277373358674\n",
      "At iteration 2000, the gradient is [[-0.0070766663597361685, -0.06742645734210205, -0.012478208199175089, 0.11430252621974768]] \n",
      " the MSE is 17.973854518879847, and the loss function is 17.986488118384525\n",
      "At iteration 2100, the gradient is [[-0.006886245832539393, -0.06499117194952121, -0.012671791476004323, 0.11089025447929975]] \n",
      " the MSE is 17.93927522133415, and the loss function is 17.951948764996757\n",
      "At iteration 2200, the gradient is [[-0.0067082995963246225, -0.06263244756500636, -0.012846513462335073, 0.10758850338457827]] \n",
      " the MSE is 17.906804508904994, and the loss function is 17.919518041992404\n",
      "At iteration 2300, the gradient is [[-0.0065392891661253975, -0.060350383229715865, -0.01300537549433994, 0.10439212265933039]] \n",
      " the MSE is 17.876310151748324, and the loss function is 17.889063649247582\n",
      "At iteration 2400, the gradient is [[-0.006377203974301586, -0.05814389319471337, -0.013150248746224677, 0.10129685232950432]] \n",
      " the MSE is 17.847668313623533, and the loss function is 17.86046168292795\n",
      "At iteration 2500, the gradient is [[-0.006220876240341064, -0.05601127703387437, -0.013282374827610592, 0.09829896838994918]] \n",
      " the MSE is 17.82076301416263, and the loss function is 17.83359609940762\n",
      "At iteration 2600, the gradient is [[-0.0060696038958789255, -0.05395053214870941, -0.013402641505897546, 0.09539508633022951]] \n",
      " the MSE is 17.79548562693646, and the loss function is 17.808358214027834\n",
      "At iteration 2700, the gradient is [[-0.00592294302675029, -0.05195952455706176, -0.013511734676837354, 0.09258205157510996]] \n",
      " the MSE is 17.77173441022369, and the loss function is 17.784646232046473\n",
      "At iteration 2800, the gradient is [[-0.005780593598472785, -0.05003608173727356, -0.013610222229529666, 0.0898568777988029]] \n",
      " the MSE is 17.74941406805346, and the loss function is 17.762364809593432\n",
      "At iteration 2900, the gradient is [[-0.005642336514928213, -0.04817804261817643, -0.013698600426603948, 0.08721671162850332]] \n",
      " the MSE is 17.728435339432203, and the loss function is 17.741424642675618\n",
      "At iteration 3000, the gradient is [[-0.0055079989265802145, -0.0463832840233632, -0.013777319649885064, 0.08465881191313183]] \n",
      " the MSE is 17.708714613877852, and the loss function is 17.721742082427497\n",
      "At iteration 3100, the gradient is [[-0.005377435085599096, -0.0446497341934548, -0.013846798784069086, 0.08218053704938962]] \n",
      " the MSE is 17.69017357154174, and the loss function is 17.70323877492645\n",
      "At iteration 3200, the gradient is [[-0.005250515757807582, -0.0429753792311398, -0.013907433341002598, 0.07977933678271681]] \n",
      " the MSE is 17.672738846326684, and the loss function is 17.68584132400235\n",
      "At iteration 3300, the gradient is [[-0.005127122344534584, -0.041358265684059044, -0.01395960013262691, 0.07745274651053616]] \n",
      " the MSE is 17.656341710520884, and the loss function is 17.669480975571624\n",
      "At iteration 3400, the gradient is [[-0.005007143598841695, -0.03979650103417626, -0.014003660038320664, 0.07519838300060264]] \n",
      " the MSE is 17.640917779567065, and the loss function is 17.65409332212051\n",
      "At iteration 3500, the gradient is [[-0.0048904737698543905, -0.03828825306493512, -0.01403995971686306, 0.07301394092568338]] \n",
      " the MSE is 17.62640673567707, and the loss function is 17.639618026049963\n",
      "At iteration 3600, the gradient is [[-0.0047770115354255115, -0.036831748640704064, -0.01406883273171706, 0.07089718988310387]] \n",
      " the MSE is 17.612752069086163, and the loss function is 17.625998560677225\n",
      "At iteration 3700, the gradient is [[-0.004666659369435269, -0.03542527219088083, -0.014090600347258566, 0.06884597171617376]] \n",
      " the MSE is 17.59990083581905, and the loss function is 17.61318196776579\n",
      "At iteration 3800, the gradient is [[-0.0045593231502952785, -0.03406716405914648, -0.014105572138220588, 0.06685819803528631]] \n",
      " the MSE is 17.587803430912334, and the loss function is 17.601118630527992\n",
      "At iteration 3900, the gradient is [[-0.0044549119029070414, -0.032755818804879606, -0.014114046490598569, 0.0649318478817003]] \n",
      " the MSE is 17.57641337610558, and the loss function is 17.589762061111536\n",
      "At iteration 4000, the gradient is [[-0.004353337616295457, -0.031489683504229316, -0.014116311037543277, 0.06306496550111437]] \n",
      " the MSE is 17.565687121076667, and the loss function is 17.5790687016448\n",
      "At iteration 4100, the gradient is [[-0.0042545151033089, -0.030267256075573002, -0.014112643054023728, 0.06125565820848097]] \n",
      " the MSE is 17.55558385735611, and the loss function is 17.568997737974723\n",
      "At iteration 4200, the gradient is [[-0.004158361885491014, -0.029087083642740545, -0.014103309823873623, 0.05950209433231063]] \n",
      " the MSE is 17.546065344110367, and the loss function is 17.55951092528637\n",
      "At iteration 4300, the gradient is [[-0.0040647980923592345, -0.027947760941927358, -0.014088568986457654, 0.057802501231595096]] \n",
      " the MSE is 17.537095745036172, and the loss function is 17.550572424845424\n",
      "At iteration 4400, the gradient is [[-0.003973746370537904, -0.026847928775680416, -0.014068668867698605, 0.056155163379956644]] \n",
      " the MSE is 17.528641475655867, and the loss function is 17.54214865115276\n",
      "At iteration 4500, the gradient is [[-0.003885131798812659, -0.025786272514065508, -0.01404384879752824, 0.05455842051391918]] \n",
      " the MSE is 17.520671060349834, and the loss function is 17.534208128846505\n",
      "At iteration 4600, the gradient is [[-0.003798881808026542, -0.024761520643255097, -0.014014339415641449, 0.053010665842086725]] \n",
      " the MSE is 17.51315499850385, and the loss function is 17.526721358728604\n",
      "At iteration 4700, the gradient is [[-0.0037149261043852096, -0.02377244336037112, -0.013980362966362058, 0.05151034431304094]] \n",
      " the MSE is 17.506065639189337, and the loss function is 17.519660692333332\n",
      "At iteration 4800, the gradient is [[-0.0036331965960179915, -0.022817851213664985, -0.013942133583402069, 0.05005595093967314]] \n",
      " the MSE is 17.499377063831727, and the loss function is 17.51300021449224\n",
      "At iteration 4900, the gradient is [[-0.0035536273219460924, -0.021896593786614956, -0.013899857564956892, 0.04864602917811326]] \n",
      " the MSE is 17.493064976356575, and the loss function is 17.506715633384704\n",
      "At iteration 5000, the gradient is [[-0.003476154383839609, -0.021007558424819977, -0.013853733639672669, 0.047279169359258834]] \n",
      " the MSE is 17.48710660033624, and the loss function is 17.500784177596252\n",
      "At iteration 5100, the gradient is [[-0.0034007158798599634, -0.020149669004136363, -0.013803953223676784, 0.045954007171322364]] \n",
      " the MSE is 17.481480582689922, and the loss function is 17.495184499737025\n",
      "At iteration 5200, the gradient is [[-0.0033272518406695767, -0.01932188473888104, -0.01375070066909987, 0.044669222191639525]] \n",
      " the MSE is 17.47616690351871, and the loss function is 17.48989658620157\n",
      "At iteration 5300, the gradient is [[-0.0032557041676512305, -0.018523199028855627, -0.013694153504399415, 0.04342353646607598]] \n",
      " the MSE is 17.47114679168398, and the loss function is 17.484901672677783\n",
      "At iteration 5400, the gradient is [[-0.0031860165731188725, -0.017752638343853717, -0.013634482666694944, 0.04221571313456795]] \n",
      " the MSE is 17.46640264576253, and the loss function is 17.480182165038073\n",
      "At iteration 5500, the gradient is [[-0.0031181345223827957, -0.01700926114444224, -0.01357185272641876, 0.04104455510126291]] \n",
      " the MSE is 17.46191796003505, and the loss function is 17.475721565268923\n",
      "At iteration 5600, the gradient is [[-0.0030520051778058716, -0.016292156837921443, -0.013506422104535813, 0.039908903747787064]] \n",
      " the MSE is 17.457677255186862, and the loss function is 17.471504402117418\n",
      "At iteration 5700, the gradient is [[-0.0029875773445475753, -0.015600444768180708, -0.013438343282541836, 0.03880763768830274]] \n",
      " the MSE is 17.453666013420033, and the loss function is 17.467516166153505\n",
      "At iteration 5800, the gradient is [[-0.002924801418235488, -0.014933273238594346, -0.013367763005595728, 0.037739671564845485]] \n",
      " the MSE is 17.44987061769526, and the loss function is 17.46374324896615\n",
      "At iteration 5900, the gradient is [[-0.002863629333950516, -0.01428981856644895, -0.013294822478751618, 0.03670395488194647]] \n",
      " the MSE is 17.44627829484015, and the loss function is 17.460172886229604\n",
      "At iteration 6000, the gradient is [[-0.00280401451745219, -0.013669284168503057, -0.013219657556901921, 0.035699470878789254]] \n",
      " the MSE is 17.442877062276967, and the loss function is 17.456793104392695\n",
      "At iteration 6100, the gradient is [[-0.0027459118374970628, -0.013070899676060373, -0.013142398928278358, 0.034725235438152674]] \n",
      " the MSE is 17.439655678139033, and the loss function is 17.45359267075999\n",
      "At iteration 6200, the gradient is [[-0.002689277559714632, -0.012493920078873587, -0.013063172291862194, 0.033780296030707986]] \n",
      " the MSE is 17.436603594559507, and the loss function is 17.450561046748387\n",
      "At iteration 6300, the gradient is [[-0.002634069302366762, -0.01193762489709613, -0.012982098529026907, 0.03286373069346026]] \n",
      " the MSE is 17.433710913930124, and the loss function is 17.447688344116465\n",
      "At iteration 6400, the gradient is [[-0.002580245993007963, -0.011401317379959176, -0.012899293869335209, 0.031974647041460816]] \n",
      " the MSE is 17.430968347940578, and the loss function is 17.44496528397707\n",
      "At iteration 6500, the gradient is [[-0.002527767826964105, -0.010884323730730535, -0.012814870050946808, 0.031112181311457356]] \n",
      " the MSE is 17.428367179220913, and the loss function is 17.442383158415335\n",
      "At iteration 6600, the gradient is [[-0.0024765962269138126, -0.01038599235678886, -0.012728934475625696, 0.030275497436635157]] \n",
      " the MSE is 17.425899225421222, and the loss function is 17.439933794546228\n",
      "At iteration 6700, the gradient is [[-0.0024266938038750045, -0.009905693144210545, -0.012641590358646093, 0.029463786151303847]] \n",
      " the MSE is 17.423556805572915, and the loss function is 17.437609520855734\n",
      "At iteration 6800, the gradient is [[-0.0023780243194869424, -0.009442816755916749, -0.012552936873678682, 0.02867626412465391]] \n",
      " the MSE is 17.421332708586295, and the loss function is 17.43540313568029\n",
      "At iteration 6900, the gradient is [[-0.0023305526492313445, -0.008996773952639042, -0.01246306929283351, 0.027912173122644303]] \n",
      " the MSE is 17.419220163748065, and the loss function is 17.43330787768788\n",
      "At iteration 7000, the gradient is [[-0.0022842447472603057, -0.008566994936131195, -0.012372079122118783, 0.027170779196959108]] \n",
      " the MSE is 17.41721281309137, and the loss function is 17.431317398233357\n",
      "At iteration 7100, the gradient is [[-0.002239067612053576, -0.008152928713654832, -0.012280054232319305, 0.026451371900348488]] \n",
      " the MSE is 17.415304685518944, and the loss function is 17.429425735468318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 7200, the gradient is [[-0.0021949892532271997, -0.007754042483182001, -0.012187078985522019, 0.025753263527406024]] \n",
      " the MSE is 17.413490172567624, and the loss function is 17.427627290093806\n",
      "At iteration 7300, the gradient is [[-0.002151978659656058, -0.007369821038733519, -0.012093234357499594, 0.025075788379856346]] \n",
      " the MSE is 17.411764005709696, and the loss function is 17.42591680265111\n",
      "At iteration 7400, the gradient is [[-0.0021100057680343513, -0.006999766194823714, -0.01199859805582814, 0.02441830205589773]] \n",
      " the MSE is 17.41012123509301, and the loss function is 17.42428933225251\n",
      "At iteration 7500, the gradient is [[-0.0020690414332615913, -0.006643396230035326, -0.011903244634327527, 0.023780180762255958]] \n",
      " the MSE is 17.40855720962829, and the loss function is 17.422740236660363\n",
      "At iteration 7600, the gradient is [[-0.002029057399088383, -0.00630024534835692, -0.011807245603433896, 0.023160820648833996]] \n",
      " the MSE is 17.40706755833784, and the loss function is 17.42126515362853\n",
      "At iteration 7700, the gradient is [[-0.001990026270068113, -0.005969863158308892, -0.01171066953703641, 0.0225596371647504]] \n",
      " the MSE is 17.40564817288516, and the loss function is 17.41985998342571\n",
      "At iteration 7800, the gradient is [[-0.001951921484242852, -0.005651814168894555, -0.011613582175651995, 0.021976064435350034]] \n",
      " the MSE is 17.404295191210448, and the loss function is 17.418520872465468\n",
      "At iteration 7900, the gradient is [[-0.0019147172869138666, -0.005345677302095921, -0.011516046526236475, 0.02140955465931457]] \n",
      " the MSE is 17.40300498220139, and the loss function is 17.41724419797237\n",
      "At iteration 8000, the gradient is [[-0.00187838870493383, -0.005051045421122234, -0.011418122958547878, 0.020859577525430828]] \n",
      " the MSE is 17.401774131333497, and the loss function is 17.416026553618362\n",
      "At iteration 8100, the gradient is [[-0.0018429115221770436, -0.004767524874188425, -0.011319869298409734, 0.02032561964811335]] \n",
      " the MSE is 17.400599427218083, and the loss function is 17.41486473606748\n",
      "At iteration 8200, the gradient is [[-0.0018082622555595598, -0.004494735052976599, -0.011221340917704301, 0.019807184021379258]] \n",
      " the MSE is 17.39947784900032, and the loss function is 17.413755732371207\n",
      "At iteration 8300, the gradient is [[-0.001774418131922122, -0.00423230796567931, -0.011122590821492815, 0.019303789490363134]] \n",
      " the MSE is 17.398406554553116, and the loss function is 17.41269670816025\n",
      "At iteration 8400, the gradient is [[-0.001741357065605108, -0.003979887823781116, -0.011023669732084365, 0.018814970240094524]] \n",
      " the MSE is 17.397382869416244, and the loss function is 17.411684996582043\n",
      "At iteration 8500, the gradient is [[-0.0017090576368090978, -0.003737130642423743, -0.010924626170361698, 0.01834027530074255]] \n",
      " the MSE is 17.396404276433394, and the loss function is 17.410718087936633\n",
      "At iteration 8600, the gradient is [[-0.0016774990706147326, -0.0035037038537744517, -0.010825506534358894, 0.017879268068922285]] \n",
      " the MSE is 17.39546840604258, and the loss function is 17.40979361996638\n",
      "At iteration 8700, the gradient is [[-0.001646661216652145, -0.0032792859329464517, -0.010726355175158925, 0.01743152584453221]] \n",
      " the MSE is 17.39457302717859, and the loss function is 17.40890936875804\n",
      "At iteration 8800, the gradient is [[-0.0016165245294481945, -0.003063566036189601, -0.010627214470291434, 0.016996639382551446]] \n",
      " the MSE is 17.39371603874839, and the loss function is 17.408063240218162\n",
      "At iteration 8900, the gradient is [[-0.001587070049493349, -0.002856243650835888, -0.01052812489463069, 0.016574212459376004]] \n",
      " the MSE is 17.39289546164313, and the loss function is 17.407253262085423\n",
      "At iteration 9000, the gradient is [[-0.0015582793846613025, -0.002657028256611091, -0.010429125088866254, 0.01616386145324583]] \n",
      " the MSE is 17.392109431252717, and the loss function is 17.406477576445788\n",
      "At iteration 9100, the gradient is [[-0.0015301346925559326, -0.0024656389981276127, -0.010330251925769769, 0.015765214938150057]] \n",
      " the MSE is 17.391356190450967, and the loss function is 17.405734432718496\n",
      "At iteration 9200, the gradient is [[-0.001502618663101282, -0.002281804367859963, -0.010231540574074096, 0.015377913291039911]] \n",
      " the MSE is 17.390634083021446, and the loss function is 17.40502218108302\n",
      "At iteration 9300, the gradient is [[-0.0014757145020661232, -0.0021052618997091195, -0.010133024560379608, 0.015001608311560497]] \n",
      " the MSE is 17.38994154749609, and the loss function is 17.404339266318928\n",
      "At iteration 9400, the gradient is [[-0.0014494059145525036, -0.001935757872211089, -0.010034735828718777, 0.014635962854378752]] \n",
      " the MSE is 17.389277111380302, and the loss function is 17.403684222032496\n",
      "At iteration 9500, the gradient is [[-0.0014236770897514237, -0.001773047021873407, -0.009936704798419899, 0.014280650473057913]] \n",
      " the MSE is 17.388639385740145, and the loss function is 17.403055665245503\n",
      "At iteration 9600, the gradient is [[-0.0013985126855334531, -0.00161689226545238, -0.009838960419720696, 0.01393535507578539]] \n",
      " the MSE is 17.38802706012853, and the loss function is 17.402452291323215\n",
      "At iteration 9700, the gradient is [[-0.001373897813876061, -0.0014670644315032827, -0.009741530227646646, 0.013599770592080477]] \n",
      " the MSE is 17.38743889782896, and the loss function is 17.40187286922003\n",
      "At iteration 9800, the gradient is [[-0.0013498180265736812, -0.001323342000627614, -0.009644440394027341, 0.013273600650339123]] \n",
      " the MSE is 17.386873731396747, and the loss function is 17.401316237022698\n",
      "At iteration 9900, the gradient is [[-0.0013262593016887684, -0.001185510854275521, -0.009547715777782823, 0.012956558265752514]] \n",
      " the MSE is 17.386330458478774, and the loss function is 17.400781297772234\n",
      "At iteration 0, the gradient is [[-46.77280453257788, -19.23501419150143, -12.084864342534651, -15.069564079752247]] \n",
      " the MSE is 607.9848717235132, and the loss function is 607.9848717235132\n",
      "At iteration 100, the gradient is [[-3.3246596002416275, 2.5033016103768855, 2.326231006930252, 1.8425060026668867]] \n",
      " the MSE is 62.97180291228301, and the loss function is 62.976539956110706\n",
      "At iteration 200, the gradient is [[-1.7934118251915327, 1.3307025497094018, 1.2787741741201206, 1.1086549241180357]] \n",
      " the MSE is 32.49260098131069, and the loss function is 32.500105578007975\n",
      "At iteration 300, the gradient is [[-0.9852740648915913, 0.6740665810292902, 0.6938938892405111, 0.69157118657408]] \n",
      " the MSE is 23.300119080911024, and the loss function is 23.309640901896213\n",
      "At iteration 400, the gradient is [[-0.5432447074709861, 0.31679407013215566, 0.3737955480840444, 0.4609819616686235]] \n",
      " the MSE is 20.47909607850246, and the loss function is 20.489879762120644\n",
      "At iteration 500, the gradient is [[-0.30140068369447576, 0.12320272132518438, 0.19853146837859037, 0.33247754335268437]] \n",
      " the MSE is 19.568248116974075, and the loss function is 19.579777115215446\n",
      "At iteration 600, the gradient is [[-0.169038879456864, 0.019068301070526004, 0.10248735786009486, 0.25987529659249]] \n",
      " the MSE is 19.23285958126013, and the loss function is 19.244821288007717\n",
      "At iteration 700, the gradient is [[-0.09655456738275829, -0.036199886936429365, 0.04977891955391588, 0.21791607459929271]] \n",
      " the MSE is 19.07341486868508, and the loss function is 19.08563005915135\n",
      "At iteration 800, the gradient is [[-0.056819300566389966, -0.06479869696948685, 0.020780951870541523, 0.19278368766408918]] \n",
      " the MSE is 18.970154837101337, and the loss function is 18.98252368398443\n",
      "At iteration 900, the gradient is [[-0.03499670521662417, -0.07886561837298896, 0.0047602165200210265, 0.17692006278658126]] \n",
      " the MSE is 18.887024660436747, and the loss function is 18.89949261236389\n",
      "At iteration 1000, the gradient is [[-0.022972831443755226, -0.08503682253612342, -0.0041535786467477714, 0.1661897000302309]] \n",
      " the MSE is 18.813027658824772, and the loss function is 18.825565410041524\n",
      "At iteration 1100, the gradient is [[-0.016310163328169452, -0.08693991502907834, -0.009171212445351662, 0.1583289377190831]] \n",
      " the MSE is 18.744679565736956, and the loss function is 18.75727169746196\n",
      "At iteration 1200, the gradient is [[-0.012581795010185089, -0.08655719706974387, -0.012048987883385083, 0.1520978678231433]] \n",
      " the MSE is 18.68075757657813, and the loss function is 18.693396229155542\n",
      "At iteration 1300, the gradient is [[-0.010460406867876795, -0.08497121900009132, -0.01374775648818064, 0.14681652432464845]] \n",
      " the MSE is 18.620727790154813, and the loss function is 18.633409172402622\n",
      "At iteration 1400, the gradient is [[-0.009220034913423004, -0.08277251500427155, -0.014793333032549654, 0.1421112270005675]] \n",
      " the MSE is 18.564273467832432, and the loss function is 18.576995943399194\n",
      "At iteration 1500, the gradient is [[-0.008463593000849085, -0.08028258790997526, -0.015473603406878348, 0.1377758573268582]] \n",
      " the MSE is 18.511152908088793, and the loss function is 18.523915951551682\n",
      "At iteration 1600, the gradient is [[-0.0079738571692341, -0.0776758560112298, -0.015946321464976553, 0.1336959897598056]] \n",
      " the MSE is 18.461156125612067, and the loss function is 18.473959756129684\n",
      "At iteration 1700, the gradient is [[-0.007631970292727321, -0.07504634319904883, -0.016298062480559677, 0.129807398100567]] \n",
      " the MSE is 18.4140912175149, and the loss function is 18.426935693958164\n",
      "At iteration 1800, the gradient is [[-0.007372872966182302, -0.07244414976225173, -0.016576464592571553, 0.126073361330776]] \n",
      " the MSE is 18.369779697296938, and the loss function is 18.382665356228166\n",
      "At iteration 1900, the gradient is [[-0.007160929011034383, -0.06989539656003592, -0.016807861686770013, 0.12247225075854078]] \n",
      " the MSE is 18.328054572973198, and the loss function is 18.34098174447439\n",
      "At iteration 2000, the gradient is [[-0.006976595234573034, -0.06741313097698277, -0.017006926977619675, 0.11899073996970597]] \n",
      " the MSE is 18.288759292552506, and the loss function is 18.30172825833408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8732/639760672.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.cat((ones_column, torch.tensor(self.X)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 2100, the gradient is [[-0.0068091311913626865, -0.06500329000225959, -0.017181947305604767, 0.11562008988909508]] \n",
      " the MSE is 18.251746986315926, and the loss function is 18.264757960661306\n",
      "At iteration 2200, the gradient is [[-0.006652612168041782, -0.06266796013618899, -0.01733770790304888, 0.11235411563909053]] \n",
      " the MSE is 18.216879829471857, and the loss function is 18.229932952399295\n",
      "At iteration 2300, the gradient is [[-0.0065037486719800525, -0.06040715899604209, -0.017477070337584687, 0.10918807320423508]] \n",
      " the MSE is 18.18402846856826, and the loss function is 18.197123805419462\n",
      "At iteration 2400, the gradient is [[-0.0063606938819581, -0.058219808490881896, -0.01760183575717855, 0.10611804917430726]] \n",
      " the MSE is 18.153071491970923, and the loss function is 18.166209036276353\n",
      "At iteration 2500, the gradient is [[-0.006222391407512631, -0.056104265909881434, -0.01771321726410891, 0.10314062566005638]] \n",
      " the MSE is 18.12389493640066, and the loss function is 18.137074614314432\n",
      "At iteration 2600, the gradient is [[-0.0060882185396787135, -0.05405861327344879, -0.017812098516489426, 0.10025269574053006]] \n",
      " the MSE is 18.09639182540513, and the loss function is 18.10961350079062\n",
      "At iteration 2700, the gradient is [[-0.005957791104681257, -0.05208081451510562, -0.01789917541110223, 0.09745136127568216]] \n",
      " the MSE is 18.07046173704944, and the loss function is 18.08372521672697\n",
      "At iteration 2800, the gradient is [[-0.005830856698094183, -0.05016880041479878, -0.017975033816309074, 0.09473387580343959]] \n",
      " the MSE is 18.0460103986998, and the loss function is 18.059315437602095\n",
      "At iteration 2900, the gradient is [[-0.005707236254690629, -0.04832051405263147, -0.018040192323395793, 0.09209761213141383]] \n",
      " the MSE is 18.022949307074583, and the loss function is 18.036295613183015\n",
      "At iteration 3000, the gradient is [[-0.005586792054214508, -0.046533934703037905, -0.01809512585914433, 0.08954004347104107]] \n",
      " the MSE is 18.001195371923362, and the loss function is 18.014582610926897\n",
      "At iteration 3100, the gradient is [[-0.005469410185585027, -0.04480708996871729, -0.018140278824149564, 0.08705873201398018]] \n",
      " the MSE is 17.98067058183112, and the loss function is 17.99409838148682\n",
      "At iteration 3200, the gradient is [[-0.0053549909196980595, -0.04313806151253444, -0.018176072495678573, 0.08465132161343679]] \n",
      " the MSE is 17.961301690755015, and the loss function is 17.974769644948374\n",
      "At iteration 3300, the gradient is [[-0.005243443408761042, -0.04152498731683704, -0.018202909286921757, 0.0823155327441454]] \n",
      " the MSE is 17.94301992399667, and the loss function is 17.956527596510245\n",
      "At iteration 3400, the gradient is [[-0.005134682751902736, -0.03996606207098826, -0.018221175280062674, 0.08004915874152456]] \n",
      " the MSE is 17.925760702398037, and the loss function is 17.93930763040153\n",
      "At iteration 3500, the gradient is [[-0.005028628357610548, -0.03845953656275672, -0.01823124180924264, 0.07785006277147377]] \n",
      " the MSE is 17.909463383626886, and the loss function is 17.923049080904104\n",
      "At iteration 3600, the gradient is [[-0.004925203014805103, -0.037003716550056194, -0.01823346651706108, 0.07571617523107484]] \n",
      " the MSE is 17.89407101948959, and the loss function is 17.907694979418253\n",
      "At iteration 3700, the gradient is [[-0.004824332353476112, -0.03559696137420606, -0.018228194117402313, 0.07364549141420568]] \n",
      " the MSE is 17.879530128275665, and the loss function is 17.89319182657588\n",
      "At iteration 3800, the gradient is [[-0.004725944519114067, -0.03423768245597462, -0.018215756991592183, 0.07163606935093432]] \n",
      " the MSE is 17.865790481200886, and the loss function is 17.8794893784676\n",
      "At iteration 3900, the gradient is [[-0.004629969964911532, -0.03292434175109833, -0.01819647568766412, 0.06968602776982667]] \n",
      " the MSE is 17.852804902073707, and the loss function is 17.866540446107678\n",
      "At iteration 4000, the gradient is [[-0.004536341309235767, -0.03165545020640605, -0.01817065936103903, 0.06779354415446859]] \n",
      " the MSE is 17.840529079364508, and the loss function is 17.854300707315566\n",
      "At iteration 4100, the gradient is [[-0.004444993230152796, -0.03042956623857209, -0.018138606177897316, 0.06595685287750881]] \n",
      " the MSE is 17.82892138990788, and the loss function is 17.8427285302434\n",
      "At iteration 4200, the gradient is [[-0.004355862380429465, -0.02924529424645181, -0.018100603692809716, 0.06417424340255548]] \n",
      " the MSE is 17.81794273351635, and the loss function is 17.831784807827063\n",
      "At iteration 4300, the gradient is [[-0.004268887315181464, -0.028101283162677373, -0.018056929207345702, 0.06244405854755163]] \n",
      " the MSE is 17.807556377828497, and the loss function is 17.82143280248301\n",
      "At iteration 4400, the gradient is [[-0.004184008426720264, -0.026996225046605267, -0.018007850113252483, 0.06076469280565353]] \n",
      " the MSE is 17.797727812756577, and the loss function is 17.811638000415225\n",
      "At iteration 4500, the gradient is [[-0.004101167884729236, -0.025928853719639808, -0.017953624222689153, 0.05913459072030991]] \n",
      " the MSE is 17.788424613938222, and the loss function is 17.80236797493617\n",
      "At iteration 4600, the gradient is [[-0.004020309579442843, -0.02489794344218694, -0.017894500086541993, 0.057552245312502276]] \n",
      " the MSE is 17.779616314633756, and the loss function is 17.793592258242644\n",
      "At iteration 4700, the gradient is [[-0.003941379067768261, -0.023902307631901272, -0.017830717301976527, 0.056016196557874195]] \n",
      " the MSE is 17.77127428554526, and the loss function is 17.78528222112206\n",
      "At iteration 4800, the gradient is [[-0.00386432352143679, -0.022940797622028112, -0.017762506809726325, 0.05452502991209142]] \n",
      " the MSE is 17.76337162206616, and the loss function is 17.777410960097335\n",
      "At iteration 4900, the gradient is [[-0.0037890916770735297, -0.022012301458844037, -0.01769009118163418, 0.053077374882783794]] \n",
      " the MSE is 17.75588303850053, and the loss function is 17.769953191549053\n",
      "At iteration 5000, the gradient is [[-0.003715633787882096, -0.021115742736956423, -0.017613684898806718, 0.051671903646572474]] \n",
      " the MSE is 17.748784768820027, and the loss function is 17.762885152382317\n",
      "At iteration 5100, the gradient is [[-0.0036439015772135614, -0.020250079471544034, -0.017533494620848352, 0.05030732970957863]] \n",
      " the MSE is 17.742054473552958, and the loss function is 17.75618450683233\n",
      "At iteration 5200, the gradient is [[-0.0035738481930722853, -0.019414303005971755, -0.017449719446219758, 0.04898240661034988]] \n",
      " the MSE is 17.735671152425507, and the loss function is 17.749830259028254\n",
      "At iteration 5300, the gradient is [[-0.003505428164671841, -0.018607436954217387, -0.017362551164365293, 0.047695926663427334]] \n",
      " the MSE is 17.729615062398416, and the loss function is 17.74380267095828\n",
      "At iteration 5400, the gradient is [[-0.003438597359703769, -0.017828536176472665, -0.017272174499522223, 0.04644671974271434]] \n",
      " the MSE is 17.723867640764734, and the loss function is 17.73808318550109\n",
      "At iteration 5500, the gradient is [[-0.0033733129438907024, -0.01707668578757041, -0.017178767346926558, 0.04523365210282819]] \n",
      " the MSE is 17.718411432994937, and the loss function is 17.732654354209696\n",
      "At iteration 5600, the gradient is [[-0.0033095333407206205, -0.016351000196237423, -0.01708250100105049, 0.044055625237980636]] \n",
      " the MSE is 17.71323002503529, and the loss function is 17.72749976955316\n",
      "At iteration 5700, the gradient is [[-0.0032472181931598675, -0.01565062217514364, -0.016983540376712916, 0.04291157477650172]] \n",
      " the MSE is 17.708307979783434, and the loss function is 17.72260400133987\n",
      "At iteration 5800, the gradient is [[-0.0031863283263695143, -0.014974721960289183, -0.016882044223010925, 0.041800469410225166]] \n",
      " the MSE is 17.70363077748236, and the loss function is 17.717952537063233\n",
      "At iteration 5900, the gradient is [[-0.0031268257114850473, -0.014322496378755366, -0.016778165330223426, 0.040721309857688126]] \n",
      " the MSE is 17.699184759790143, and the loss function is 17.713531725926916\n",
      "At iteration 6000, the gradient is [[-0.003068673430601105, -0.013693168004213285, -0.01667205073014625, 0.03967312785986314]] \n",
      " the MSE is 17.69495707729766, and the loss function is 17.709328726321534\n",
      "At iteration 6100, the gradient is [[-0.0030118356430410163, -0.013085984339149915, -0.016563841889933126, 0.03865498520748972]] \n",
      " the MSE is 17.690935640280664, and the loss function is 17.705331456539\n",
      "At iteration 6200, the gradient is [[-0.002956277552159151, -0.012500217022790208, -0.01645367489953678, 0.0376659727991986]] \n",
      " the MSE is 17.68710907248618, and the loss function is 17.70152854852419\n",
      "At iteration 6300, the gradient is [[-0.002901965373795468, -0.011935161064344077, -0.016341680653260193, 0.0367052097290585]] \n",
      " the MSE is 17.683466667764986, and the loss function is 17.697909304475562\n",
      "At iteration 6400, the gradient is [[-0.0028488663052746886, -0.011390134100353345, -0.01622798502530235, 0.035771842402997]] \n",
      " the MSE is 17.67999834937426, and the loss function is 17.694463656118526\n",
      "At iteration 6500, the gradient is [[-0.0027969484955471884, -0.01086447567560158, -0.01611270903966173, 0.03486504368297229]] \n",
      " the MSE is 17.676694631784944, and the loss function is 17.691182126486027\n",
      "At iteration 6600, the gradient is [[-0.0027461810161903124, -0.010357546546743347, -0.01599596903448935, 0.0339840120581216]] \n",
      " the MSE is 17.67354658483895, and the loss function is 17.688055794051184\n",
      "At iteration 6700, the gradient is [[-0.0026965338333533196, -0.009868728008065606, -0.015877876821181164, 0.03312797084192988]] \n",
      " the MSE is 17.670545800110524, and the loss function is 17.685076259066253\n",
      "At iteration 6800, the gradient is [[-0.0026479777807315785, -0.009397421238551716, -0.015758539838258104, 0.03229616739468298]] \n",
      " the MSE is 17.667684359335738, and the loss function is 17.68223561197164\n",
      "At iteration 6900, the gradient is [[-0.0026004845329080165, -0.00894304666950716, -0.01563806130019514, 0.03148787237048667]] \n",
      " the MSE is 17.664954804781914, and the loss function is 17.67952640374671\n",
      "At iteration 7000, the gradient is [[-0.002554026580133671, -0.008505043372427771, -0.01551654034153743, 0.030702378987803088]] \n",
      " the MSE is 17.662350111437032, and the loss function is 17.67694161808221\n",
      "At iteration 7100, the gradient is [[-0.002508577203391816, -0.008082868466048849, -0.015394072156221498, 0.029939002323089905]] \n",
      " the MSE is 17.659863660906673, and the loss function is 17.67447464526188\n",
      "At iteration 7200, the gradient is [[-0.0024641104504922, -0.007675996542268456, -0.015270748132401462, 0.02919707862659386]] \n",
      " the MSE is 17.65748921691285, and the loss function is 17.672119257647317\n",
      "At iteration 7300, the gradient is [[-0.002420601112869873, -0.007283919110250892, -0.015146655982904489, 0.02847596465966155]] \n",
      " the MSE is 17.655220902295664, and the loss function is 17.669869586667094\n",
      "At iteration 7400, the gradient is [[-0.002378024703111369, -0.0069061440581041685, -0.015021879871402856, 0.02777503705292433]] \n",
      " the MSE is 17.65305317742501, and the loss function is 17.667720101217114\n",
      "At iteration 7500, the gradient is [[-0.0023363574330542676, -0.006542195131602629, -0.014896500534483702, 0.02709369168467223]] \n",
      " the MSE is 17.650980819935068, and the loss function is 17.665665587384954\n",
      "At iteration 7600, the gradient is [[-0.0022955761926586426, -0.006191611429418263, -0.01477059539972167, 0.026431343078786387]] \n",
      " the MSE is 17.64899890570001, and the loss function is 17.663701129416474\n",
      "At iteration 7700, the gradient is [[-0.002255658529717098, -0.00585394691444106, -0.014644238699968953, 0.025787423821516147]] \n",
      " the MSE is 17.647102790974113, and the loss function is 17.661822091847803\n",
      "At iteration 7800, the gradient is [[-0.002216582629757577, -0.00552876994037008, -0.014517501583774309, 0.02516138399677445]] \n",
      " the MSE is 17.645288095624473, and the loss function is 17.660024102730908\n",
      "At iteration 7900, the gradient is [[-0.0021783272970790344, -0.005215662793568769, -0.014390452222370849, 0.024552690638976728]] \n",
      " the MSE is 17.643550687388856, and the loss function is 17.658303037885055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 8000, the gradient is [[-0.0021408719358279814, -0.00491422124912401, -0.014263155912950628, 0.023960827203324625]] \n",
      " the MSE is 17.64188666709535, and the loss function is 17.656655006110924\n",
      "At iteration 8100, the gradient is [[-0.0021041965322065996, -0.0046240541412824075, -0.014135675178776874, 0.023385293052500863]] \n",
      " the MSE is 17.64029235478454, and the loss function is 17.655076335307932\n",
      "At iteration 8200, the gradient is [[-0.00206828163689191, -0.004344782947302384, -0.014008069865873463, 0.02282560295966208]] \n",
      " the MSE is 17.638764276678547, and the loss function is 17.653563559439093\n",
      "At iteration 8300, the gradient is [[-0.0020331083479946366, -0.004076041384504985, -0.013880397236561634, 0.022281286627039248]] \n",
      " the MSE is 17.6372991529446, and the loss function is 17.65211340629101\n",
      "At iteration 8400, the gradient is [[-0.001998658294742256, -0.00381747502018964, -0.01375271205998952, 0.021751888219594336]] \n",
      " the MSE is 17.63589388620427, and the loss function is 17.65072278598011\n",
      "At iteration 8500, the gradient is [[-0.00196491362152422, -0.003568740893828514, -0.013625066699630905, 0.021236965913405823]] \n",
      " the MSE is 17.63454555074234, and the loss function is 17.649388780159022\n",
      "At iteration 8600, the gradient is [[-0.0019318569723939193, -0.0033295071512502578, -0.01349751119792086, 0.020736091458235295]] \n",
      " the MSE is 17.633251382372258, and the loss function is 17.648108631879975\n",
      "At iteration 8700, the gradient is [[-0.0018994714762518427, -0.0030994526904704585, -0.0133700933581393, 0.020248849753796323]] \n",
      " the MSE is 17.632008768917675, and the loss function is 17.646879736074798\n",
      "At iteration 8800, the gradient is [[-0.0018677407322385442, -0.0028782668186762513, -0.013242858823552877, 0.019774838439396224]] \n",
      " the MSE is 17.630815241272163, and the loss function is 17.64569963061341\n",
      "At iteration 8900, the gradient is [[-0.0018366487957300213, -0.0026656489201145723, -0.013115851153970027, 0.01931366749645051]] \n",
      " the MSE is 17.62966846500155, and the loss function is 17.644565987905338\n",
      "At iteration 9000, the gradient is [[-0.001806180164715984, -0.0024613081345426445, -0.012989111899785851, 0.018864958863467837]] \n",
      " the MSE is 17.628566232455423, and the loss function is 17.64347660701072\n",
      "At iteration 9100, the gradient is [[-0.001776319766630795, -0.0022649630458203983, -0.012862680673564442, 0.018428346063146427]] \n",
      " the MSE is 17.627506455356546, and the loss function is 17.642429406229528\n",
      "At iteration 9200, the gradient is [[-0.001747052945523549, -0.002076341380413552, -0.01273659521925941, 0.01800347384119091]] \n",
      " the MSE is 17.626487157838774, and the loss function is 17.641422416139562\n",
      "At iteration 9300, the gradient is [[-0.0017183654497416718, -0.0018951797154529207, -0.012610891479160363, 0.01758999781644425]] \n",
      " the MSE is 17.62550646990594, and the loss function is 17.640453773055658\n",
      "At iteration 9400, the gradient is [[-0.0016902434198452728, -0.0017212231960295615, -0.012485603658594035, 0.01718758414204857]] \n",
      " the MSE is 17.624562621285776, and the loss function is 17.639521712884157\n",
      "At iteration 9500, the gradient is [[-0.0016626733769727177, -0.0015542252614735754, -0.012360764288505606, 0.01679590917722371]] \n",
      " the MSE is 17.623653935654687, and the loss function is 17.638624565348454\n",
      "At iteration 9600, the gradient is [[-0.0016356422115809434, -0.0013939473803177076, -0.012236404285961979, 0.01641465916934811]] \n",
      " the MSE is 17.622778825210634, and the loss function is 17.637760748562798\n",
      "At iteration 9700, the gradient is [[-0.0016091371725515907, -0.0012401587937106119, -0.012112553012665986, 0.016043529945997656]] \n",
      " the MSE is 17.62193578557268, and the loss function is 17.636928763932954\n",
      "At iteration 9800, the gradient is [[-0.001583145856470544, -0.0010926362668410583, -0.011989238331453624, 0.015682226616742425]] \n",
      " the MSE is 17.621123390987215, and the loss function is 17.636127191363666\n",
      "At iteration 9900, the gradient is [[-0.0015576561974814662, -0.0009511638484144887, -0.011866486661013672, 0.01533046328418532]] \n",
      " the MSE is 17.620340289822135, and the loss function is 17.635354684754244\n",
      "At iteration 0, the gradient is [[-47.11218126345606, -19.032068010198316, -11.812353343347178, -14.876146883788449]] \n",
      " the MSE is 615.2761165456094, and the loss function is 615.2761165456094\n",
      "At iteration 100, the gradient is [[-3.229826081233329, 2.486524000649172, 2.2795665158107856, 1.8725450933203414]] \n",
      " the MSE is 63.23757207103735, and the loss function is 63.242351388482284\n",
      "At iteration 200, the gradient is [[-1.7549724822841528, 1.3354088097265076, 1.2735988073379239, 1.13627303255201]] \n",
      " the MSE is 33.443442667213695, and the loss function is 33.450908498066624\n",
      "At iteration 300, the gradient is [[-0.9739347544991462, 0.6811677931966317, 0.7038599266175398, 0.7116196964701631]] \n",
      " the MSE is 24.258448351073987, and the loss function is 24.267898905745643\n",
      "At iteration 400, the gradient is [[-0.5420832610089306, 0.321280195965757, 0.3885938952957895, 0.47441548810934225]] \n",
      " the MSE is 21.377731980746002, and the loss function is 21.388440894188513\n",
      "At iteration 500, the gradient is [[-0.3032373583021291, 0.12410416646358707, 0.21404975485261776, 0.34094094100541283]] \n",
      " the MSE is 20.428588234421788, and the loss function is 20.440049966019874\n",
      "At iteration 600, the gradient is [[-0.1711010806813672, 0.016830685160874676, 0.11731739362717207, 0.26488499001253546]] \n",
      " the MSE is 20.074042413769234, and the loss function is 20.085946526938375\n",
      "At iteration 700, the gradient is [[-0.09796384506986112, -0.040792607397104025, 0.06361520121212687, 0.22064143150226614]] \n",
      " the MSE is 19.905115078565807, and the loss function is 19.917281236770986\n",
      "At iteration 800, the gradient is [[-0.057447755925671994, -0.07101842446386092, 0.033713059783538446, 0.19405129068842253]] \n",
      " the MSE is 19.796530315802855, and the loss function is 19.80885682497505\n",
      "At iteration 900, the gradient is [[-0.03496915761882885, -0.08614803652736518, 0.01697884204349962, 0.1772845027403714]] \n",
      " the MSE is 19.709761992553133, and the loss function is 19.722192614108035\n",
      "At iteration 1000, the gradient is [[-0.022465097271894, -0.09298089327465696, 0.007534016713558714, 0.16601013747674354]] \n",
      " the MSE is 19.63282286273439, and the loss function is 19.645327022949417\n",
      "At iteration 1100, the gradient is [[-0.015477735543361503, -0.09527469188332384, 0.0021281914868499694, 0.15783189501941036]] \n",
      " the MSE is 19.561855001048514, and the loss function is 19.574416413727192\n",
      "At iteration 1200, the gradient is [[-0.011542399967504724, -0.09510547754923517, -0.0010359922307601513, 0.1514231339388934]] \n",
      " the MSE is 19.495499502426355, and the loss function is 19.508109750435292\n",
      "At iteration 1300, the gradient is [[-0.009296385299329794, -0.09361960307353852, -0.002952542013667182, 0.14604865113199533]] \n",
      " the MSE is 19.43317349258555, and the loss function is 19.44582844464935\n",
      "At iteration 1400, the gradient is [[-0.007986234442890166, -0.09144946461179104, -0.004171305862217486, 0.14130028581740217]] \n",
      " the MSE is 19.374542105528683, and the loss function is 19.387239928875537\n",
      "At iteration 1500, the gradient is [[-0.007195327995865262, -0.08894334883903342, -0.004996559265272626, 0.13695074087372613]] \n",
      " the MSE is 19.319356087349394, and the loss function is 19.332096151027\n",
      "At iteration 1600, the gradient is [[-0.006693276183262403, -0.0862925076292054, -0.005596798644640262, 0.13287276324267172]] \n",
      " the MSE is 19.267401376027838, and the loss function is 19.280183648936248\n",
      "At iteration 1700, the gradient is [[-0.006352651879111056, -0.08360141278080208, -0.006065443316731868, 0.12899445955380037]] \n",
      " the MSE is 19.218483041847534, and the loss function is 19.231307764736318\n",
      "At iteration 1800, the gradient is [[-0.006102954607994823, -0.08092659663998088, -0.006454396157007849, 0.1252745899345703]] \n",
      " the MSE is 19.17241981735472, and the loss function is 19.185287326189556\n",
      "At iteration 1900, the gradient is [[-0.0059051585010043185, -0.07829812479440076, -0.006792598602415818, 0.12168890723723763]] \n",
      " the MSE is 19.129041924272222, and the loss function is 19.14195255703956\n",
      "At iteration 2000, the gradient is [[-0.005737640537868271, -0.07573146656776243, -0.007096289440818332, 0.11822260300675841]] \n",
      " the MSE is 19.088189952852733, and the loss function is 19.10114400221251\n",
      "At iteration 2100, the gradient is [[-0.00558840062751634, -0.07323405676535683, -0.007374676851442277, 0.11486612973486433]] \n",
      " the MSE is 19.04971409838879, and the loss function is 19.06271178939606\n",
      "At iteration 2200, the gradient is [[-0.00545076026994863, -0.07080892239741225, -0.007633074554650636, 0.11161288980730276]] \n",
      " the MSE is 19.01347353661716, and the loss function is 19.026515018037625\n",
      "At iteration 2300, the gradient is [[-0.005320984427017496, -0.06845668674414344, -0.00787463593184571, 0.10845795653282024]] \n",
      " the MSE is 18.9793358675227, and the loss function is 18.992421210413614\n",
      "At iteration 2400, the gradient is [[-0.00519696668361688, -0.06617667633175317, -0.008101312996174921, 0.10539736581854497]] \n",
      " the MSE is 18.94717660333293, and the loss function is 18.9603058034061\n",
      "At iteration 2500, the gradient is [[-0.0050775022717723575, -0.06396753196363848, -0.00831438679785881, 0.1024277233772145]] \n",
      " the MSE is 18.916878691295818, and the loss function is 18.930051673151834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8732/639760672.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.cat((ones_column, torch.tensor(self.X)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 2600, the gradient is [[-0.004961886109674414, -0.061827545588348944, -0.008514760881504173, 0.09954598641858058]] \n",
      " the MSE is 18.888332066750817, and the loss function is 18.90154868894217\n",
      "At iteration 2700, the gradient is [[-0.004849690532793719, -0.05975484561924628, -0.008703123734883226, 0.09674934184328608]] \n",
      " the MSE is 18.861433233753733, and the loss function is 18.874693294113776\n",
      "At iteration 2800, the gradient is [[-0.00474064237299635, -0.057747498495819616, -0.008880038800591905, 0.09403513782390038]] \n",
      " the MSE is 18.83608487121676, and the loss function is 18.84938811215564\n",
      "At iteration 2900, the gradient is [[-0.004634554965188606, -0.0558035639646668, -0.009045994433145127, 0.09140084493504196]] \n",
      " the MSE is 18.812195462857392, and the loss function is 18.825541576471455\n",
      "At iteration 3000, the gradient is [[-0.004531290523241967, -0.053921124799999116, -0.009201431705129087, 0.08884403365209366]] \n",
      " the MSE is 18.78967894944183, and the loss function is 18.80306758236247\n",
      "At iteration 3100, the gradient is [[-0.004430739306728942, -0.052098302418147675, -0.009346759960969899, 0.0863623609304881]] \n",
      " the MSE is 18.76845440194117, and the loss function is 18.78188515989256\n",
      "At iteration 3200, the gradient is [[-0.004332808072189242, -0.05033326471825723, -0.009482365591172465, 0.08395356183522121]] \n",
      " the MSE is 18.748445714321974, and the loss function is 18.7619181663803\n",
      "At iteration 3300, the gradient is [[-0.004237413658322933, -0.04862422964921631, -0.009608617052917151, 0.08161544399147634]] \n",
      " the MSE is 18.729581314780184, and the loss function is 18.743094997339405\n",
      "At iteration 3400, the gradient is [[-0.00414447941035655, -0.04696946643717773, -0.009725867810175058, 0.0793458836230431]] \n",
      " the MSE is 18.711793894304453, and the loss function is 18.725348314759746\n",
      "At iteration 3500, the gradient is [[-0.004053933175425129, -0.04536729554250068, -0.00983445811853525, 0.0771428224957307]] \n",
      " the MSE is 18.695020151524904, and the loss function is 18.708614791687932\n",
      "At iteration 3600, the gradient is [[-0.003965706166764237, -0.04381608793608999, -0.009934716166309929, 0.07500426538765839]] \n",
      " the MSE is 18.679200552867755, and the loss function is 18.69283487212898\n",
      "At iteration 3700, the gradient is [[-0.003879732309518918, -0.04231426402098183, -0.010026958855073483, 0.07292827787629831]] \n",
      " the MSE is 18.66427910709571, and the loss function is 18.67795254534911\n",
      "At iteration 3800, the gradient is [[-0.0037959478534412916, -0.04086029237841886, -0.010111492376188629, 0.07091298432550881]] \n",
      " the MSE is 18.650203153370263, and the loss function is 18.663915133715495\n",
      "At iteration 3900, the gradient is [[-0.00371429113417834, -0.03945268843707751, -0.010188612670132333, 0.06895656600710608]] \n",
      " the MSE is 18.63692316202357, and the loss function is 18.65067309325996\n",
      "At iteration 4000, the gradient is [[-0.0036347024168488884, -0.03809001311901524, -0.010258605816518883, 0.06705725932034237]] \n",
      " the MSE is 18.624392547277075, and the loss function is 18.638179826203153\n",
      "At iteration 4100, the gradient is [[-0.0035571237865619767, -0.036770871491960906, -0.010321748381821825, 0.06521335408784676]] \n",
      " the MSE is 18.612567491189576, and the loss function is 18.626391504721067\n",
      "At iteration 4200, the gradient is [[-0.0034814990652452457, -0.035493911443211186, -0.010378307739506876, 0.0634231919159211]] \n",
      " the MSE is 18.601406778160698, and the loss function is 18.61526690527913\n",
      "At iteration 4300, the gradient is [[-0.003407773743510827, -0.034257822383154245, -0.010428542370920231, 0.06168516461170424]] \n",
      " the MSE is 18.590871639356216, and the loss function is 18.604767252899578\n",
      "At iteration 4400, the gradient is [[-0.0033358949220038537, -0.033061333982569614, -0.010472702151844029, 0.059997712652206686]] \n",
      " the MSE is 18.580925606459733, and the loss function is 18.59485607476589\n",
      "At iteration 4500, the gradient is [[-0.0032658112581972314, -0.03190321494513655, -0.010511028627310804, 0.058359323702149385]] \n",
      " the MSE is 18.571534374190886, and the loss function is 18.585499062603816\n",
      "At iteration 4600, the gradient is [[-0.0031974729170311006, -0.03078227181557137, -0.010543755276340692, 0.05676853117809382]] \n",
      " the MSE is 18.562665671064117, and the loss function is 18.576663943312347\n",
      "At iteration 4700, the gradient is [[-0.0031308315241773367, -0.02969734782319237, -0.010571107767653845, 0.055223912856937445]] \n",
      " the MSE is 18.55428913789328, and the loss function is 18.568320357349375\n",
      "At iteration 4800, the gradient is [[-0.0030658401215410327, -0.02864732176027129, -0.010593304207011909, 0.053724089527110136]] \n",
      " the MSE is 18.54637621357738, and the loss function is 18.56043974440668\n",
      "At iteration 4900, the gradient is [[-0.0030024531242537757, -0.02763110689423719, -0.010610555376580802, 0.052267723681123784]] \n",
      " the MSE is 18.53890002773052, and the loss function is 18.55299523593691\n",
      "At iteration 5000, the gradient is [[-0.0029406262793161327, -0.02664764991299757, -0.01062306496679608, 0.05085351824798914]] \n",
      " the MSE is 18.5318352997452, and the loss function is 18.545961554121057\n",
      "At iteration 5100, the gradient is [[-0.0028803166255896616, -0.025695929902399415, -0.01063102980098509, 0.049480215364281925]] \n",
      " the MSE is 18.525158243903174, and the loss function is 18.539314916890348\n",
      "At iteration 5200, the gradient is [[-0.002821482455036364, -0.024774957354788605, -0.010634640052937376, 0.048146595182725065]] \n",
      " the MSE is 18.518846480170822, and the loss function is 18.53303294863887\n",
      "At iteration 5300, the gradient is [[-0.002764083275108716, -0.023883773207866915, -0.010634079457750816, 0.04685147471703377]] \n",
      " the MSE is 18.512878950337996, and the loss function is 18.527094596285668\n",
      "At iteration 5400, the gradient is [[-0.002708079772430213, -0.023021447912871567, -0.010629525516124108, 0.045593706721958595]] \n",
      " the MSE is 18.507235839179817, and the loss function is 18.52148005036527\n",
      "At iteration 5500, the gradient is [[-0.0026534337775406362, -0.02218708053127552, -0.010621149692351188, 0.0443721786073923]] \n",
      " the MSE is 18.501898500340054, and the loss function is 18.516170670844968\n",
      "At iteration 5600, the gradient is [[-0.0026001082304820257, -0.02137979785886754, -0.010609117606054283, 0.04318581138568715]] \n",
      " the MSE is 18.49684938665272, and the loss function is 18.511148917385185\n",
      "At iteration 5700, the gradient is [[-0.0025480671479201906, -0.0205987535769076, -0.01059358921817151, 0.04203355865080051]] \n",
      " the MSE is 18.492071984635796, and the loss function is 18.50639828377643\n",
      "At iteration 5800, the gradient is [[-0.0024972755907446634, -0.019843127428871284, -0.010574719010969348, 0.04091440558875259]] \n",
      " the MSE is 18.487550752906735, and the loss function is 18.501903236302287\n",
      "At iteration 5900, the gradient is [[-0.0024476996330675253, -0.019112124422621012, -0.010552656162637482, 0.03982736801803751]] \n",
      " the MSE is 18.483271064284484, and the loss function is 18.49764915579286\n",
      "At iteration 6000, the gradient is [[-0.002399306331728271, -0.018404974056691854, -0.010527544716321263, 0.038771491459406664]] \n",
      " the MSE is 18.479219151356915, and the loss function is 18.49362228314728\n",
      "At iteration 6100, the gradient is [[-0.002352063697246426, -0.01772092957047316, -0.010499523744059368, 0.03774585023380233]] \n",
      " the MSE is 18.475382055305904, and the loss function is 18.489809668117314\n",
      "At iteration 6200, the gradient is [[-0.0023059406651270856, -0.017059267217047404, -0.010468727505475738, 0.03674954658793034]] \n",
      " the MSE is 18.471747577794535, and the loss function is 18.486199121156318\n",
      "At iteration 6300, the gradient is [[-0.002260907068431155, -0.0164192855584544, -0.010435285601689673, 0.03578170984629865]] \n",
      " the MSE is 18.468304235732937, and the loss function is 18.482779168149786\n",
      "At iteration 6400, the gradient is [[-0.002216933610918891, -0.015800304782305437, -0.010399323124315013, 0.03484149558919737]] \n",
      " the MSE is 18.46504121875, and the loss function is 18.479539007854576\n",
      "At iteration 6500, the gradient is [[-0.0021739918412078275, -0.015201666039444573, -0.010360960799942158, 0.03392808485558857]] \n",
      " the MSE is 18.46194834920885, and the loss function is 18.47646847188451\n",
      "At iteration 6600, the gradient is [[-0.0021320541275172485, -0.014622730801686083, -0.010320315130034043, 0.03304068337036205]] \n",
      " the MSE is 18.459016044613442, and the loss function is 18.473557987089496\n",
      "At iteration 6700, the gradient is [[-0.0020910936333326698, -0.014062880239314986, -0.010277498526545345, 0.032178520795031]] \n",
      " the MSE is 18.456235282262984, and the loss function is 18.470798540184784\n",
      "At iteration 6800, the gradient is [[-0.0020510842936772794, -0.013521514617470701, -0.010232619443242778, 0.03134085000131799]] \n",
      " the MSE is 18.453597566019393, and the loss function is 18.468181644495296\n",
      "At iteration 6900, the gradient is [[-0.0020120007922291502, -0.012998052711122785, -0.010185782503043002, 0.03052694636674556]] \n",
      " the MSE is 18.45109489506113, and the loss function is 18.465699308688354\n",
      "At iteration 7000, the gradient is [[-0.0019738185390429618, -0.012491931237806835, -0.01013708862129469, 0.029736107091758276]] \n",
      " the MSE is 18.448719734504355, and the loss function is 18.463344007375536\n",
      "At iteration 7100, the gradient is [[-0.0019365136489145826, -0.012002604307708657, -0.010086635125236177, 0.028967650537613]] \n",
      " the MSE is 18.446464987779443, and the loss function is 18.46110865347156\n",
      "At iteration 7200, the gradient is [[-0.0019000629204984389, -0.01152954289062211, -0.01003451586977528, 0.028220915584368837]] \n",
      " the MSE is 18.44432397065769, and the loss function is 18.458986572205003\n",
      "At iteration 7300, the gradient is [[-0.0018644438160173313, -0.011072234299147336, -0.00998082134963942, 0.027495261008447756]] \n",
      " the MSE is 18.44229038682928, and the loss function is 18.456971476681698\n",
      "At iteration 7400, the gradient is [[-0.001829634441469444, -0.010630181687637755, -0.00992563880797165, 0.026790064879179287]] \n",
      " the MSE is 18.440358304939625, and the loss function is 18.45505744490788\n",
      "At iteration 7500, the gradient is [[-0.001795613527651656, -0.010202903566563054, -0.009869052341633214, 0.02610472397362545]] \n",
      " the MSE is 18.438522136996596, and the loss function is 18.453238898185518\n",
      "At iteration 7600, the gradient is [[-0.00176236041164107, -0.009789933331671464, -0.009811143003177689, 0.025438653209238788]] \n",
      " the MSE is 18.436776618066556, and the loss function is 18.4515105807976\n",
      "At iteration 7700, the gradient is [[-0.0017298550187204447, -0.009390818807478335, -0.009751988899596538, 0.024791285093850743]] \n",
      " the MSE is 18.435116787182064, and the loss function is 18.44986754090618\n",
      "At iteration 7800, the gradient is [[-0.0016980778449892223, -0.009005121804765702, -0.009691665288025177, 0.024162069192327464]] \n",
      " the MSE is 18.433537969388546, and the loss function is 18.448305112590436\n",
      "At iteration 7900, the gradient is [[-0.0016670099406013376, -0.008632417691698405, -0.009630244668512113, 0.02355047160939988]] \n",
      " the MSE is 18.43203575886171, and the loss function is 18.446818898956376\n",
      "At iteration 8000, the gradient is [[-0.0016366328931433274, -0.008272294977806297, -0.009567796873723992, 0.02295597448841211]] \n",
      " the MSE is 18.43060600303177, and the loss function is 18.44540475625426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 8100, the gradient is [[-0.0016069288120297755, -0.007924354910995298, -0.00950438915605779, 0.022378075525070616]] \n",
      " the MSE is 18.429244787653925, and the loss function is 18.44405877894303\n",
      "At iteration 8200, the gradient is [[-0.0015778803127179243, -0.007588211086537314, -0.009440086271797184, 0.021816287496258083]] \n",
      " the MSE is 18.427948422768637, and the loss function is 18.442777285645345\n",
      "At iteration 8300, the gradient is [[-0.0015494705021852433, -0.007263489068386841, -0.009374950562869874, 0.021270137802891543]] \n",
      " the MSE is 18.426713429498342, and the loss function is 18.441556805939697\n",
      "At iteration 8400, the gradient is [[-0.0015216829640193686, -0.006949826021713181, -0.009309042035822872, 0.020739168026962972]] \n",
      " the MSE is 18.425536527630534, and the loss function is 18.440394067939547\n",
      "At iteration 8500, the gradient is [[-0.001494501744662744, -0.006646870356955601, -0.009242418438496212, 0.020222933501826603]] \n",
      " the MSE is 18.424414623940184, and the loss function is 18.439285986612383\n",
      "At iteration 8600, the gradient is [[-0.0014679113396164132, -0.006354281384586351, -0.00917513533418093, 0.01972100289570338]] \n",
      " the MSE is 18.423344801207232, and the loss function is 18.438229652794394\n",
      "At iteration 8700, the gradient is [[-0.0014418966801690333, -0.006071728980448228, -0.009107246173464579, 0.019232957807824216]] \n",
      " the MSE is 18.42232430788754, and the loss function is 18.437222322859107\n",
      "At iteration 8800, the gradient is [[-0.0014164431207119034, -0.005798893261489522, -0.00903880236392842, 0.01875839237674109]] \n",
      " the MSE is 18.421350548398227, and the loss function is 18.43626140900084\n",
      "At iteration 8900, the gradient is [[-0.0013915364262492903, -0.005535464271175826, -0.008969853337483182, 0.018296912900725763]] \n",
      " the MSE is 18.42042107398067, and the loss function is 18.435344470096272\n",
      "At iteration 9000, the gradient is [[-0.0013671627601581594, -0.005281141674648839, -0.008900446615693197, 0.01784813746962201]] \n",
      " the MSE is 18.419533574106538, and the loss function is 18.434469203109416\n",
      "At iteration 9100, the gradient is [[-0.001343308672614744, -0.005035634463220889, -0.008830627873038495, 0.01741169560787594]] \n",
      " the MSE is 18.418685868394476, and the loss function is 18.4336334350076\n",
      "At iteration 9200, the gradient is [[-0.001319961089403402, -0.004798660667966598, -0.00876044099823248, 0.01698722792837428]] \n",
      " the MSE is 18.41787589900679, and the loss function is 18.43283511515777\n",
      "At iteration 9300, the gradient is [[-0.0012971073005081232, -0.004569947081801384, -0.008689928153419317, 0.016574385797025512]] \n",
      " the MSE is 18.417101723497634, and the loss function is 18.432072308174583\n",
      "At iteration 9400, the gradient is [[-0.001274734949712545, -0.004349228990402229, -0.008619129831743507, 0.016172831007264264]] \n",
      " the MSE is 18.416361508085423, and the loss function is 18.431343187193026\n",
      "At iteration 9500, the gradient is [[-0.0012528320241619617, -0.004136249911166755, -0.008548084912959648, 0.015782235464614422]] \n",
      " the MSE is 18.415653521324415, and the loss function is 18.430646027540405\n",
      "At iteration 9600, the gradient is [[-0.0012313868442972433, -0.00393076134020212, -0.008476830717337272, 0.0154022808807909]] \n",
      " the MSE is 18.414976128151427, and the loss function is 18.429979200783716\n",
      "At iteration 9700, the gradient is [[-0.0012103880540610154, -0.003732522507022392, -0.008405403057821548, 0.015032658477136628]] \n",
      " the MSE is 18.414327784285323, and the loss function is 18.429341169129994\n",
      "At iteration 9800, the gradient is [[-0.0011898246115490119, -0.0035413001368436, -0.008333836290624095, 0.01467306869699408]] \n",
      " the MSE is 18.41370703095824, and the loss function is 18.42873048015858\n",
      "At iteration 9900, the gradient is [[-0.001169685779639959, -0.0033568682199900334, -0.008262163364092932, 0.014323220926951106]] \n",
      " the MSE is 18.41311248995866, and the loss function is 18.42814576186537\n",
      "At iteration 0, the gradient is [[-47.06458921813031, -18.905835724645907, -11.873051242052263, -14.980637140522616]] \n",
      " the MSE is 616.5149274169975, and the loss function is 616.5149274169975\n",
      "At iteration 100, the gradient is [[-3.281990338932932, 2.5271687653438533, 2.306583942246084, 1.853265199207004]] \n",
      " the MSE is 64.60696574016357, and the loss function is 64.61177601730166\n",
      "At iteration 200, the gradient is [[-1.7650097355677388, 1.3377406556855491, 1.2681299654897922, 1.1220361949771405]] \n",
      " the MSE is 34.4037019046349, and the loss function is 34.41125783499209\n",
      "At iteration 300, the gradient is [[-0.9690929345842905, 0.6714223970326841, 0.6880918542214787, 0.7054766261135059]] \n",
      " the MSE is 25.306235847865388, and the loss function is 25.3157897286051\n",
      "At iteration 400, the gradient is [[-0.5343629350706801, 0.3094687454189053, 0.37117497655362075, 0.4752734430307972]] \n",
      " the MSE is 22.508331061032802, and the loss function is 22.519134135899286\n",
      "At iteration 500, the gradient is [[-0.29682909175995814, 0.11369110975173162, 0.1979721105496055, 0.34693938950201103]] \n",
      " the MSE is 21.59431813101256, and the loss function is 21.605859756105474\n",
      "At iteration 600, the gradient is [[-0.16698538481599118, 0.008600961994629801, 0.10325752966286857, 0.27431352665124975]] \n",
      " the MSE is 21.246960357179855, and the loss function is 21.25893232211796\n",
      "At iteration 700, the gradient is [[-0.09595361490709169, -0.04702327465702027, 0.05141194201169383, 0.23218550361023613]] \n",
      " the MSE is 21.073088718163696, and the loss function is 21.085314847536353\n",
      "At iteration 800, the gradient is [[-0.05704187944710256, -0.0756904254053865, 0.02298380859250823, 0.20678593621866465]] \n",
      " the MSE is 20.955151852781196, and the loss function is 20.9675343847028\n",
      "At iteration 900, the gradient is [[-0.03567394785308142, -0.08969056244401134, 0.007350614072035009, 0.19059328487431862]] \n",
      " the MSE is 20.857824627755782, and the loss function is 20.870310412870246\n",
      "At iteration 1000, the gradient is [[-0.02388981493691825, -0.09573347343370171, -0.0012887249680921611, 0.17949795100540356]] \n",
      " the MSE is 20.77032743646241, and the loss function is 20.782888138730378\n",
      "At iteration 1100, the gradient is [[-0.017342519400044953, -0.09748167151493792, -0.0061023577860072386, 0.17125362228918753]] \n",
      " the MSE is 20.68921980070221, and the loss function is 20.701840691688602\n",
      "At iteration 1200, the gradient is [[-0.013658142571168081, -0.09693363991805526, -0.008820611950086502, 0.16463156893710448]] \n",
      " the MSE is 20.61326595656532, and the loss function is 20.62593968461235\n",
      "At iteration 1300, the gradient is [[-0.011540223423102302, -0.09517920361487903, -0.010388630088889251, 0.15895881313395602]] \n",
      " the MSE is 20.541901597536658, and the loss function is 20.554624757391146\n",
      "At iteration 1400, the gradient is [[-0.010280690336208069, -0.09281202717351049, -0.011322767190220625, 0.15386592703338267]] \n",
      " the MSE is 20.474775172122804, and the loss function is 20.48754642846152\n",
      "At iteration 1500, the gradient is [[-0.009492786752985184, -0.0901548734171741, -0.011905215104379922, 0.14914930643587423]] \n",
      " the MSE is 20.411609998271306, and the loss function is 20.424429064786302\n",
      "At iteration 1600, the gradient is [[-0.008965225019275554, -0.0873826130624017, -0.012290284868837102, 0.1446959583119979]] \n",
      " the MSE is 20.352162269935977, and the loss function is 20.36502935739643\n",
      "At iteration 1700, the gradient is [[-0.0085825414778012, -0.08458939744425471, -0.012562446428463794, 0.14044242643671237]] \n",
      " the MSE is 20.296207797029204, and the loss function is 20.309123317672114\n",
      "At iteration 1800, the gradient is [[-0.008281603268331375, -0.08182533941873743, -0.012768023644698244, 0.13635235948750196]] \n",
      " the MSE is 20.24353740459464, and the loss function is 20.256501816280153\n",
      "At iteration 1900, the gradient is [[-0.008027857141543979, -0.07911654308924508, -0.012932502769345202, 0.13240425961846372]] \n",
      " the MSE is 20.19395497844091, and the loss function is 20.20696870480472\n",
      "At iteration 2000, the gradient is [[-0.007802359149709636, -0.0764760405655909, -0.013069984598322483, 0.12858479051969948]] \n",
      " the MSE is 20.147276348853666, and the loss function is 20.160339740616827\n",
      "At iteration 2100, the gradient is [[-0.007594691647165771, -0.0739097632014244, -0.013188346377740802, 0.12488512149278662]] \n",
      " the MSE is 20.10332846391262, and the loss function is 20.11644178233395\n",
      "At iteration 2200, the gradient is [[-0.0073990952810903, -0.07141980125656473, -0.013292060851103975, 0.12129892950527928]] \n",
      " the MSE is 20.061948683048676, and the loss function is 20.075112095153994\n",
      "At iteration 2300, the gradient is [[-0.007212356652013755, -0.06900618283661827, -0.01338373589370405, 0.11782130669302769]] \n",
      " the MSE is 20.022984136428864, and the loss function is 20.03619771637564\n",
      "At iteration 2400, the gradient is [[-0.007032654726614377, -0.06666784426228575, -0.013464955469359243, 0.1144481623607725]] \n",
      " the MSE is 19.98629113111776, and the loss function is 19.999554864633236\n",
      "At iteration 2500, the gradient is [[-0.0068589308149484786, -0.06440315892130682, -0.013536739041925209, 0.11117589506534786]] \n",
      " the MSE is 19.95173459614126, and the loss function is 19.965048386379713\n",
      "At iteration 2600, the gradient is [[-0.006690544462715891, -0.06221022504588859, -0.013599792624396811, 0.10800121222893455]] \n",
      " the MSE is 19.919187562295942, and the loss function is 19.932551236228427\n",
      "At iteration 2700, the gradient is [[-0.006527085478867616, -0.06008702187305688, -0.013654646038165567, 0.10492103035741233]] \n",
      " the MSE is 19.888530673902668, and the loss function is 19.901943988770753\n",
      "At iteration 2800, the gradient is [[-0.006368271230047881, -0.05803149396155777, -0.013701728028325274, 0.10193241931470476]] \n",
      " the MSE is 19.859651730276656, and the loss function is 19.87311437986906\n",
      "At iteration 2900, the gradient is [[-0.006213890498781265, -0.05604159630482406, -0.013741407437574078, 0.09903257069386581]] \n",
      " the MSE is 19.832445254982282, and the loss function is 19.845956875616555\n",
      "At iteration 3000, the gradient is [[-0.006063772771799894, -0.054115318064024004, -0.013774015840532023, 0.09621877938353436]] \n",
      " the MSE is 19.806812091128265, and the loss function is 19.8203722672861\n",
      "At iteration 3100, the gradient is [[-0.0059177714172753415, -0.052250694653544906, -0.013799860049189665, 0.09348843237574089]] \n",
      " the MSE is 19.782659021097096, and the loss function is 19.796267290697372\n",
      "At iteration 3200, the gradient is [[-0.005775754447482676, -0.05044581349240638, -0.013819229082377064, 0.09083900156284368]] \n",
      " the MSE is 19.75989840921653, and the loss function is 19.77355426852873\n",
      "At iteration 3300, the gradient is [[-0.005637599426039303, -0.04869881632306507, -0.013832398107950143, 0.0882680387460408]] \n",
      " the MSE is 19.738447865979232, and the loss function is 19.752150774189367\n",
      "At iteration 3400, the gradient is [[-0.005503190639666953, -0.04700789968083471, -0.013839630727495836, 0.08577317188404869]] \n",
      " the MSE is 19.71822993250554, and the loss function is 19.731979315951197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8732/639760672.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.cat((ones_column, torch.tensor(self.X)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 3500, the gradient is [[-0.005372417507699861, -0.04537131437765403, -0.013841180351693814, 0.08335210205057479]] \n",
      " the MSE is 19.69917178402542, and the loss function is 19.712967040118354\n",
      "At iteration 3600, the gradient is [[-0.0052451736696548295, -0.043787364471693714, -0.013837291075358263, 0.08100260080897158]] \n",
      " the MSE is 19.681204951230928, and the loss function is 19.69504545208606\n",
      "At iteration 3700, the gradient is [[-0.005121356443590085, -0.04225440597869512, -0.013828198275107556, 0.07872250784456991]] \n",
      " the MSE is 19.664265058420384, and the loss function is 19.67815015420966\n",
      "At iteration 3800, the gradient is [[-0.005000866488712663, -0.04077084546484875, -0.01381412905201926, 0.07650972876609641]] \n",
      " the MSE is 19.648291577421173, and the loss function is 19.66222059947013\n",
      "At iteration 3900, the gradient is [[-0.00488360758099668, -0.03933513859660262, -0.013795302586036905, 0.0743622330271208]] \n",
      " the MSE is 19.63322759633912, and the loss function is 19.647199859983036\n",
      "At iteration 4000, the gradient is [[-0.004769486451235166, -0.037945788687623945, -0.01377193043848672, 0.07227805194010606]] \n",
      " the MSE is 19.61901960224031, and the loss function is 19.633034409456002\n",
      "At iteration 4100, the gradient is [[-0.004658412658616604, -0.036601345264657775, -0.013744216822997965, 0.07025527676696182]] \n",
      " the MSE is 19.605617276924846, and the loss function is 19.61967391875307\n",
      "At iteration 4200, the gradient is [[-0.004550298485122863, -0.035300402663429854, -0.013712358855876518, 0.06829205687657043]] \n",
      " the MSE is 19.592973305003127, and the loss function is 19.607071063775813\n",
      "At iteration 4300, the gradient is [[-0.0044450588417984495, -0.03404159865979509, -0.01367654679193688, 0.06638659796354361]] \n",
      " the MSE is 19.58104319353267, and the loss function is 19.595181344918196\n",
      "At iteration 4400, the gradient is [[-0.004342611183407759, -0.0328236131389014, -0.013636964249482518, 0.06453716032389833]] \n",
      " the MSE is 19.56978510251829, and the loss function is 19.583962917397233\n",
      "At iteration 4500, the gradient is [[-0.004242875427879192, -0.031645166802693775, -0.01359378842613974, 0.0627420571850867]] \n",
      " the MSE is 19.55915968562063, and the loss function is 19.573376431803634\n",
      "At iteration 4600, the gradient is [[-0.004145773879993385, -0.030505019915984845, -0.01354719030696514, 0.06099965308779658]] \n",
      " the MSE is 19.549129940457476, and the loss function is 19.563384884256106\n",
      "At iteration 4700, the gradient is [[-0.004051231158183779, -0.029401971090291683, -0.013497334865438321, 0.059308362317712734]] \n",
      " the MSE is 19.53966106791934, and the loss function is 19.553953475580183\n",
      "At iteration 4800, the gradient is [[-0.003959174124015672, -0.028334856104661885, -0.013444381257860773, 0.05766664738549743]] \n",
      " the MSE is 19.530720339955963, and the loss function is 19.545049478967442\n",
      "At iteration 4900, the gradient is [[-0.0038695318137887172, -0.027302546762298256, -0.013388483011383722, 0.05607301755354881]] \n",
      " the MSE is 19.522276975322864, and the loss function is 19.53664211560374\n",
      "At iteration 5000, the gradient is [[-0.0037822353728617685, -0.026303949782513814, -0.013329788206277433, 0.0545260274076572]] \n",
      " the MSE is 19.514302022808053, and the loss function is 19.52870243778595\n",
      "At iteration 5100, the gradient is [[-0.003697217991583235, -0.025338005726322273, -0.013268439652227825, 0.05302427547266439]] \n",
      " the MSE is 19.506768251487987, and the loss function is 19.521203219075737\n",
      "At iteration 5200, the gradient is [[-0.0036144148434176454, -0.024403687955090136, -0.013204575059168765, 0.0515664028704016]] \n",
      " the MSE is 19.499650047588943, and the loss function is 19.514118851066094\n",
      "At iteration 5300, the gradient is [[-0.0035337630250554613, -0.023500001621228707, -0.013138327202796472, 0.05015109201866137]] \n",
      " the MSE is 19.492923317555654, and the loss function is 19.50742524636191\n",
      "At iteration 5400, the gradient is [[-0.00345520149817279, -0.02262598268971167, -0.013069824084790013, 0.048777065370097086]] \n",
      " the MSE is 19.48656539695303, and the loss function is 19.50109974740006\n",
      "At iteration 5500, the gradient is [[-0.0033786710330852227, -0.021780696989702992, -0.012999189088066644, 0.04744308418964589]] \n",
      " the MSE is 19.48055496484927, and the loss function is 19.49512104075679\n",
      "At iteration 5600, the gradient is [[-0.0033041141540633705, -0.02096323929528097, -0.012926541127127184, 0.04614794736940564]] \n",
      " the MSE is 19.474871963350104, and the loss function is 19.489469076611858\n",
      "At iteration 5700, the gradient is [[-0.0032314750865080357, -0.020172732434442488, -0.012851994793742707, 0.04489049027970359]] \n",
      " the MSE is 19.46949752197362, and the loss function is 19.484124993058433\n",
      "At iteration 5800, the gradient is [[-0.0031606997054043013, -0.019408326425344256, -0.012775660497974272, 0.043669583655434885]] \n",
      " the MSE is 19.46441388657392, and the loss function is 19.479071044966695\n",
      "At iteration 5900, the gradient is [[-0.0030917354855778184, -0.018669197639144857, -0.01269764460482021, 0.04248413251639984]] \n",
      " the MSE is 19.45960435253946, and the loss function is 19.47429053712664\n",
      "At iteration 6000, the gradient is [[-0.003024531453408286, -0.017954547988554666, -0.012618049566569839, 0.04133307512068972]] \n",
      " the MSE is 19.45505320200858, and the loss function is 19.46976776141232\n",
      "At iteration 6100, the gradient is [[-0.0029590381401141345, -0.01726360414130048, -0.012536974050992763, 0.04021538195007707]] \n",
      " the MSE is 19.45074564485996, and the loss function is 19.465487937724998\n",
      "At iteration 6200, the gradient is [[-0.0028952075362187373, -0.016595616757672925, -0.01245451306544565, 0.03913005472652106]] \n",
      " the MSE is 19.446667763250684, and the loss function is 19.461437158487506\n",
      "At iteration 6300, the gradient is [[-0.0028329930475552863, -0.015949859751557133, -0.012370758077127944, 0.03807612545869944]] \n",
      " the MSE is 19.442806459488104, and the loss function is 19.45760233647596\n",
      "At iteration 6400, the gradient is [[-0.002772349452761708, -0.015325629574199428, -0.012285797129568062, 0.03705265551767434]] \n",
      " the MSE is 19.439149407034623, and the loss function is 19.453971155787507\n",
      "At iteration 6500, the gradient is [[-0.0027132328617094923, -0.014722244519821142, -0.012199714955344486, 0.03605873474096004]] \n",
      " the MSE is 19.435685004456747, and the loss function is 19.450532025755372\n",
      "At iteration 6600, the gradient is [[-0.0026556006755613136, -0.014139044052723792, -0.012112593085362689, 0.035093480563863196]] \n",
      " the MSE is 19.43240233214084, and the loss function is 19.447274037633406\n",
      "At iteration 6700, the gradient is [[-0.002599411547777546, -0.013575388154940662, -0.012024509954607437, 0.03415603717750056]] \n",
      " the MSE is 19.429291111609196, and the loss function is 19.444186923883507\n",
      "At iteration 6800, the gradient is [[-0.0025446253463731945, -0.013030656693946392, -0.011935541004567445, 0.03324557471253319]] \n",
      " the MSE is 19.426341667279523, and the loss function is 19.44126101990891\n",
      "At iteration 6900, the gradient is [[-0.0024912031174990665, -0.012504248809876559, -0.011845758782483132, 0.03236128844782887]] \n",
      " the MSE is 19.42354489052092, and the loss function is 19.438487228086185\n",
      "At iteration 7000, the gradient is [[-0.002439107049959889, -0.011995582321503386, -0.01175523303742192, 0.03150239804336192]] \n",
      " the MSE is 19.420892205867826, and the loss function is 19.435856983957333\n",
      "At iteration 7100, the gradient is [[-0.002388300440672135, -0.011504093150350952, -0.01166403081323411, 0.03066814679666734]] \n",
      " the MSE is 19.418375539262172, and the loss function is 19.433362224452008\n",
      "At iteration 7200, the gradient is [[-0.002338747661623085, -0.011029234762768469, -0.011572216538753543, 0.02985780092182619]] \n",
      " the MSE is 19.415987288201475, and the loss function is 19.430995358017498\n",
      "At iteration 7300, the gradient is [[-0.00229041412729855, -0.010570477628841954, -0.011479852114937125, 0.029070648850736347]] \n",
      " the MSE is 19.41372029367817, and the loss function is 19.42874923654164\n",
      "At iteration 7400, the gradient is [[-0.002243266263520955, -0.010127308698164271, -0.011386996999401395, 0.028306000555577145]] \n",
      " the MSE is 19.41156781380239, and the loss function is 19.426617128960714\n",
      "At iteration 7500, the gradient is [[-0.0021972714770952537, -0.009699230891623908, -0.01129370828822623, 0.027563186892052432]] \n",
      " the MSE is 19.409523499006703, and the loss function is 19.424592696450798\n",
      "At iteration 7600, the gradient is [[-0.002152398126098681, -0.009285762608695642, -0.01120004079507946, 0.026841558962815596]] \n",
      " the MSE is 19.40758136873779, and the loss function is 19.422669969107336\n",
      "At iteration 7700, the gradient is [[-0.002108615491717478, -0.008886437250132148, -0.011106047128010626, 0.02614048750014882]] \n",
      " the MSE is 19.40573578954539, and the loss function is 19.420843324023295\n",
      "At iteration 7800, the gradient is [[-0.002065893750186089, -0.008500802755046732, -0.011011777763624506, 0.025459362267737706]] \n",
      " the MSE is 19.403981454484445, and the loss function is 19.419107464681645\n",
      "At iteration 7900, the gradient is [[-0.002024203946152318, -0.008128421152405675, -0.010917281119012601, 0.024797591480599034]] \n",
      " the MSE is 19.4023133637516, and the loss function is 19.417457401583246\n",
      "At iteration 8000, the gradient is [[-0.0019835179665786216, -0.007768868126227104, -0.010822603621350589, 0.024154601242794116]] \n",
      " the MSE is 19.40072680648154, and the loss function is 19.415888434035608\n",
      "At iteration 8100, the gradient is [[-0.0019438085155301544, -0.007421732594172121, -0.010727789775300861, 0.02352983500231817]] \n",
      " the MSE is 19.399217343633556, and the loss function is 19.41439613303277\n",
      "At iteration 8200, the gradient is [[-0.001905049089689042, -0.007086616299065689, -0.010632882228250057, 0.02292275302267536]] \n",
      " the MSE is 19.397780791902637, and the loss function is 19.412976325160567\n",
      "At iteration 8300, the gradient is [[-0.0018672139545790386, -0.006763133412969538, -0.010537921833469502, 0.02233283187059844]] \n",
      " the MSE is 19.396413208593668, and the loss function is 19.411625077465754\n",
      "At iteration 8400, the gradient is [[-0.0018302781215420224, -0.006450910153426455, -0.010442947711258265, 0.021759563919443958]] \n",
      " the MSE is 19.39511087740052, and the loss function is 19.4103386832308\n",
      "At iteration 8500, the gradient is [[-0.0017942173255798347, -0.006149584411589911, -0.010347997308195054, 0.021202456867690504]] \n",
      " the MSE is 19.393870295035843, and the loss function is 19.409113648600016\n",
      "At iteration 8600, the gradient is [[-0.0017590080033963863, -0.005858805391549633, -0.010253106454340207, 0.020661033272338005]] \n",
      " the MSE is 19.39268815866039, and the loss function is 19.407946680005807\n",
      "At iteration 8700, the gradient is [[-0.0017246272728385667, -0.005578233261103847, -0.010158309418845265, 0.02013483009629369]] \n",
      " the MSE is 19.391561354063697, and the loss function is 19.406834672346896\n",
      "At iteration 8800, the gradient is [[-0.001691052912159476, -0.005307538812885693, -0.010063638963533548, 0.019623398269887352]] \n",
      " the MSE is 19.39048694455104, and the loss function is 19.405774697873326\n",
      "At iteration 8900, the gradient is [[-0.0016582633406140228, -0.005046403136239156, -0.009969126394975292, 0.019126302265539307]] \n",
      " the MSE is 19.389462160494194, and the loss function is 19.404763995735703\n",
      "At iteration 9000, the gradient is [[-0.001626237598984952, -0.004794517298880799, -0.00987480161470219, 0.018643119685649367]] \n",
      " the MSE is 19.388484389506186, and the loss function is 19.403799962158917\n",
      "At iteration 9100, the gradient is [[-0.001594955331357948, -0.004551582038657859, -0.009780693168005344, 0.01817344086285231]] \n",
      " the MSE is 19.387551167202403, and the loss function is 19.402880141202612\n",
      "At iteration 9200, the gradient is [[-0.001564396766925693, -0.004317307464521271, -0.009686828290998798, 0.01771686847269103]] \n",
      " the MSE is 19.386660168512975, and the loss function is 19.402002216073253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 9300, the gradient is [[-0.0015345427026487177, -0.004091412766883586, -0.009593232956288316, 0.017273017157980634]] \n",
      " the MSE is 19.385809199513215, and the loss function is 19.40116400095464\n",
      "At iteration 9400, the gradient is [[-0.001505374486301234, -0.003873625936773387, -0.0094999319171014, 0.01684151316473115]] \n",
      " the MSE is 19.384996189741127, and the loss function is 19.400363433325754\n",
      "At iteration 9500, the gradient is [[-0.0014768740000733927, -0.003663683493641073, -0.00940694875000177, 0.01642199398919219]] \n",
      " the MSE is 19.38421918497261, and the loss function is 19.3995985667366\n",
      "At iteration 9600, the gradient is [[-0.0014490236446043443, -0.003461330221595671, -0.009314305896272669, 0.016014108035629807]] \n",
      " the MSE is 19.383476340426988, and the loss function is 19.398867564014587\n",
      "At iteration 9700, the gradient is [[-0.001421806323653586, -0.0032663189137050884, -0.00922202470191545, 0.015617514284604787]] \n",
      " the MSE is 19.382765914376975, and the loss function is 19.398168690875636\n",
      "At iteration 9800, the gradient is [[-0.0013952054291533754, -0.003078410124209457, -0.009130125456405562, 0.015231881971337662]] \n",
      " the MSE is 19.38208626213886, and the loss function is 19.397500309915632\n",
      "At iteration 9900, the gradient is [[-0.0013692048266199818, -0.002897371928292493, -0.009038627430142884, 0.014856890273959368]] \n",
      " the MSE is 19.381435830420042, and the loss function is 19.396860874959476\n",
      "At iteration 0, the gradient is [[-46.774504215297405, -18.93461757507084, -11.876132973786893, -14.921154109863943]] \n",
      " the MSE is 608.1055221184139, and the loss function is 608.1055221184139\n",
      "At iteration 100, the gradient is [[-3.270378343623901, 2.475431812050213, 2.2969026141571836, 1.882273419714938]] \n",
      " the MSE is 63.22128662179165, and the loss function is 63.226024861192435\n",
      "At iteration 200, the gradient is [[-1.7662577400871262, 1.317389249324682, 1.2617745232772744, 1.1423187721683936]] \n",
      " the MSE is 33.23839376550112, and the loss function is 33.2458498927737\n",
      "At iteration 300, the gradient is [[-0.9733821467845565, 0.6661029241846181, 0.6815601769112634, 0.7195821476910613]] \n",
      " the MSE is 24.135515556118506, and the loss function is 24.144959726989114\n",
      "At iteration 400, the gradient is [[-0.5385983444968031, 0.3109786431501137, 0.3633252124888786, 0.4850403862816199]] \n",
      " the MSE is 21.314764033656765, and the loss function is 21.32545722337598\n",
      "At iteration 500, the gradient is [[-0.30010273403379834, 0.1181913297128307, 0.18875195076958948, 0.3537848090836878]] \n",
      " the MSE is 20.38774856140525, and the loss function is 20.399183498679918\n",
      "At iteration 600, the gradient is [[-0.16922887734253023, 0.014344840371741581, 0.09295231502452313, 0.2792397983794901]] \n",
      " the MSE is 20.034885904722177, and the loss function is 20.046754752012955\n",
      "At iteration 700, the gradient is [[-0.0973637518487468, -0.04079958636955628, 0.04034987056168394, 0.2358668194736806]] \n",
      " the MSE is 19.85914847432679, and the loss function is 19.871274388978808\n",
      "At iteration 800, the gradient is [[-0.05785428825749281, -0.0693016012163703, 0.011438861862367207, 0.20966141219504045]] \n",
      " the MSE is 19.740877492267085, and the loss function is 19.75316186365544\n",
      "At iteration 900, the gradient is [[-0.03608736471892967, -0.08325388951854214, -0.004475412514996377, 0.19294296189313836]] \n",
      " the MSE is 19.643825403597184, and the loss function is 19.656214351176875\n",
      "At iteration 1000, the gradient is [[-0.02405098282545022, -0.08928462770968583, -0.013256822116908014, 0.18149809387709565]] \n",
      " the MSE is 19.556843562282225, and the loss function is 19.569308199532188\n",
      "At iteration 1100, the gradient is [[-0.017352333133516664, -0.09102603515106689, -0.018120634309179928, 0.17301448492542437]] \n",
      " the MSE is 19.476348291962193, and the loss function is 19.48887350090477\n",
      "At iteration 1200, the gradient is [[-0.01358289515050827, -0.09046780077294979, -0.020829961838518388, 0.16622216955181632]] \n",
      " the MSE is 19.401046884766462, and the loss function is 19.413625045653966\n",
      "At iteration 1300, the gradient is [[-0.011422073521339773, -0.08869912205946096, -0.022351737350421008, 0.1604227432406113]] \n",
      " the MSE is 19.33035005532646, and the loss function is 19.342977579577727\n",
      "At iteration 1400, the gradient is [[-0.010145750150197927, -0.08631553966613746, -0.023216363699277164, 0.15523123733806993]] \n",
      " the MSE is 19.263893184779818, and the loss function is 19.276568610464814\n",
      "At iteration 1500, the gradient is [[-0.009356838865853293, -0.08364199018995211, -0.02371489909994718, 0.15043459512541965]] \n",
      " the MSE is 19.20139124191676, and the loss function is 19.214114193278316\n",
      "At iteration 1600, the gradient is [[-0.008837557030914357, -0.0808550972470525, -0.024007167785164596, 0.14591407668249795]] \n",
      " the MSE is 19.142594233747726, and the loss function is 19.15536485631969\n",
      "At iteration 1700, the gradient is [[-0.008468430685948464, -0.07805021816910526, -0.024181033938001903, 0.14160271394186275]] \n",
      " the MSE is 19.08727301807904, and the loss function is 19.10009167480903\n",
      "At iteration 1800, the gradient is [[-0.008183887573101655, -0.07527820201227446, -0.024284899990395792, 0.13746198271206564]] \n",
      " the MSE is 19.035214338865003, and the loss function is 19.048081449121018\n",
      "At iteration 1900, the gradient is [[-0.007947910133266898, -0.07256554129101384, -0.02434552468305682, 0.1334690108161426]] \n",
      " the MSE is 18.986218703031483, and the loss function is 18.99913465960938\n",
      "At iteration 2000, the gradient is [[-0.007740686752106505, -0.06992541906722814, -0.02437779240820858, 0.12960956290673248]] \n",
      " the MSE is 18.940099164786638, and the loss function is 18.953064293326964\n",
      "At iteration 2100, the gradient is [[-0.007551292998137236, -0.0673637642986561, -0.024390069687720603, 0.12587419250721343]] \n",
      " the MSE is 18.89668042645042, and the loss function is 18.90969496776221\n",
      "At iteration 2200, the gradient is [[-0.0073736789159245175, -0.0648825704196539, -0.024387142209498783, 0.12225613059802734]] \n",
      " the MSE is 18.855798071979667, and the loss function is 18.868862176597972\n",
      "At iteration 2300, the gradient is [[-0.007204468926783921, -0.06248171348327085, -0.024371825352891174, 0.11875012634944214]] \n",
      " the MSE is 18.817297873694734, and the loss function is 18.83041160284061\n",
      "At iteration 2400, the gradient is [[-0.007041755525267476, -0.060159947702315086, -0.024345847422123285, 0.11535180993868482]] \n",
      " the MSE is 18.78103515120395, and the loss function is 18.794198480969865\n",
      "At iteration 2500, the gradient is [[-0.006884437845714253, -0.05791545002623575, -0.024310334121591635, 0.11205734165943991]] \n",
      " the MSE is 18.746874173755238, and the loss function is 18.76008700079295\n",
      "At iteration 2600, the gradient is [[-0.006731858964136059, -0.055746117507690644, -0.02426607439694075, 0.10886321804594304]] \n",
      " the MSE is 18.71468760133145, and the loss function is 18.727949749115588\n",
      "At iteration 2700, the gradient is [[-0.006583606989788348, -0.05364972917050481, -0.024213666399181644, 0.1057661641303281]] \n",
      " the MSE is 18.68435596130336, and the loss function is 18.697667186479165\n",
      "At iteration 2800, the gradient is [[-0.006439405958666966, -0.05162403362506404, -0.024153597717069937, 0.10276307297025711]] \n",
      " the MSE is 18.655767158092456, and the loss function is 18.669127156653868\n",
      "At iteration 2900, the gradient is [[-0.006299055964542534, -0.04966679600964517, -0.02408628956434872, 0.09985097113771424]] \n",
      " the MSE is 18.628816013629606, and the loss function is 18.64222442680309\n",
      "At iteration 3000, the gradient is [[-0.006162400286349095, -0.04777582266605842, -0.024012121197968472, 0.09702699848481153]] \n",
      " the MSE is 18.603403836609818, and the loss function is 18.616860256389234\n",
      "At iteration 3100, the gradient is [[-0.006029307318978851, -0.04594897364189221, -0.023931443491592767, 0.09428839577869633]] \n",
      " the MSE is 18.579438018703634, and the loss function is 18.592941993018968\n",
      "At iteration 3200, the gradient is [[-0.005899660621453272, -0.0441841685509468, -0.02384458655712169, 0.09163249669140228]] \n",
      " the MSE is 18.556831656018954, and the loss function is 18.570382693541337\n",
      "At iteration 3300, the gradient is [[-0.0057733534168691375, -0.042479388823933435, -0.02375186409714578, 0.08905672221646788]] \n",
      " the MSE is 18.535503194222475, and the loss function is 18.549100768818032\n",
      "At iteration 3400, the gradient is [[-0.0056502855349295465, -0.04083267801077691, -0.02365357595945053, 0.08655857645391483]] \n",
      " the MSE is 18.51537609583488, and the loss function is 18.529019650684646\n",
      "At iteration 3500, the gradient is [[-0.005530361694638139, -0.039242141044359156, -0.02355000970030571, 0.08413564318212444]] \n",
      " the MSE is 18.49637852830913, and the loss function is 18.510067479714262\n",
      "At iteration 3600, the gradient is [[-0.005413490523082157, -0.037705942963537686, -0.023441441598964236, 0.08178558289672536]] \n",
      " the MSE is 18.478443071589943, and the loss function is 18.492176812481837\n",
      "At iteration 3700, the gradient is [[-0.005299583979252313, -0.03622230736767958, -0.023328137366261367, 0.0795061301397705]] \n",
      " the MSE is 18.46150644393481, and the loss function is 18.47528434710937\n",
      "At iteration 3800, the gradient is [[-0.005188557000995336, -0.03478951475083517, -0.023210352680472893, 0.07729509102145281]] \n",
      " the MSE is 18.445509244853447, and the loss function is 18.45933066594801\n",
      "At iteration 3900, the gradient is [[-0.0050803272754078675, -0.033405900796033484, -0.023088333623683635, 0.07515034087958108]] \n",
      " the MSE is 18.430395714094736, and the loss function is 18.444259994325236\n",
      "At iteration 4000, the gradient is [[-0.004974815078240793, -0.03206985467303707, -0.022962317059022484, 0.07306982204574335]] \n",
      " the MSE is 18.416113505677064, and the loss function is 18.43001997435194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8732/639760672.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.cat((ones_column, torch.tensor(self.X)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 4100, the gradient is [[-0.004871943151993253, -0.030779817362320317, -0.022832530970891204, 0.07105154170025732]] \n",
      " the MSE is 18.402613476021088, and the loss function is 18.41656145284752\n",
      "At iteration 4200, the gradient is [[-0.004771636606598123, -0.029534280017319302, -0.02269919478070732, 0.06909356980488954]] \n",
      " the MSE is 18.389849485302825, and the loss function is 18.403838282499844\n",
      "At iteration 4300, the gradient is [[-0.004673822833178106, -0.0283317823703412, -0.022562519644855186, 0.06719403710671093]] \n",
      " the MSE is 18.377778211200223, and the loss function is 18.391807135432323\n",
      "At iteration 4400, the gradient is [[-0.004578431426251129, -0.02717091118481752, -0.022422708738950057, 0.06535113320820746]] \n",
      " the MSE is 18.366358974258155, and the loss function is 18.380427328402146\n",
      "At iteration 4500, the gradient is [[-0.004485394111297035, -0.02605029875427144, -0.02227995753053763, 0.06356310470039231]] \n",
      " the MSE is 18.355553574145247, and the loss function is 18.36966065890223\n",
      "At iteration 4600, the gradient is [[-0.004394644676484808, -0.024968621447896626, -0.022134454041775838, 0.06182825335601201]] \n",
      " the MSE is 18.345326136121525, and the loss function is 18.359471251485125\n",
      "At iteration 4700, the gradient is [[-0.004306118907217454, -0.02392459830161315, -0.02198637910279087, 0.06014493438073919]] \n",
      " the MSE is 18.335642967078442, and the loss function is 18.349825413669638\n",
      "At iteration 4800, the gradient is [[-0.004219754523526544, -0.022916989653788727, -0.021835906596486005, 0.05851155472008421]] \n",
      " the MSE is 18.32647242055277, and the loss function is 18.34069150083101\n",
      "At iteration 4900, the gradient is [[-0.004135491119385464, -0.02194459582408052, -0.02168320369507766, 0.0569265714203504]] \n",
      " the MSE is 18.317784770153377, and the loss function is 18.33203978951303\n",
      "At iteration 5000, the gradient is [[-0.0040532701044587605, -0.021006255834464287, -0.021528431088895274, 0.055388490041595184]] \n",
      " the MSE is 18.309552090875055, and the loss function is 18.323842358635567\n",
      "At iteration 5100, the gradient is [[-0.003973034647434148, -0.020100846170803148, -0.02137174320753264, 0.05389586312119704]] \n",
      " the MSE is 18.30174814780627, and the loss function is 18.316072978103936\n",
      "At iteration 5200, the gradient is [[-0.0038947296214822086, -0.019227279584059384, -0.02121328843385734, 0.052447288686106305]] \n",
      " the MSE is 18.294348291768856, and the loss function is 18.30870700435752\n",
      "At iteration 5300, the gradient is [[-0.0038183015511634995, -0.018384503929552733, -0.0210532093108871, 0.05104140881250124]] \n",
      " the MSE is 18.287329361456397, and the loss function is 18.301721282423856\n",
      "At iteration 5400, the gradient is [[-0.0037436985612408526, -0.01757150104334004, -0.020891642741969377, 0.04967690823109559]] \n",
      " the MSE is 18.280669591665124, and the loss function is 18.295094054071686\n",
      "At iteration 5500, the gradient is [[-0.003670870327086804, -0.01678728565447083, -0.020728720184408855, 0.04835251297674773]] \n",
      " the MSE is 18.274348527236732, and the loss function is 18.2888048716818\n",
      "At iteration 5600, the gradient is [[-0.0035997680265031297, -0.01603090433175022, -0.020564567836640155, 0.04706698908107321]] \n",
      " the MSE is 18.268346942356317, and the loss function is 18.282834517478527\n",
      "At iteration 5700, the gradient is [[-0.003530344293329113, -0.015301434464355935, -0.020399306819423087, 0.0458191413064004]] \n",
      " the MSE is 18.262646764870663, and the loss function is 18.277164927786757\n",
      "At iteration 5800, the gradient is [[-0.003462553172076962, -0.014597983274719709, -0.020233053350935552, 0.04460781192012742]] \n",
      " the MSE is 18.257231005313574, and the loss function is 18.271779122000762\n",
      "At iteration 5900, the gradient is [[-0.0033963500745759117, -0.013919686863134207, -0.020065918916256448, 0.043431879507806435]] \n",
      " the MSE is 18.25208369034405, and the loss function is 18.266661135970356\n",
      "At iteration 6000, the gradient is [[-0.0033316917375483517, -0.01326570928269861, -0.019898010431198928, 0.0422902578240508]] \n",
      " the MSE is 18.247189800321852, and the loss function is 18.26179595952856\n",
      "At iteration 6100, the gradient is [[-0.0032685361815355785, -0.012635241643793024, -0.01972943040077244, 0.0411818946798992]] \n",
      " the MSE is 18.24253521076201, and the loss function is 18.25716947790212\n",
      "At iteration 6200, the gradient is [[-0.003206842671454696, -0.012027501247216583, -0.019560277072476432, 0.04010577086544605]] \n",
      " the MSE is 18.238106637426213, and the loss function is 18.252768416762457\n",
      "At iteration 6300, the gradient is [[-0.003146571677903497, -0.011441730744853074, -0.01939064458447649, 0.03906089910678837]] \n",
      " the MSE is 18.233891584823933, and the loss function is 18.248580290689713\n",
      "At iteration 6400, the gradient is [[-0.0030876848401793916, -0.01087719732727115, -0.0192206231089918, 0.03804632305596197]] \n",
      " the MSE is 18.229878297910513, and the loss function is 18.244593354836873\n",
      "At iteration 6500, the gradient is [[-0.0030301449301231003, -0.010333191937161807, -0.01905029899089729, 0.037061116313016264]] \n",
      " the MSE is 18.226055716782707, and the loss function is 18.240796559594216\n",
      "At iteration 6600, the gradient is [[-0.0029739158172068843, -0.009809028507877236, -0.018879754881760406, 0.036104381479141794]] \n",
      " the MSE is 18.222413434184567, and the loss function is 18.237179508066788\n",
      "At iteration 6700, the gradient is [[-0.0029189624346711244, -0.009304043226283707, -0.018709069869456012, 0.0351752492398306]] \n",
      " the MSE is 18.218941655648482, and the loss function is 18.233732416189536\n",
      "At iteration 6800, the gradient is [[-0.002865250746876317, -0.008817593819221954, -0.018538319603557025, 0.03427287747709842]] \n",
      " the MSE is 18.21563116210675, and the loss function is 18.23044607531527\n",
      "At iteration 6900, the gradient is [[-0.002812747717429902, -0.00834905886260109, -0.01836757641648401, 0.03339645040998055]] \n",
      " the MSE is 18.21247327481984, and the loss function is 18.22731181712148\n",
      "At iteration 7000, the gradient is [[-0.0027614212786336283, -0.007897837112708975, -0.01819690944074828, 0.032545177762188265]] \n",
      " the MSE is 18.20945982247676, and the loss function is 18.224321480691195\n",
      "At iteration 7100, the gradient is [[-0.0027112403015870775, -0.007463346858747738, -0.018026384722247584, 0.03171829395627797]] \n",
      " the MSE is 18.206583110332062, and the loss function is 18.221467381632404\n",
      "At iteration 7200, the gradient is [[-0.0026621745673368093, -0.007045025296070995, -0.01785606532981526, 0.030915057333363076]] \n",
      " the MSE is 18.20383589125262, and the loss function is 18.218742283108888\n",
      "At iteration 7300, the gradient is [[-0.0026141947390249157, -0.006642327919487086, -0.017686011461179162, 0.03013474939755571]] \n",
      " the MSE is 18.201211338555087, and the loss function is 18.216139368663335\n",
      "At iteration 7400, the gradient is [[-0.0025672723348395218, -0.006254727935885299, -0.017516280545367798, 0.029376674084404624]] \n",
      " the MSE is 18.198703020522483, and the loss function is 18.21365221672107\n",
      "At iteration 7500, the gradient is [[-0.0025213797016947737, -0.005881715695606824, -0.017346927341716972, 0.028640157052576276]] \n",
      " the MSE is 18.196304876495297, and the loss function is 18.21127477666962\n",
      "At iteration 7600, the gradient is [[-0.002476489989947842, -0.005522798142024264, -0.01717800403561044, 0.0279245449979498]] \n",
      " the MSE is 18.19401119443901, and the loss function is 18.209001346415967\n",
      "At iteration 7700, the gradient is [[-0.002432577128847091, -0.005177498278713024, -0.017009560331039834, 0.027229204989474308]] \n",
      " the MSE is 18.19181658989614, and the loss function is 18.20682655132949\n",
      "At iteration 7800, the gradient is [[-0.0023896158025276514, -0.0048453546535148, -0.016841643540012476, 0.026553523826191562]] \n",
      " the MSE is 18.189715986236504, and the loss function is 18.20474532448417\n",
      "At iteration 7900, the gradient is [[-0.0023475814271524645, -0.004525920859265453, -0.016674298669090915, 0.025896907414509826]] \n",
      " the MSE is 18.187704596125133, and the loss function is 18.202752888119438\n",
      "At iteration 8000, the gradient is [[-0.002306450128442009, -0.004218765050323018, -0.016507568502943715, 0.025258780165341572]] \n",
      " the MSE is 18.185777904131715, and the loss function is 18.200844736243464\n",
      "At iteration 8100, the gradient is [[-0.0022661987199937453, -0.00392346947460023, -0.016341493685159008, 0.024638584410325598]] \n",
      " the MSE is 18.18393165041088, and the loss function is 18.199016618308132\n",
      "At iteration 8200, the gradient is [[-0.0022268046824088627, -0.003639630020599308, -0.016176112796382, 0.02403577983651271]] \n",
      " the MSE is 18.18216181538652, and the loss function is 18.1972645238888\n",
      "At iteration 8300, the gradient is [[-0.0021882461428940512, -0.003366855778804563, -0.016011462429755822, 0.023449842939082095]] \n",
      " the MSE is 18.180464605377797, and the loss function is 18.19558466830645\n",
      "At iteration 8400, the gradient is [[-0.002150501855629096, -0.0031047686172006564, -0.015847577263920175, 0.022880266491310364]] \n",
      " the MSE is 18.17883643910829, and the loss function is 18.19397347913366\n",
      "At iteration 8500, the gradient is [[-0.002113551182663642, -0.0028530027702957273, -0.01568449013351789, 0.02232655903140711]] \n",
      " the MSE is 18.177273935043388, and the loss function is 18.19242758352938\n",
      "At iteration 8600, the gradient is [[-0.0020773740754261878, -0.002611204441261286, -0.015522232097317124, 0.021788244365625088]] \n",
      " the MSE is 18.17577389950449, and the loss function is 18.19094379635109\n",
      "At iteration 8700, the gradient is [[-0.0020419510568617915, -0.002379031416865818, -0.015360832504105361, 0.02126486108706836]] \n",
      " the MSE is 18.174333315511785, and the loss function is 18.189519108996\n",
      "At iteration 8800, the gradient is [[-0.002007263204096302, -0.002156152694673712, -0.015200319056333424, 0.020755962109790523]] \n",
      " the MSE is 18.17294933231033, and the loss function is 18.188150678926053\n",
      "At iteration 8900, the gradient is [[-0.001973292131657087, -0.0019422481221418385, -0.015040717871594432, 0.020261114217666343]] \n",
      " the MSE is 18.17161925553711, and the loss function is 18.186835819834272\n",
      "At iteration 9000, the gradient is [[-0.0019400199750940152, -0.0017370080472553994, -0.01488205354205317, 0.01977989762757341]] \n",
      " the MSE is 18.170340537989187, and the loss function is 18.18557199241262\n",
      "At iteration 9100, the gradient is [[-0.0019074293753045882, -0.0015401329803801818, -0.014724349191868838, 0.019311905566374174]] \n",
      " the MSE is 18.169110770955818, and the loss function is 18.18435679568415\n",
      "At iteration 9200, the gradient is [[-0.0018755034633361708, -0.0013513332669524715, -0.014567626532720318, 0.018856743861287762]] \n",
      " the MSE is 18.167927676079433, and the loss function is 18.183187958864316\n",
      "At iteration 9300, the gradient is [[-0.0018442258453281284, -0.001170328770436471, -0.014411905917300438, 0.01841403054339518]] \n",
      " the MSE is 18.16678909771281, and the loss function is 18.18206333371881\n",
      "At iteration 9400, the gradient is [[-0.001813580588523686, -0.0009968485657524985, -0.014257206391231273, 0.017983395463449767]] \n",
      " the MSE is 18.16569299574169, and the loss function is 18.180980887387015\n",
      "At iteration 9500, the gradient is [[-0.0017835522069859036, -0.0008306306421556355, -0.014103545742970703, 0.017564479920140756]] \n",
      " the MSE is 18.16463743884395, and the loss function is 18.17993869564236\n",
      "At iteration 9600, the gradient is [[-0.0017541256485330817, -0.0006714216159529747, -0.013950940552249795, 0.017156936299865883]] \n",
      " the MSE is 18.16362059815843, and the loss function is 18.178934936562396\n",
      "At iteration 9700, the gradient is [[-0.0017252862814091875, -0.0005189764521316301, -0.01379940623667858, 0.016760427728138603]] \n",
      " the MSE is 18.162640741337977, and the loss function is 18.177967884583392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 9800, the gradient is [[-0.0016970198818001912, -0.0003730581950885878, -0.013648957096888131, 0.016374627731882054]] \n",
      " the MSE is 18.16169622696302, and the loss function is 18.177035904915567\n",
      "At iteration 9900, the gradient is [[-0.0016693126214652478, -0.0002334377078276261, -0.013499606360033932, 0.015999219912532146]] \n",
      " the MSE is 18.160785499293354, and the loss function is 18.176137448296693\n",
      "At iteration 0, the gradient is [[-47.443059450424904, -18.898130333144493, -11.752400630286004, -14.984866586558908]] \n",
      " the MSE is 623.3149268538247, and the loss function is 623.3149268538247\n",
      "At iteration 100, the gradient is [[-3.2309163941772727, 2.490033784690403, 2.3148959645774374, 1.8770188874565084]] \n",
      " the MSE is 63.158119642781344, and the loss function is 63.16298411000525\n",
      "At iteration 200, the gradient is [[-1.7440598309365716, 1.3252341327841342, 1.2873069747276862, 1.1377550314207312]] \n",
      " the MSE is 33.26401153020861, and the loss function is 33.271582831825285\n",
      "At iteration 300, the gradient is [[-0.9625924427641527, 0.6670196559600555, 0.7085472794512714, 0.7130804850648379]] \n",
      " the MSE is 24.149133159521366, and the loss function is 24.158689186176204\n",
      "At iteration 400, the gradient is [[-0.5330863230965714, 0.3073428959550883, 0.39015656008839167, 0.4770231226872619]] \n",
      " the MSE is 21.315131028729162, and the loss function is 21.325937179203954\n",
      "At iteration 500, the gradient is [[-0.2969468896943777, 0.1116907544943937, 0.21488588182664248, 0.3447273094046172]] \n",
      " the MSE is 20.383243401857587, and the loss function is 20.394793484262728\n",
      "At iteration 600, the gradient is [[-0.16707461462044024, 0.006112441877049652, 0.1182794708752397, 0.2695337286958838]] \n",
      " the MSE is 20.030632506895756, and the loss function is 20.042618308001316\n",
      "At iteration 700, the gradient is [[-0.09560415924258868, -0.05002945553911905, 0.06491532505667037, 0.2257982870700076]] \n",
      " the MSE is 19.857435040026413, and the loss function is 19.869678927294732\n",
      "At iteration 800, the gradient is [[-0.056231153197464266, -0.07906508532112339, 0.03532692891546369, 0.19942587462315797]] \n",
      " the MSE is 19.742578608677913, and the loss function is 19.7549811934243\n",
      "At iteration 900, the gradient is [[-0.03450000616360496, -0.09326394720288914, 0.018816080301479184, 0.18266817550120185]] \n",
      " the MSE is 19.64922111493421, and the loss function is 19.661727877888847\n",
      "At iteration 1000, the gradient is [[-0.022466598645483338, -0.09936679993206682, 0.009503196290915733, 0.17126481481766598]] \n",
      " the MSE is 19.565983961754885, and the loss function is 19.578565525942413\n",
      "At iteration 1100, the gradient is [[-0.01576517667433754, -0.10107615755464876, 0.004156768018101788, 0.16287228052271296]] \n",
      " the MSE is 19.489198970828017, and the loss function is 19.501839848260715\n",
      "At iteration 1200, the gradient is [[-0.011996470887187098, -0.10042493358766073, 0.001000542450133493, 0.15620053892102523]] \n",
      " the MSE is 19.417543568782083, and the loss function is 19.430235857616267\n",
      "At iteration 1300, the gradient is [[-0.009841891230890283, -0.09852857417315417, -0.0009419883766329972, 0.15053856157653372]] \n",
      " the MSE is 19.350418398664605, and the loss function is 19.36315828915928\n",
      "At iteration 1400, the gradient is [[-0.00857673475100973, -0.09599838917213044, -0.002207822106264375, 0.1454935775898787]] \n",
      " the MSE is 19.287455801184354, and the loss function is 19.30024165150957\n",
      "At iteration 1500, the gradient is [[-0.007802670046907371, -0.09316869547456455, -0.0030924125891088907, 0.1408477782013356]] \n",
      " the MSE is 19.228370199123113, and the loss function is 19.241201481453466\n",
      "At iteration 1600, the gradient is [[-0.007300724684424243, -0.09022164132560924, -0.003758462289691216, 0.13647956709937045]] \n",
      " the MSE is 19.17291196513943, and the loss function is 19.185788691964667\n",
      "At iteration 1700, the gradient is [[-0.0069504947160896205, -0.0872558015596931, -0.004295709263839543, 0.13232028172951088]] \n",
      " the MSE is 19.120852685952276, and the loss function is 19.13377510056768\n",
      "At iteration 1800, the gradient is [[-0.006685746401038563, -0.08432387232832989, -0.004753783309429227, 0.12833040766789822]] \n",
      " the MSE is 19.071980005289703, and the loss function is 19.08494841725806\n",
      "At iteration 1900, the gradient is [[-0.006470017171000817, -0.08145338458166734, -0.005160262335525355, 0.12448650542262213]] \n",
      " the MSE is 19.026095429596698, and the loss function is 19.03911012954404\n",
      "At iteration 2000, the gradient is [[-0.006283207135382574, -0.0786580855979015, -0.005530595536599522, 0.12077402428289141]] \n",
      " the MSE is 18.983013082550304, and the loss function is 18.99607430020403\n",
      "At iteration 2100, the gradient is [[-0.006114210412332893, -0.0759441922095728, -0.005873556978402904, 0.11718335143866505]] \n",
      " the MSE is 18.94255879044386, and the loss function is 18.955666676090257\n",
      "At iteration 2200, the gradient is [[-0.005956865633238872, -0.07331382583026361, -0.006194242921906767, 0.11370763909105035]] \n",
      " the MSE is 18.904569305561015, and the loss function is 18.917723924047564\n",
      "At iteration 2300, the gradient is [[-0.005807730492872543, -0.07076689879191582, -0.006495719304213913, 0.1103416087072522]] \n",
      " the MSE is 18.868891604987603, and the loss function is 18.882092936314717\n",
      "At iteration 2400, the gradient is [[-0.005664858701472216, -0.0683021496477095, -0.006779927406717375, 0.10708089231512406]] \n",
      " the MSE is 18.835382242799287, and the loss function is 18.848630186048716\n",
      "At iteration 2500, the gradient is [[-0.005527127805650803, -0.06591771083328465, -0.007048181852068184, 0.10392166897736123]] \n",
      " the MSE is 18.803906746462765, and the loss function is 18.817201125294318\n",
      "At iteration 2600, the gradient is [[-0.005393869740459873, -0.06361141937810091, -0.007301444557377671, 0.10086046352969978]] \n",
      " the MSE is 18.774339052601167, and the loss function is 18.78767962136377\n",
      "At iteration 2700, the gradient is [[-0.00526466774749688, -0.06138098645009383, -0.0075404755554557374, 0.09789403453865282]] \n",
      " the MSE is 18.746560978850237, and the loss function is 18.759947428799848\n",
      "At iteration 2800, the gradient is [[-0.0051392447214154025, -0.05922408936133909, -0.007765916140664046, 0.0950193113353411]] \n",
      " the MSE is 18.720461729200128, and the loss function is 18.73389369456134\n",
      "At iteration 2900, the gradient is [[-0.005017401802018091, -0.05713842099955376, -0.007978334815394875, 0.09223335806353192]] \n",
      " the MSE is 18.695937430564094, and the loss function is 18.709414494305356\n",
      "At iteration 3000, the gradient is [[-0.00489898458009796, -0.05512171589911019, -0.008178252785513932, 0.08953335261640699]] \n",
      " the MSE is 18.672890698536133, and the loss function is 18.68641239780094\n",
      "At iteration 3100, the gradient is [[-0.0047838644801895215, -0.05317176350947592, -0.008366158209176437, 0.08691657379693159]] \n",
      " the MSE is 18.65123023046477, and the loss function is 18.664796061639795\n",
      "At iteration 3200, the gradient is [[-0.0046719284848623635, -0.051286414461734174, -0.00854251425734182, 0.08438039303797541]] \n",
      " the MSE is 18.630870424105936, and the loss function is 18.64447984752772\n",
      "At iteration 3300, the gradient is [[-0.004563073444889985, -0.04946358302012364, -0.008707763766193624, 0.08192226866716304]] \n",
      " the MSE is 18.611731020236707, and the loss function is 18.62538346454892\n",
      "At iteration 3400, the gradient is [[-0.004457202910665413, -0.04770124746869323, -0.008862332009431682, 0.07953974160807878]] \n",
      " the MSE is 18.59373676771827, and the loss function is 18.607431633896585\n",
      "At iteration 3500, the gradient is [[-0.004354225350410635, -0.04599744939383017, -0.009006628430327625, 0.07723043190747637]] \n",
      " the MSE is 18.576817109594323, and the loss function is 18.590553774658037\n",
      "At iteration 3600, the gradient is [[-0.004254053132216586, -0.044350292390008796, -0.00914104779560377, 0.07499203575166202]] \n",
      " the MSE is 18.560905888901306, and the loss function is 18.574683709331328\n",
      "At iteration 3700, the gradient is [[-0.004156601926438613, -0.0427579404770063, -0.009265971024818756, 0.07282232278626885]] \n",
      " the MSE is 18.54594107295097, and the loss function is 18.559759387833463\n",
      "At iteration 3800, the gradient is [[-0.00406179034104793, -0.04121861638672551, -0.009381765835352943, 0.07071913363572131]] \n",
      " the MSE is 18.531864494924022, and the loss function is 18.54572262883826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8732/639760672.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.cat((ones_column, torch.tensor(self.X)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 3900, the gradient is [[-0.003969539685490518, -0.03973059980513157, -0.009488787279835726, 0.06868037756476529]] \n",
      " the MSE is 18.518621611686964, and the loss function is 18.53251887735498\n",
      "At iteration 4000, the gradient is [[-0.0038797738067970506, -0.03829222561564999, -0.009587378218568502, 0.06670403024913664]] \n",
      " the MSE is 18.50616127681258, and the loss function is 18.520096977527277\n",
      "At iteration 4100, the gradient is [[-0.003792418966034849, -0.036901882168710355, -0.009677869750576131, 0.06478813163633917]] \n",
      " the MSE is 18.494435527848825, and the loss function is 18.50840895969606\n",
      "At iteration 4200, the gradient is [[-0.003707403738475379, -0.035558009590070126, -0.009760581616355011, 0.06293078388508523]] \n",
      " the MSE is 18.483399386940775, and the loss function is 18.49740984083008\n",
      "At iteration 4300, the gradient is [[-0.0036246589274119673, -0.03425909813412943, -0.009835822579712305, 0.06113014937626597]] \n",
      " the MSE is 18.473010673966808, and the loss function is 18.487057437484395\n",
      "At iteration 4400, the gradient is [[-0.003544117486818678, -0.03300368658496903, -0.009903890792993576, 0.05938444879046084]] \n",
      " the MSE is 18.463229831402796, and the loss function is 18.47731219049957\n",
      "At iteration 4500, the gradient is [[-0.003465714449285585, -0.031790360705589674, -0.009965074148013736, 0.057691959248683124]] \n",
      " the MSE is 18.454019760177353, and the loss function is 18.4681370007039\n",
      "At iteration 4600, the gradient is [[-0.003389386858536355, -0.030617751735460313, -0.010019650614520913, 0.056051012513206946]] \n",
      " the MSE is 18.445345665827887, and the loss function is 18.459497074927516\n",
      "At iteration 4700, the gradient is [[-0.003315073704738328, -0.029484534935117795, -0.01006788856689685, 0.054459993246451364]] \n",
      " the MSE is 18.437174914310223, and the loss function is 18.451359781680452\n",
      "At iteration 4800, the gradient is [[-0.003242715862526843, -0.028389428176913815, -0.010110047099967184, 0.05291733732566092]] \n",
      " the MSE is 18.42947689685543, and the loss function is 18.443694515887593\n",
      "At iteration 4900, the gradient is [[-0.0031722560310714224, -0.027331190580472463, -0.01014637633433659, 0.05142153021158321]] \n",
      " the MSE is 18.422222903305443, and the loss function is 18.436472572111455\n",
      "At iteration 5000, the gradient is [[-0.003103638676723961, -0.02630862119186275, -0.010177117711875662, 0.04997110536912209]] \n",
      " the MSE is 18.415386003395113, and the loss function is 18.429667025729774\n",
      "At iteration 5100, the gradient is [[-0.0030368099769059473, -0.025320557704614725, -0.010202504281404937, 0.04856464273866298]] \n",
      " the MSE is 18.408940935481333, and the loss function is 18.423252621568054\n",
      "At iteration 5200, the gradient is [[-0.002971717766423641, -0.02436587522191514, -0.01022276097531676, 0.047200767255955046]] \n",
      " the MSE is 18.402864002251548, and the loss function is 18.41720566951877\n",
      "At iteration 5300, the gradient is [[-0.0029083114850609808, -0.02344348505807939, -0.010238104877056089, 0.045878147419437035]] \n",
      " the MSE is 18.39713297297341, and the loss function is 18.41150394670855\n",
      "At iteration 5400, the gradient is [[-0.0028465421272172195, -0.022552333578566827, -0.010248745480092714, 0.04459549390311353]] \n",
      " the MSE is 18.391726991874542, and the loss function is 18.40612660580182\n",
      "At iteration 5500, the gradient is [[-0.0027863621928998594, -0.02169140107692887, -0.010254884938416739, 0.043351558213793266]] \n",
      " the MSE is 18.38662649226768, and the loss function is 18.401054089055688\n",
      "At iteration 5600, the gradient is [[-0.0027277256403690294, -0.020859700687752145, -0.010256718308982312, 0.042145131391117335]] \n",
      " the MSE is 18.381813116060087, and the loss function is 18.39626804776465\n",
      "At iteration 5700, the gradient is [[-0.002670587840239611, -0.02005627733427319, -0.010254433786245035, 0.04097504274911374]] \n",
      " the MSE is 18.37726963830937, and the loss function is 18.391751266756792\n",
      "At iteration 5800, the gradient is [[-0.0026149055312169947, -0.019280206709804116, -0.010248212929221932, 0.03984015865778186]] \n",
      " the MSE is 18.372979896508703, and the loss function is 18.38748759362416\n",
      "At iteration 5900, the gradient is [[-0.0025606367770025385, -0.018530594291563514, -0.010238230881077035, 0.03873938136368515]] \n",
      " the MSE is 18.368928724304386, and the loss function is 18.383461872389926\n",
      "At iteration 6000, the gradient is [[-0.002507740924593201, -0.01780657438611586, -0.010224656581656683, 0.03767164784809218]] \n",
      " the MSE is 18.36510188936766, and the loss function is 18.379659881333886\n",
      "At iteration 6100, the gradient is [[-0.0024561785642380728, -0.017107309205432984, -0.010207652973191164, 0.03663592872148913]] \n",
      " the MSE is 18.361486035159707, and the loss function is 18.376068274715006\n",
      "At iteration 6200, the gradient is [[-0.0024059114901184347, -0.016431987972300867, -0.010187377199205894, 0.03563122715349637]] \n",
      " the MSE is 18.358068626345567, and the loss function is 18.372674528146465\n",
      "At iteration 6300, the gradient is [[-0.002356902662757906, -0.015779826054545604, -0.010163980797118884, 0.034656577836768766]] \n",
      " the MSE is 18.35483789762782, and the loss function is 18.369466887393717\n",
      "At iteration 6400, the gradient is [[-0.002309116172390406, -0.015150064126842125, -0.010137609884529261, 0.0337110459840195]] \n",
      " the MSE is 18.35178280578534, and the loss function is 18.366434320380726\n",
      "At iteration 6500, the gradient is [[-0.002262517203500612, -0.014541967359347645, -0.010108405339464926, 0.032793726357024024]] \n",
      " the MSE is 18.348892984715988, and the loss function is 18.363566472203033\n",
      "At iteration 6600, the gradient is [[-0.0022170720004916697, -0.013954824632276557, -0.010076502974789766, 0.03190374232660506]] \n",
      " the MSE is 18.346158703294574, and the loss function is 18.360853622958654\n",
      "At iteration 6700, the gradient is [[-0.002172747834577674, -0.01338794777567139, -0.010042033706985278, 0.03104024496253993]] \n",
      " the MSE is 18.343570825869538, and the loss function is 18.358286648220243\n",
      "At iteration 6800, the gradient is [[-0.0021295129715539736, -0.012840670833408396, -0.010005123719419183, 0.030202412152543566]] \n",
      " the MSE is 18.341120775232547, and the loss function is 18.355856981982377\n",
      "At iteration 6900, the gradient is [[-0.0020873366405874424, -0.01231234935076837, -0.009965894620304757, 0.02938944774935769]] \n",
      " the MSE is 18.338800497905993, and the loss function is 18.3535565819289\n",
      "At iteration 7000, the gradient is [[-0.0020461890043013595, -0.011802359684897425, -0.009924463595614687, 0.028600580744933726]] \n",
      " the MSE is 18.336602431602802, and the loss function is 18.3513778968745\n",
      "At iteration 7100, the gradient is [[-0.002006041129235222, -0.011310098337111838, -0.009880943556874319, 0.027835064471138184]] \n",
      " the MSE is 18.334519474722292, and the loss function is 18.349313836244182\n",
      "At iteration 7200, the gradient is [[-0.0019668649578274723, -0.010834981306803977, -0.009835443284314712, 0.027092175825747935]] \n",
      " the MSE is 18.332544957754344, and the loss function is 18.347357741462694\n",
      "At iteration 7300, the gradient is [[-0.0019286332809531613, -0.010376443465844829, -0.009788067565252167, 0.026371214523264654]] \n",
      " the MSE is 18.330672616472103, and the loss function is 18.345503359134053\n",
      "At iteration 7400, the gradient is [[-0.001891319711407661, -0.009933937953135257, -0.00973891732806973, 0.025671502369506077]] \n",
      " the MSE is 18.328896566801035, and the loss function is 18.343744815898834\n",
      "At iteration 7500, the gradient is [[-0.0018548986582006643, -0.009506935588379383, -0.009688089771717798, 0.024992382559459427]] \n",
      " the MSE is 18.327211281259203, and the loss function is 18.34207659486405\n",
      "At iteration 7600, the gradient is [[-0.001819345301769418, -0.009094924304787132, -0.00963567849113681, 0.02433321899736923]] \n",
      " the MSE is 18.32561156687017, and the loss function is 18.340493513506825\n",
      "At iteration 7700, the gradient is [[-0.0017846355698692635, -0.00869740859978214, -0.009581773598483372, 0.023693395638622675]] \n",
      " the MSE is 18.324092544456114, and the loss function is 18.338990702959435\n",
      "At iteration 7800, the gradient is [[-0.0017507461142847367, -0.008313909003392381, -0.009526461840485063, 0.023072315852542134]] \n",
      " the MSE is 18.322649629224767, and the loss function is 18.337563588589198\n",
      "At iteration 7900, the gradient is [[-0.0017176542882117452, -0.007943961563560802, -0.009469826711908266, 0.02246940180554286]] \n",
      " the MSE is 18.32127851256884, and the loss function is 18.33620787179182\n",
      "At iteration 8000, the gradient is [[-0.0016853381244318023, -0.007587117348026612, -0.009411948565400263, 0.021884093863901506]] \n",
      " the MSE is 18.319975145002022, and the loss function is 18.334919512922184\n",
      "At iteration 8100, the gradient is [[-0.0016537763140735766, -0.007242941962019705, -0.009352904717650858, 0.0213158500156375]] \n",
      " the MSE is 18.31873572016031, and the loss function is 18.333694715291255\n",
      "At iteration 8200, the gradient is [[-0.0016229481863268107, -0.006911015081597612, -0.009292769552242819, 0.02076414531067469]] \n",
      " the MSE is 18.31755665980178, and the loss function is 18.332529910162226\n",
      "At iteration 8300, the gradient is [[-0.001592833688292735, -0.006590930001643049, -0.00923161461895012, 0.020228471319063922]] \n",
      " the MSE is 18.31643459974235, and the loss function is 18.33142174268327\n",
      "At iteration 8400, the gradient is [[-0.0015634133660144861, -0.006282293198593795, -0.00916950872998377, 0.019708335606250283]] \n",
      " the MSE is 18.315366376668738, and the loss function is 18.330367058698148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 8500, the gradient is [[-0.0015346683457220667, -0.005984723906964296, -0.009106518052937993, 0.019203261225244884]] \n",
      " the MSE is 18.314349015773747, and the loss function is 18.329362892379713\n",
      "At iteration 8600, the gradient is [[-0.0015065803158416952, -0.005697853709558634, -0.009042706200813107, 0.01871278622485735]] \n",
      " the MSE is 18.31337971916231, and the loss function is 18.328406454634685\n",
      "At iteration 8700, the gradient is [[-0.001479131509569517, -0.005421326140746125, -0.00897813431903523, 0.01823646317364985]] \n",
      " the MSE is 18.312455854979966, and the loss function is 18.327495122231316\n",
      "At iteration 8800, the gradient is [[-0.0014523046879518094, -0.005154796302432658, -0.008912861169646069, 0.017773858699065174]] \n",
      " the MSE is 18.31157494721861, and the loss function is 18.326626427604793\n",
      "At iteration 8900, the gradient is [[-0.0014260831234152633, -0.004897930492319582, -0.008846943212719383, 0.017324553041243553]] \n",
      " the MSE is 18.310734666156957, and the loss function is 18.325798049297735\n",
      "At iteration 9000, the gradient is [[-0.0014004505840921823, -0.004650405844122789, -0.008780434685156562, 0.016888139621007808]] \n",
      " the MSE is 18.309932819396174, and the loss function is 18.325007802996197\n",
      "At iteration 9100, the gradient is [[-0.0013753913183729765, -0.0044119099792578375, -0.008713387676857011, 0.01646422462166095]] \n",
      " the MSE is 18.30916734345311, and the loss function is 18.32425363312362\n",
      "At iteration 9200, the gradient is [[-0.0013508900399701478, -0.004182140669657116, -0.008645852204397462, 0.016052426584119456]] \n",
      " the MSE is 18.308436295876593, and the loss function is 18.32353360495812\n",
      "At iteration 9300, the gradient is [[-0.0013269319136384968, -0.003960805511445062, -0.008577876282311995, 0.015652376014891427]] \n",
      " the MSE is 18.307737847853684, and the loss function is 18.322845897240004\n",
      "At iteration 9400, the gradient is [[-0.00130350254119568, -0.0037476216090568722, -0.008509505992043042, 0.015263715006563717]] \n",
      " the MSE is 18.30707027727551, and the loss function is 18.32218879523911\n",
      "At iteration 9500, the gradient is [[-0.001280587947910941, -0.0035423152693797987, -0.00844078554856661, 0.014886096870433817]] \n",
      " the MSE is 18.306431962233724, and the loss function is 18.321560684252955\n",
      "At iteration 9600, the gradient is [[-0.0012581745696126803, -0.0033446217058466184, -0.008371757364924497, 0.01451918578072734]] \n",
      " the MSE is 18.305821374920797, and the loss function is 18.320960043508926\n",
      "At iteration 9700, the gradient is [[-0.001236249239867289, -0.003154284751853444, -0.008302462114542827, 0.014162656430270629]] \n",
      " the MSE is 18.305237075908753, and the loss function is 18.32038544044508\n",
      "At iteration 9800, the gradient is [[-0.001214799177737844, -0.002971056583439512, -0.00823293879155285, 0.013816193697058596]] \n",
      " the MSE is 18.304677708782783, and the loss function is 18.31983552534596\n",
      "At iteration 9900, the gradient is [[-0.0011938119759684854, -0.0027946974508560375, -0.008163224769126568, 0.013479492321442237]] \n",
      " the MSE is 18.304141995107475, and the loss function is 18.31930902631116\n",
      "At iteration 0, the gradient is [[-47.102549580736515, -19.048158664022683, -11.915110804898589, -15.025325980288054]] \n",
      " the MSE is 614.9805095104819, and the loss function is 614.9805095104819\n",
      "At iteration 100, the gradient is [[-3.2426247688603116, 2.499711950638657, 2.3001497169637015, 1.8203026787253207]] \n",
      " the MSE is 63.058569463825144, and the loss function is 63.06334818675565\n",
      "At iteration 200, the gradient is [[-1.7511175893731532, 1.3353372212218004, 1.2740662856252232, 1.0980381444343459]] \n",
      " the MSE is 33.37953852794455, and the loss function is 33.387006651508905\n",
      "At iteration 300, the gradient is [[-0.9654596131710466, 0.6789961522578938, 0.6976110026131347, 0.6844703279861988]] \n",
      " the MSE is 24.364489651096314, and the loss function is 24.37392621697498\n",
      "At iteration 400, the gradient is [[-0.5341427765540324, 0.3205138427503168, 0.3809202945593183, 0.45503901902522126]] \n",
      " the MSE is 21.580192378930086, and the loss function is 21.590865606294553\n",
      "At iteration 500, the gradient is [[-0.29728333327453205, 0.12549974574487271, 0.20684895639393674, 0.3267773566826317]] \n",
      " the MSE is 20.67760737415587, and the loss function is 20.689013818975287\n",
      "At iteration 600, the gradient is [[-0.16716720875331229, 0.02015884829211809, 0.1110748610822196, 0.25411992692169216]] \n",
      " the MSE is 20.345960794757893, and the loss function is 20.35779422716508\n",
      "At iteration 700, the gradient is [[-0.09564684493290335, -0.036014248732348944, 0.05829021127891062, 0.21205298427850852]] \n",
      " the MSE is 20.190003743395152, and the loss function is 20.202087762193187\n",
      "At iteration 800, the gradient is [[-0.05629325742252192, -0.0652518024019635, 0.029113642859849954, 0.1868439958215701]] \n",
      " the MSE is 20.09038784951841, and the loss function is 20.102623741253396\n",
      "At iteration 900, the gradient is [[-0.034599108434723407, -0.07975572883004393, 0.012906058034869383, 0.17095291811906463]] \n",
      " the MSE is 20.010926488832467, and the loss function is 20.023260047316388\n",
      "At iteration 1000, the gradient is [[-0.022601029605136387, -0.0862221961184988, 0.003827130129140292, 0.1602389202839712]] \n",
      " the MSE is 19.94051246087743, and the loss function is 19.952914405581154\n",
      "At iteration 1100, the gradient is [[-0.01592773590085334, -0.08832523822215062, -0.0013293392890329839, 0.1524271112080244]] \n",
      " the MSE is 19.875612254424166, and the loss function is 19.888067074652117\n",
      "At iteration 1200, the gradient is [[-0.012179691901141472, -0.08808004545951524, -0.004323625078878144, 0.14626690167027465]] \n",
      " the MSE is 19.81498833729089, and the loss function is 19.827488050650008\n",
      "At iteration 1300, the gradient is [[-0.010039686740668536, -0.0865912931254888, -0.006122301910075469, 0.14107001505120567]] \n",
      " the MSE is 19.7581069565043, and the loss function is 19.77064765173337\n",
      "At iteration 1400, the gradient is [[-0.008784615559212956, -0.08446390853251591, -0.007256259629901016, 0.13645689280330411]] \n",
      " the MSE is 19.704657040302884, and the loss function is 19.717236974338352\n",
      "At iteration 1500, the gradient is [[-0.008017489099520507, -0.0820285455936161, -0.008017223516176437, 0.13221749092239368]] \n",
      " the MSE is 19.654404245781965, and the loss function is 19.667022801475603\n",
      "At iteration 1600, the gradient is [[-0.007520329773207279, -0.07946534969905773, -0.00856562747785147, 0.128234867375412]] \n",
      " the MSE is 19.607146202748478, and the loss function is 19.619803321006987\n",
      "At iteration 1700, the gradient is [[-0.0071734240875998435, -0.07687189268617077, -0.008989827575226707, 0.12444323646015502]] \n",
      " the MSE is 19.562698380841333, and the loss function is 19.575394253537315\n",
      "At iteration 1800, the gradient is [[-0.006910999055472489, -0.07430046199638227, -0.009338605515943135, 0.12080494262249106]] \n",
      " the MSE is 19.520889269725274, and the loss function is 19.5336241756246\n",
      "At iteration 1900, the gradient is [[-0.006696892266494493, -0.07177852793787959, -0.009639010616052029, 0.11729781949184026]] \n",
      " the MSE is 19.481558425329148, and the loss function is 19.494332644268102\n",
      "At iteration 2000, the gradient is [[-0.006511196774897319, -0.06931997720400145, -0.009906153819043714, 0.11390824956444473]] \n",
      " the MSE is 19.444555424474, and the loss function is 19.457369194210038\n",
      "At iteration 2100, the gradient is [[-0.006342930226732248, -0.06693127789353583, -0.01014858407166629, 0.11062735310304443]] \n",
      " the MSE is 19.409739129000897, and the loss function is 19.42259262521454\n",
      "At iteration 2200, the gradient is [[-0.006186010782965591, -0.06461486239644454, -0.010371239781179363, 0.10744889475504008]] \n",
      " the MSE is 19.376977073299088, and the loss function is 19.389870402044913\n",
      "At iteration 2300, the gradient is [[-0.006037048192306285, -0.06237098316196127, -0.010577069189917984, 0.10436813309608213]] \n",
      " the MSE is 19.346144915412076, and the loss function is 19.359078112160194\n",
      "At iteration 2400, the gradient is [[-0.00589413122882441, -0.06019873024580941, -0.01076792009049411, 0.10138118780184156]] \n",
      " the MSE is 19.317125931030976, and the loss function is 19.330098963098777\n",
      "At iteration 2500, the gradient is [[-0.005756162049637683, -0.05809658878023993, -0.010945028462611523, 0.09848469099533008]] \n",
      " the MSE is 19.289810542107137, and the loss function is 19.302823312739573\n",
      "At iteration 2600, the gradient is [[-0.0056224907671563375, -0.05606274393496203, -0.011109286943956088, 0.09567559462238695]] \n",
      " the MSE is 19.264095875941276, and the loss function is 19.27714822909762\n",
      "At iteration 2700, the gradient is [[-0.005492714814683171, -0.05409524730480117, -0.011261392440779115, 0.09295106351237115]] \n",
      " the MSE is 19.239885352092294, and the loss function is 19.25297707743544\n",
      "At iteration 2800, the gradient is [[-0.0053665687710374044, -0.05219210726420953, -0.011401927388752512, 0.09030841551044455]] \n",
      " the MSE is 19.21708829506084, and the loss function is 19.23021913288485\n",
      "At iteration 2900, the gradient is [[-0.0052438638407626015, -0.050351337616697224, -0.011531404585971225, 0.08774508748477014]] \n",
      " the MSE is 19.19561957100642, and the loss function is 19.208789216968754\n",
      "At iteration 3000, the gradient is [[-0.005124454592484482, -0.04857098338168152, -0.011650292023029819, 0.08525861557212175]] \n",
      " the MSE is 19.175399246941062, and the loss function is 19.18860735653696\n",
      "At iteration 3100, the gradient is [[-0.005008220660806117, -0.04684913406034712, -0.011759026726057498, 0.08284662327364015]] \n",
      " the MSE is 19.15635227097477, and the loss function is 19.169598463729617\n",
      "At iteration 3200, the gradient is [[-0.004895056663278373, -0.04518393005665528, -0.01185802256212905, 0.08050681389272427]] \n",
      " the MSE is 19.138408172293463, and the loss function is 19.151692035669242\n",
      "At iteration 3300, the gradient is [[-0.004784866627881375, -0.04357356536802999, -0.011947674723863412, 0.07823696538871815]] \n",
      " the MSE is 19.12150077964119, and the loss function is 19.13482187266381\n",
      "At iteration 3400, the gradient is [[-0.004677560897669437, -0.042016288254731554, -0.012028362384896433, 0.07603492658770966]] \n",
      " the MSE is 19.105567957158886, and the loss function is 19.118925813778315\n",
      "At iteration 3500, the gradient is [[-0.004573054396109249, -0.04051040082494199, -0.012100450345055557, 0.0738986141686093]] \n",
      " the MSE is 19.090551356505436, and the loss function is 19.103945488702973\n",
      "At iteration 3600, the gradient is [[-0.0044712656408339825, -0.03905425804944177, -0.012164290115111356, 0.0718260101038881]] \n",
      " the MSE is 19.07639618425474, and the loss function is 19.08982608491251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8732/639760672.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.cat((ones_column, torch.tensor(self.X)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 3700, the gradient is [[-0.004372116168467608, -0.0376462664866816, -0.01222022068794148, 0.06981515937846318]] \n",
      " the MSE is 19.06305098362521, and the loss function is 19.07651612917295\n",
      "At iteration 3800, the gradient is [[-0.004275530187448024, -0.03628488287243459, -0.012268569132247576, 0.06786416788833896]] \n",
      " the MSE is 19.050467429656827, and the loss function is 19.063967282510546\n",
      "At iteration 3900, the gradient is [[-0.004181434355453002, -0.034968612656833244, -0.01230965108305808, 0.06597120046498285]] \n",
      " the MSE is 19.038600137005933, and the loss function is 19.05213414781238\n",
      "At iteration 4000, the gradient is [[-0.004089757627523803, -0.033696008534748585, -0.012343771170700926, 0.06413447899403193]] \n",
      " the MSE is 19.02740647957901, and the loss function is 19.04097408927916\n",
      "At iteration 4100, the gradient is [[-0.0040004311430392285, -0.032465668993039806, -0.012371223410631101, 0.06235228061104559]] \n",
      " the MSE is 19.01684642127507, and the loss function is 19.03044706299899\n",
      "At iteration 4200, the gradient is [[-0.00391338813542306, -0.031276236887609485, -0.012392291566998422, 0.06062293596354382]] \n",
      " the MSE is 19.006882357151337, and the loss function is 19.020515457956147\n",
      "At iteration 4300, the gradient is [[-0.0038285638550372295, -0.030126398056386266, -0.012407249496971109, 0.05894482753290545]] \n",
      " the MSE is 18.99747896436913, and the loss function is 19.0111439468309\n",
      "At iteration 4400, the gradient is [[-0.003745895500251056, -0.029014879971134847, -0.012416361479924939, 0.05731638801174176]] \n",
      " the MSE is 18.988603062316812, and the loss function is 19.002299345986675\n",
      "At iteration 4500, the gradient is [[-0.003665322153583087, -0.02794045042885233, -0.0124198825337104, 0.055736098733844074]] \n",
      " the MSE is 18.98022348134359, and the loss function is 18.99395048407759\n",
      "At iteration 4600, the gradient is [[-0.0035867847217933985, -0.026901916283045134, -0.012418058719717824, 0.05420248815402518]] \n",
      " the MSE is 18.972310939573145, and the loss function is 18.986068078744765\n",
      "At iteration 4700, the gradient is [[-0.0035102258786812368, -0.025898122213937884, -0.01241112743737776, 0.052714130376166964]] \n",
      " the MSE is 18.96483792729885, and the loss function is 18.978624620902558\n",
      "At iteration 4800, the gradient is [[-0.003435590010199919, -0.024927949536984993, -0.012399317708954008, 0.051269643727570806]] \n",
      " the MSE is 18.957778598492812, and the loss function is 18.971594266146422\n",
      "At iteration 4900, the gradient is [[-0.0033628231614269972, -0.023990315048495303, -0.012382850454943594, 0.049867689378159545]] \n",
      " the MSE is 18.95110866899032, and the loss function is 18.96495273284347\n",
      "At iteration 5000, the gradient is [[-0.0032918729856887007, -0.02308416990760528, -0.012361938760683403, 0.04850697000285563]] \n",
      " the MSE is 18.944805320937785, and the loss function is 18.958677206493306\n",
      "At iteration 5100, the gradient is [[-0.003222688695010236, -0.02220849855310509, -0.012336788134228891, 0.047186228486041634]] \n",
      " the MSE is 18.938847113118285, and the loss function is 18.95274624997281\n",
      "At iteration 5200, the gradient is [[-0.0031552210122001456, -0.02136231765439238, -0.01230759675602028, 0.045904246666522014]] \n",
      " the MSE is 18.933213896791965, and the loss function is 18.947139719301752\n",
      "At iteration 5300, the gradient is [[-0.0030894221246060742, -0.020544675095361807, -0.01227455572051836, 0.04465984412180268]] \n",
      " the MSE is 18.92788673671167, and the loss function is 18.941838684589097\n",
      "At iteration 5400, the gradient is [[-0.0030252456392097326, -0.019754648990263704, -0.012237849270127436, 0.043451876990412544]] \n",
      " the MSE is 18.922847836994467, and the loss function is 18.936825355840412\n",
      "At iteration 5500, the gradient is [[-0.0029626465392168077, -0.018991346730461132, -0.012197655021578421, 0.04227923683110635]] \n",
      " the MSE is 18.918080471549924, and the loss function is 18.93208301332683\n",
      "At iteration 5600, the gradient is [[-0.0029015811417951467, -0.01825390406111443, -0.01215414418504389, 0.04114084951783232]] \n",
      " the MSE is 18.913568918784193, and the loss function is 18.927595942234323\n",
      "At iteration 5700, the gradient is [[-0.002842007057383216, -0.01754148418701697, -0.012107481776294074, 0.04003567416917905]] \n",
      " the MSE is 18.90929840031637, and the loss function is 18.92334937132938\n",
      "At iteration 5800, the gradient is [[-0.002783883150067592, -0.016853276906397142, -0.012057826821956807, 0.038962702111448595]] \n",
      " the MSE is 18.90525502345988, and the loss function is 18.9193294153937\n",
      "At iteration 5900, the gradient is [[-0.002727169499288476, -0.01618849777207093, -0.012005332558266303, 0.0379209558740972]] \n",
      " the MSE is 18.901425727236894, and the loss function is 18.91552302119549\n",
      "At iteration 6000, the gradient is [[-0.0026718273627018246, -0.015546387278934093, -0.011950146623399207, 0.036909488216643555]] \n",
      " the MSE is 18.897798231708006, and the loss function is 18.91191791677943\n",
      "At iteration 6100, the gradient is [[-0.002617819140255915, -0.014926210077005874, -0.01189241124363724, 0.03592738118603375]] \n",
      " the MSE is 18.894360990413034, and the loss function is 18.908502563870933\n",
      "At iteration 6200, the gradient is [[-0.0025651083391814503, -0.014327254209185115, -0.01183226341352829, 0.03497374520352411]] \n",
      " the MSE is 18.891103145731137, and the loss function is 18.905266113202632\n",
      "At iteration 6300, the gradient is [[-0.002513659540504633, -0.013748830373121837, -0.011769835070346868, 0.03404771818002942]] \n",
      " the MSE is 18.888014486980516, and the loss function is 18.902198362583228\n",
      "At iteration 6400, the gradient is [[-0.0024634383659575434, -0.013190271206065612, -0.011705253262782536, 0.0331484646593483]] \n",
      " the MSE is 18.88508541108898, and the loss function is 18.89928971753969\n",
      "At iteration 6500, the gradient is [[-0.0024144114464007826, -0.012650930592379235, -0.011638640314324336, 0.03227517498804152]] \n",
      " the MSE is 18.882306885676893, and the loss function is 18.89653115437429\n",
      "At iteration 6600, the gradient is [[-0.002366546391256497, -0.012130182992901233, -0.011570113981418427, 0.03142706451125044]] \n",
      " the MSE is 18.87967041440412, and the loss function is 18.893914185487745\n",
      "At iteration 6700, the gradient is [[-0.0023198117584928278, -0.011627422795195515, -0.011499787606404537, 0.03060337279382718]] \n",
      " the MSE is 18.877168004441458, and the loss function is 18.89143082682897\n",
      "At iteration 6800, the gradient is [[-0.0022741770258026973, -0.011142063684328626, -0.011427770265574429, 0.02980336286575499]] \n",
      " the MSE is 18.874792135935813, and the loss function is 18.88907356734044\n",
      "At iteration 6900, the gradient is [[-0.0022296125627017643, -0.010673538033532909, -0.011354166912522204, 0.029026320491124788]] \n",
      " the MSE is 18.87253573334632, and the loss function is 18.886835340276267\n",
      "At iteration 7000, the gradient is [[-0.002186089603462298, -0.010221296313977994, -0.011279078516831693, 0.02827155345999713]] \n",
      " the MSE is 18.87039213853634, and the loss function is 18.884709496277814\n",
      "At iteration 7100, the gradient is [[-0.002143580220692269, -0.009784806522993773, -0.011202602198201406, 0.027538390902530047]] \n",
      " the MSE is 18.86835508551318, and the loss function is 18.88268977809855\n",
      "At iteration 7200, the gradient is [[-0.002102057300043785, -0.009363553630529795, -0.011124831356409699, 0.026826182624360997]] \n",
      " the MSE is 18.86641867671418, and the loss function is 18.880770296876648\n",
      "At iteration 7300, the gradient is [[-0.0020614945154040203, -0.00895703904276775, -0.01104585579690125, 0.026134298462965336]] \n",
      " the MSE is 18.864577360744068, and the loss function is 18.878945509860234\n",
      "At iteration 7400, the gradient is [[-0.002021866305131823, -0.008564780082840182, -0.010965761852470477, 0.025462127663987328]] \n",
      " the MSE is 18.8628259114742, and the loss function is 18.87721019949568\n",
      "At iteration 7500, the gradient is [[-0.0019831478488392005, -0.008186309487758748, -0.0108846325009059, 0.02480907827717017]] \n",
      " the MSE is 18.861159408419987, and the loss function is 18.875559453795205\n",
      "At iteration 7600, the gradient is [[-0.001945315044820739, -0.007821174921155428, -0.010802547478841578, 0.024174576571175894]] \n",
      " the MSE is 18.859573218317976, and the loss function is 18.873988647905232\n",
      "At iteration 7700, the gradient is [[-0.0019083444885942232, -0.0074689385015391605, -0.01071958339201025, 0.023558066466588]] \n",
      " the MSE is 18.858062977828556, and the loss function is 18.872493426801256\n",
      "At iteration 7800, the gradient is [[-0.0018722134514959873, -0.007129176345109312, -0.010635813821755442, 0.022959008986833347]] \n",
      " the MSE is 18.856624577295428, and the loss function is 18.87106968904043\n",
      "At iteration 7900, the gradient is [[-0.001836899860423965, -0.006801478123247544, -0.010551309428264287, 0.022376881726088677]] \n",
      " the MSE is 18.855254145496662, and the loss function is 18.869713571506516\n",
      "At iteration 8000, the gradient is [[-0.0018023822780591412, -0.006485446633781627, -0.010466138050362185, 0.021811178333895877]] \n",
      " the MSE is 18.853948035326386, and the loss function is 18.868421435086255\n",
      "At iteration 8100, the gradient is [[-0.0017686398835226663, -0.006180697385746901, -0.010380364802047896, 0.021261408015938763]] \n",
      " the MSE is 18.852702810350074, and the loss function is 18.86718985122003\n",
      "At iteration 8200, the gradient is [[-0.0017356524540207002, -0.005886858197335508, -0.010294052165991632, 0.02072709505027591]] \n",
      " the MSE is 18.851515232179654, and the loss function is 18.866015589272987\n",
      "At iteration 8300, the gradient is [[-0.0017034003464580691, -0.005603568806280881, -0.010207260083826863, 0.020207778318926572]] \n",
      " the MSE is 18.850382248618086, and the loss function is 18.864895604676228\n",
      "At iteration 8400, the gradient is [[-0.0016718644804160476, -0.005330480492835697, -0.010120046043732376, 0.01970301085379369]] \n",
      " the MSE is 18.84930098252618, and the loss function is 18.863827027790794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 8500, the gradient is [[-0.0016410263209213093, -0.005067255714315885, -0.01003246516495413, 0.019212359397039226]] \n",
      " the MSE is 18.84826872136734, and the loss function is 18.862807153450053\n",
      "At iteration 8600, the gradient is [[-0.001610867862017001, -0.004813567751251124, -0.009944570279639634, 0.0187354039751221]] \n",
      " the MSE is 18.84728290738855, and the loss function is 18.861833431138834\n",
      "At iteration 8700, the gradient is [[-0.0015813716110615575, -0.004569100364826781, -0.009856412012114091, 0.018271737486013278]] \n",
      " the MSE is 18.846341128398617, and the loss function is 18.8609034557702\n",
      "At iteration 8800, the gradient is [[-0.001552520573138985, -0.004333547464872452, -0.009768038855383618, 0.017820965299513773]] \n",
      " the MSE is 18.845441109106993, and the loss function is 18.8600149590232\n",
      "At iteration 8900, the gradient is [[-0.0015242982362658262, -0.004106612788530323, -0.009679497245311813, 0.01738270486987761]] \n",
      " the MSE is 18.844580702988797, and the loss function is 18.859165801207197\n",
      "At iteration 9000, the gradient is [[-0.0014966885567860638, -0.003888009588843958, -0.009590831632233591, 0.016956585360688912]] \n",
      " the MSE is 18.84375788464382, and the loss function is 18.858353963620466\n",
      "At iteration 9100, the gradient is [[-0.0014696759453991658, -0.003677460333337179, -0.009502084550337115, 0.01654224728132208]] \n",
      " the MSE is 18.842970742619123, and the loss function is 18.857577541372684\n",
      "At iteration 9200, the gradient is [[-0.0014432452535336681, -0.0034746964119541706, -0.00941329668469315, 0.01613934213486308]] \n",
      " the MSE is 18.842217472666896, and the loss function is 18.856834736642995\n",
      "At iteration 9300, the gradient is [[-0.0014173817601891609, -0.003279457854280516, -0.009324506936141935, 0.015747532076954385]] \n",
      " the MSE is 18.8414963714109, and the loss function is 18.85612385234684\n",
      "At iteration 9400, the gradient is [[-0.0013920711590983338, -0.0030914930556469, -0.009235752484034144, 0.015366489585325521]] \n",
      " the MSE is 18.840805830396462, and the loss function is 18.855443286186585\n",
      "At iteration 9500, the gradient is [[-0.0013672995464541306, -0.002910558511884317, -0.009147068846921059, 0.01499589713962192]] \n",
      " the MSE is 18.840144330500507, and the loss function is 18.854791525062396\n",
      "At iteration 9600, the gradient is [[-0.0013430534088401222, -0.002736418562411447, -0.009058489941238822, 0.014635446911245992]] \n",
      " the MSE is 18.839510436679554, and the loss function is 18.85416713982125\n",
      "At iteration 9700, the gradient is [[-0.0013193196115329074, -0.0025688451413891527, -0.00897004813804518, 0.014284840462915595]] \n",
      " the MSE is 18.838902793035203, and the loss function is 18.853568780323574\n",
      "At iteration 9800, the gradient is [[-0.0012960853872554354, -0.002407617536804821, -0.008881774317943014, 0.013943788457524788]] \n",
      " the MSE is 18.83832011817733, and the loss function is 18.852995170807777\n",
      "At iteration 9900, the gradient is [[-0.0012733383253364402, -0.002252522157105223, -0.008793697924167466, 0.013612010376130874]] \n",
      " the MSE is 18.837761200867163, and the loss function is 18.852445105534755\n",
      "At iteration 0, the gradient is [[-46.8776203852691, -19.194334300283305, -11.787962196151721, -15.024704469151386]] \n",
      " the MSE is 610.582180083853, and the loss function is 610.582180083853\n",
      "At iteration 100, the gradient is [[-3.2731101718748503, 2.4732180173744522, 2.3112217432863713, 1.8585546789770242]] \n",
      " the MSE is 63.4604235370005, and the loss function is 63.465171558607764\n",
      "At iteration 200, the gradient is [[-1.7629748695583456, 1.308357432005478, 1.2711267641751443, 1.1262114034787396]] \n",
      " the MSE is 33.563694479436066, and the loss function is 33.571157798603224\n",
      "At iteration 300, the gradient is [[-0.9685955499218576, 0.6553967843981425, 0.6900216522632178, 0.7091224287238814]] \n",
      " the MSE is 24.536504399229727, and the loss function is 24.545946787968447\n",
      "At iteration 400, the gradient is [[-0.5342549644489073, 0.30042917897775395, 0.37209477383217243, 0.47840070787785477]] \n",
      " the MSE is 21.752042028229987, and the loss function is 21.76272371921203\n",
      "At iteration 500, the gradient is [[-0.29669964450284425, 0.10832921152818022, 0.19807171477413263, 0.3496624312971497]] \n",
      " the MSE is 20.83864921917073, and the loss function is 20.85006484722432\n",
      "At iteration 600, the gradient is [[-0.16672580080889413, 0.005203033056886863, 0.10272991411336069, 0.27675465587998493]] \n",
      " the MSE is 20.48937867824224, and the loss function is 20.501222811370024\n",
      "At iteration 700, the gradient is [[-0.0955674584504491, -0.04934372495567934, 0.05041302400785975, 0.23444406214036595]] \n",
      " the MSE is 20.3132781451463, and the loss function is 20.32537600130783\n",
      "At iteration 800, the gradient is [[-0.056565139450502316, -0.07739195175167848, 0.02162800072156991, 0.20893377724683615]] \n",
      " the MSE is 20.193198747750127, and the loss function is 20.205453250778785\n",
      "At iteration 900, the gradient is [[-0.03514455411241162, -0.09101097839472588, 0.005717974147496715, 0.19267907963075875]] \n",
      " the MSE is 20.093854350662937, and the loss function is 20.106212690796063\n",
      "At iteration 1000, the gradient is [[-0.02333814921006474, -0.09679707250613222, -0.003143350411496937, 0.1815532446833883]] \n",
      " the MSE is 20.00444930723232, and the loss function is 20.01688332358331\n",
      "At iteration 1100, the gradient is [[-0.01679014634692557, -0.0983545420317322, -0.008141474230021904, 0.17329816634802614]] \n",
      " the MSE is 19.921526919070757, and the loss function is 19.93402198197043\n",
      "At iteration 1200, the gradient is [[-0.013119255418811978, -0.09765210399440738, -0.011018216208458633, 0.16667734042405172]] \n",
      " the MSE is 19.843840267607654, and the loss function is 19.85638908925853\n",
      "At iteration 1300, the gradient is [[-0.011023602283552184, -0.09576438284830292, -0.012726150281172483, 0.16101281523270414]] \n",
      " the MSE is 19.77081718049608, and the loss function is 19.783416403597332\n",
      "At iteration 1400, the gradient is [[-0.009791410303614868, -0.0932772713646395, -0.013786373834761497, 0.15593204972972974]] \n",
      " the MSE is 19.702099981334758, and the loss function is 19.71474831102658\n",
      "At iteration 1500, the gradient is [[-0.009033499531214238, -0.09050953184309493, -0.014484093027407004, 0.15122952394034309]] \n",
      " the MSE is 19.637406819516155, and the loss function is 19.65010400604504\n",
      "At iteration 1600, the gradient is [[-0.008537049226121734, -0.0876339395823343, -0.014975545794296512, 0.1467910822844339]] \n",
      " the MSE is 19.576489328777757, and the loss function is 19.589235617003563\n",
      "At iteration 1700, the gradient is [[-0.008185652648735492, -0.0847435081330099, -0.01534645490318259, 0.14255257179214934]] \n",
      " the MSE is 19.519119213034774, and the loss function is 19.531915047372795\n",
      "At iteration 1800, the gradient is [[-0.007915612533498175, -0.08188769189009969, -0.015643983432559624, 0.13847722871510762]] \n",
      " the MSE is 19.465083569545634, and the loss function is 19.477929438329028\n",
      "At iteration 1900, the gradient is [[-0.007692048630408573, -0.07909217593418229, -0.0158942045362692, 0.13454331456866572]] \n",
      " the MSE is 19.414182889085307, and the loss function is 19.427079244811072\n",
      "At iteration 2000, the gradient is [[-0.007495836114461028, -0.07636969338868588, -0.01611165214344246, 0.13073735538780962]] \n",
      " the MSE is 19.36622990658465, and the loss function is 19.37917712729226\n",
      "At iteration 2100, the gradient is [[-0.007316465019135057, -0.07372593773947408, -0.01630454245185421, 0.12705044419928785]] \n",
      " the MSE is 19.321048747078308, and the loss function is 19.33404711987462\n",
      "At iteration 2200, the gradient is [[-0.0071481365781855145, -0.07116279370779234, -0.01647762871938731, 0.12347621814224705]] \n",
      " the MSE is 19.27847419514539, and the loss function is 19.291523911514993\n",
      "At iteration 2300, the gradient is [[-0.006987629123420167, -0.06868010226917713, -0.01663376221686981, 0.12000975113288424]] \n",
      " the MSE is 19.238351032489312, and the loss function is 19.25145218974609\n",
      "At iteration 2400, the gradient is [[-0.006833131364752806, -0.0662766243568926, -0.016774745855302607, 0.11664694708471289]] \n",
      " the MSE is 19.2005334241935, and the loss function is 19.21368603000714\n",
      "At iteration 2500, the gradient is [[-0.006683604521357706, -0.06395056653974401, -0.016901801123469354, 0.1133842068164153]] \n",
      " the MSE is 19.164884345553325, and the loss function is 19.178088323892847\n",
      "At iteration 2600, the gradient is [[-0.006538433565814293, -0.061699867275447934, -0.017015823620886023, 0.1102182446223928]] \n",
      " the MSE is 19.13127504517113, and the loss function is 19.144530242776273\n",
      "At iteration 2700, the gradient is [[-0.006397236522661242, -0.05952235231072534, -0.01711752301181918, 0.10714598670203711]] \n",
      " the MSE is 19.099584541386026, and the loss function is 19.112890734296588\n",
      "At iteration 2800, the gradient is [[-0.00625976017333247, -0.0574158185802312, -0.017207499786319833, 0.10416451437974295]] \n",
      " the MSE is 19.069699149698682, and the loss function is 19.08305604960005\n",
      "At iteration 2900, the gradient is [[-0.00612582299903544, -0.055378079050032926, -0.01728628746699013, 0.10127103184994979]] \n",
      " the MSE is 19.041512039158256, and the loss function is 19.05491929942515\n",
      "At iteration 3000, the gradient is [[-0.005995283948903134, -0.0534069862419691, -0.017354375917887684, 0.09846284736725132]] \n",
      " the MSE is 19.01492281587376, and the loss function is 19.028380037260998\n",
      "At iteration 3100, the gradient is [[-0.0058680253275129966, -0.05150044413415654, -0.017412224314981486, 0.09573736182304692]] \n",
      " the MSE is 18.98983713195827, and the loss function is 19.003343867922975\n",
      "At iteration 3200, the gradient is [[-0.005743943402706846, -0.04965641373688469, -0.01746026845760921, 0.09309206139556561]] \n",
      " the MSE is 18.966166318334395, and the loss function is 18.979722079992815\n",
      "At iteration 3300, the gradient is [[-0.0056229432345945695, -0.04787291523969935, -0.01749892497918779, 0.09052451246110771]] \n",
      " the MSE is 18.943827039934018, and the loss function is 18.957431300665615\n",
      "At iteration 3400, the gradient is [[-0.0055049358136158005, -0.0461480283122107, -0.017528593856145108, 0.08803235777451092]] \n",
      " the MSE is 18.922740971919634, and the loss function is 18.9363931716355\n",
      "At iteration 3500, the gradient is [[-0.005389836461937829, -0.0444798914226543, -0.01754965997993725, 0.08561331337568245]] \n",
      " the MSE is 18.902834495641052, and the loss function is 18.91653404473567\n",
      "At iteration 3600, the gradient is [[-0.005277563926574612, -0.04286670064585041, -0.017562494210568793, 0.08326516592416286]] \n",
      " the MSE is 18.884038413120862, and the loss function is 18.89778469612643\n",
      "At iteration 3700, the gradient is [[-0.005168039851564074, -0.041306708217451214, -0.017567454140479534, 0.08098577029793345]] \n",
      " the MSE is 18.866287678936803, and the loss function is 18.880080057899114\n",
      "At iteration 3800, the gradient is [[-0.005061188458489262, -0.03979822097424979, -0.01756488469418836, 0.0787730473658663]] \n",
      " the MSE is 18.849521148438512, and the loss function is 18.863358966032568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8732/639760672.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.cat((ones_column, torch.tensor(self.X)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 3900, the gradient is [[-0.004956936341588017, -0.038339598756025536, -0.017555118632304134, 0.0766249818835513]] \n",
      " the MSE is 18.833681341301485, and the loss function is 18.847563923704115\n",
      "At iteration 4000, the gradient is [[-0.004855212326892633, -0.03692925280975846, -0.017538476997817732, 0.07453962048386417]] \n",
      " the MSE is 18.81871421948176, and the loss function is 18.832640879017596\n",
      "At iteration 4100, the gradient is [[-0.004755947366844319, -0.03556564421742135, -0.017515269525316957, 0.07251506974606989]] \n",
      " the MSE is 18.80456897869254, and the loss function is 18.818539016268684\n",
      "At iteration 4200, the gradient is [[-0.004659074455206667, -0.034247282358441974, -0.017485795024688434, 0.07054949433367087]] \n",
      " the MSE is 18.79119785257739, and the loss function is 18.805210559921168\n",
      "At iteration 4300, the gradient is [[-0.0045645285542364, -0.032972723412356394, -0.017450341745896422, 0.06864111519467805]] \n",
      " the MSE is 18.77855592880511, and the loss function is 18.79261059051843\n",
      "At iteration 4400, the gradient is [[-0.004472246529011, -0.03174056890372199, -0.017409187728413687, 0.06678820782035806]] \n",
      " the MSE is 18.766600976358923, and the loss function is 18.78069687180188\n",
      "At iteration 4500, the gradient is [[-0.004382167086626007, -0.030549464290013442, -0.017362601137531825, 0.06498910055934422]] \n",
      " the MSE is 18.755293283336847, and the loss function is 18.76942968835238\n",
      "At iteration 4600, the gradient is [[-0.004294230718791235, -0.029398097592170137, -0.017310840588888977, 0.06324217298476288]] \n",
      " the MSE is 18.744595504621888, and the loss function is 18.758771693112582\n",
      "At iteration 4700, the gradient is [[-0.0042083796470565975, -0.02828519806710982, -0.017254155462105864, 0.061545854312341955]] \n",
      " the MSE is 18.734472518820063, and the loss function is 18.748687764187427\n",
      "At iteration 4800, the gradient is [[-0.0041245577701448715, -0.027209534921107673, -0.01719278620409072, 0.05989862186779253]] \n",
      " the MSE is 18.724891293900612, and the loss function is 18.73914487035653\n",
      "At iteration 4900, the gradient is [[-0.004042710613106025, -0.02616991606304593, -0.017126964622557885, 0.058298999601755684]] \n",
      " the MSE is 18.715820761007897, and the loss function is 18.730111944767263\n",
      "At iteration 5000, the gradient is [[-0.00396278527829877, -0.025165186896360714, -0.017056914170122794, 0.05674555665080349]] \n",
      " the MSE is 18.70723169594626, and the loss function is 18.721559766309294\n",
      "At iteration 5100, the gradient is [[-0.003884730397786078, -0.024194229148469592, -0.016982850219276854, 0.05523690594307734]] \n",
      " the MSE is 18.69909660787005, and the loss function is 18.713460848202185\n",
      "At iteration 5200, the gradient is [[-0.003808496087255103, -0.02325595973662425, -0.01690498032860564, 0.05377170284710949]] \n",
      " the MSE is 18.691389634739355, and the loss function is 18.705789333356048\n",
      "At iteration 5300, the gradient is [[-0.0037340339016223544, -0.022349329669139134, -0.016823504500581465, 0.052348643862417034]] \n",
      " the MSE is 18.684086445128884, and the loss function is 18.698520896092276\n",
      "At iteration 5400, the gradient is [[-0.003661296791520724, -0.021473322980560608, -0.016738615430981034, 0.050966465350833136]] \n",
      " the MSE is 18.67716414600264, and the loss function is 18.69163264983651\n",
      "At iteration 5500, the gradient is [[-0.0035902390616140084, -0.020626955700270722, -0.016650498750537722, 0.049623942306905484]] \n",
      " the MSE is 18.67060119609056, and the loss function is 18.68510306041957\n",
      "At iteration 5600, the gradient is [[-0.0035208163299542414, -0.019809274853009567, -0.016559333258784517, 0.0483198871664937]] \n",
      " the MSE is 18.66437732452588, and the loss function is 18.678911864644736\n",
      "At iteration 5700, the gradient is [[-0.0034529854885303603, -0.01901935749055254, -0.01646529115045951, 0.047053148652222945]] \n",
      " the MSE is 18.65847345442217, and the loss function is 18.67303999379992\n",
      "At iteration 5800, the gradient is [[-0.0033867046651558192, -0.018256309753505912, -0.01636853823467503, 0.045822610654698566]] \n",
      " the MSE is 18.652871631089393, and the loss function is 18.667469501813784\n",
      "At iteration 5900, the gradient is [[-0.003321933186527984, -0.017519265962439245, -0.01626923414717414, 0.04462719114825898]] \n",
      " the MSE is 18.647554954605873, and the loss function is 18.662183497772315\n",
      "At iteration 6000, the gradient is [[-0.003258631542198669, -0.01680738773706117, -0.016167532555651707, 0.04346584114046867]] \n",
      " the MSE is 18.642507516481107, and the loss function is 18.657166082530487\n",
      "At iteration 6100, the gradient is [[-0.0031967613501093627, -0.01611986314317902, -0.01606358135873939, 0.04233754365386656]] \n",
      " the MSE is 18.637714340159924, and the loss function is 18.652402289169274\n",
      "At iteration 6200, the gradient is [[-0.0031362853224642844, -0.015455905865786584, -0.01595752287835863, 0.041241312739557855]] \n",
      " the MSE is 18.633161325134044, and the loss function is 18.6478780270637\n",
      "At iteration 6300, the gradient is [[-0.00307716723344634, -0.014814754408312541, -0.01584949404618116, 0.040176192521002733]] \n",
      " the MSE is 18.628835194441383, and the loss function is 18.643580029342164\n",
      "At iteration 6400, the gradient is [[-0.003019371887274274, -0.01419567131652982, -0.01573962658395643, 0.039141256267627926]] \n",
      " the MSE is 18.624723445346632, and the loss function is 18.63949580353019\n",
      "At iteration 6500, the gradient is [[-0.0029628650875608118, -0.013597942426733903, -0.015628047178115327, 0.03813560549701754]] \n",
      " the MSE is 18.62081430300954, and the loss function is 18.63561358518488\n",
      "At iteration 6600, the gradient is [[-0.002907613607647572, -0.013020876137380922, -0.015514877648818518, 0.037158369104841764]] \n",
      " the MSE is 18.617096676958816, and the loss function is 18.63192229433783\n",
      "At iteration 6700, the gradient is [[-0.0028535851615289027, -0.012463802703283107, -0.015400235113503462, 0.03620870252182208]] \n",
      " the MSE is 18.61356012020102, and the loss function is 18.62841149457552\n",
      "At iteration 6800, the gradient is [[-0.002800748376035978, -0.011926073551917498, -0.015284232145271843, 0.0352857868966603]] \n",
      " the MSE is 18.61019479080389, and the loss function is 18.625071354596685\n",
      "At iteration 6900, the gradient is [[-0.002749072763617203, -0.011407060620966152, -0.015166976926139086, 0.034388828304307964]] \n",
      " the MSE is 18.606991415803662, and the loss function is 18.621892612095802\n",
      "At iteration 7000, the gradient is [[-0.0026985286959617665, -0.010906155716464772, -0.015048573395363939, 0.033517056978745734]] \n",
      " the MSE is 18.603941257294856, and the loss function is 18.618866539831224\n",
      "At iteration 7100, the gradient is [[-0.0026490873787059174, -0.010422769891073283, -0.014929121393072982, 0.03266972656939044]] \n",
      " the MSE is 18.601036080569784, and the loss function is 18.615984913744914\n",
      "At iteration 7200, the gradient is [[-0.0026007208265138803, -0.009956332841556597, -0.014808716799180529, 0.03184611342063543]] \n",
      " the MSE is 18.59826812418312, and the loss function is 18.613239983009073\n",
      "At iteration 7300, the gradient is [[-0.002553401839029307, -0.009506292325077633, -0.014687451667853964, 0.03104551587363249]] \n",
      " the MSE is 18.595630071824424, and the loss function is 18.610624441882383\n",
      "At iteration 7400, the gradient is [[-0.0025071039778438633, -0.009072113593819204, -0.01456541435774777, 0.030267253589564523]] \n",
      " the MSE is 18.593115025888697, and the loss function is 18.608131403265855\n",
      "At iteration 7500, the gradient is [[-0.0024618015438754312, -0.008653278847079926, -0.014442689657944865, 0.02951066689395172]] \n",
      " the MSE is 18.590716482641724, and the loss function is 18.605754373854875\n",
      "At iteration 7600, the gradient is [[-0.0024174695553423786, -0.008249286700368681, -0.014319358909778437, 0.028775116141296745]] \n",
      " the MSE is 18.5884283088833, and the loss function is 18.603487230790495\n",
      "At iteration 7700, the gradient is [[-0.0023740837267984677, -0.007859651671322973, -0.014195500124930212, 0.028059981099147636]] \n",
      " the MSE is 18.58624472001728, and the loss function is 18.60132419971876\n",
      "At iteration 7800, the gradient is [[-0.002331620448472606, -0.007483903681384008, -0.014071188099510754, 0.0273646603514273]] \n",
      " the MSE is 18.584160259442992, and the loss function is 18.599259834172585\n",
      "At iteration 7900, the gradient is [[-0.002290056766277634, -0.0071215875731009655, -0.013946494524541327, 0.026688570720113023]] \n",
      " the MSE is 18.582169779187705, and the loss function is 18.597288996195704\n",
      "At iteration 8000, the gradient is [[-0.002249370362636044, -0.006772262642526221, -0.013821488092877515, 0.026031146704769248]] \n",
      " the MSE is 18.580268421704968, and the loss function is 18.595406838133506\n",
      "At iteration 8100, the gradient is [[-0.0022095395376587356, -0.006435502186142207, -0.013696234602645109, 0.025391839939448033]] \n",
      " the MSE is 18.57845160276768, and the loss function is 18.59360878551947\n",
      "At iteration 8200, the gradient is [[-0.0021705431909174954, -0.006110893061855577, -0.013570797057296743, 0.0247701186663976]] \n",
      " the MSE is 18.57671499538988, and the loss function is 18.591890520991225\n",
      "At iteration 8300, the gradient is [[-0.0021323608039328346, -0.005798035263823445, -0.013445235762508083, 0.024165467225943846]] \n",
      " the MSE is 18.575054514714434, and the loss function is 18.59024796917318\n",
      "At iteration 8400, the gradient is [[-0.0020949724230283737, -0.005496541510391181, -0.013319608419840128, 0.023577385562233478]] \n",
      " the MSE is 18.573466303808274, and the loss function is 18.588677282467533\n",
      "At iteration 8500, the gradient is [[-0.0020583586427615694, -0.00520603684495366, -0.013193970217411633, 0.023005388744202687]] \n",
      " the MSE is 18.571946720310084, and the loss function is 18.587174827698295\n",
      "At iteration 8600, the gradient is [[-0.002022500589900698, -0.004926158249288489, -0.013068373917634586, 0.022449006501322945]] \n",
      " the MSE is 18.570492323878685, and the loss function is 18.58573717355665\n",
      "At iteration 8700, the gradient is [[-0.0019873799077862993, -0.004656554268852343, -0.012942869942036557, 0.021907782773756594]] \n",
      " the MSE is 18.569099864393607, and the loss function is 18.58436107879907\n",
      "At iteration 8800, the gradient is [[-0.001952978741287136, -0.004396884649879443, -0.012817506453397095, 0.021381275276336882]] \n",
      " the MSE is 18.567766270862386, and the loss function is 18.58304348115264\n",
      "At iteration 8900, the gradient is [[-0.0019192797220706275, -0.0041468199877063225, -0.012692329435144945, 0.020869055076069412]] \n",
      " the MSE is 18.56648864099158, and the loss function is 18.581781486884584\n",
      "At iteration 9000, the gradient is [[-0.001886265954403134, -0.003906041386110879, -0.012567382768196153, 0.020370706182661066]] \n",
      " the MSE is 18.565264231381473, and the loss function is 18.58057236099591\n",
      "At iteration 9100, the gradient is [[-0.001853921001529737, -0.0036742401273293355, -0.01244270830530776, 0.019885825151647767]] \n",
      " the MSE is 18.56409044830657, and the loss function is 18.57941351800122\n",
      "At iteration 9200, the gradient is [[-0.001822228872040779, -0.003451117352214658, -0.012318345942916887, 0.01941402069990334]] \n",
      " the MSE is 18.562964839046643, and the loss function is 18.578302513259462\n",
      "At iteration 9300, the gradient is [[-0.001791174007027476, -0.0032363837505323474, -0.01219433369071367, 0.018954913332902362]] \n",
      " the MSE is 18.561885083734758, and the loss function is 18.577237034821945\n",
      "At iteration 9400, the gradient is [[-0.00176074126767652, -0.0030297592609095972, -0.012070707738942692, 0.01850813498347622]] \n",
      " the MSE is 18.560848987691237, and the loss function is 18.57621489576662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 9500, the gradient is [[-0.0017309159228364834, -0.002830972780029989, -0.011947502523412614, 0.0180733286618455]] \n",
      " the MSE is 18.55985447421408, and the loss function is 18.575234026989076\n",
      "At iteration 9600, the gradient is [[-0.0017016836372604192, -0.002639761880983685, -0.011824750788427861, 0.017650148116370543]] \n",
      " the MSE is 18.558899577798297, and the loss function is 18.57429247042269\n",
      "At iteration 9700, the gradient is [[-0.0016730304601678893, -0.002455872540435991, -0.011702483647669118, 0.01723825750476036]] \n",
      " the MSE is 18.557982437758213, and the loss function is 18.573388372661984\n",
      "At iteration 9800, the gradient is [[-0.0016449428141804795, -0.0022790588743033186, -0.011580730643060227, 0.01683733107543329]] \n",
      " the MSE is 18.557101292228477, and the loss function is 18.572519978964856\n",
      "At iteration 9900, the gradient is [[-0.001617407484375481, -0.002109082881568362, -0.011459519801634451, 0.016447052858797596]] \n",
      " the MSE is 18.556254472520873, and the loss function is 18.571685627610822\n",
      "At iteration 0, the gradient is [[-46.37167136543909, -19.271161496883867, -12.09307158899357, -14.803578306994197]] \n",
      " the MSE is 598.4231710753544, and the loss function is 598.4231710753544\n",
      "At iteration 100, the gradient is [[-3.3288271569488797, 2.4744526159409803, 2.298842175155525, 1.8715583496820756]] \n",
      " the MSE is 62.50248449827652, and the loss function is 62.50714911112832\n",
      "At iteration 200, the gradient is [[-1.7909938079005543, 1.310207812171156, 1.2666293994520144, 1.1141426482451668]] \n",
      " the MSE is 32.24258779109274, and the loss function is 32.25000459339661\n",
      "At iteration 300, the gradient is [[-0.9799929963497397, 0.6602423593701757, 0.6918153566952385, 0.6858600774412797]] \n",
      " the MSE is 23.16724558216695, and the loss function is 23.176661899247023\n",
      "At iteration 400, the gradient is [[-0.5374874206142826, 0.30745660611678577, 0.3779214838212271, 0.4498199839156626]] \n",
      " the MSE is 20.402256158546656, and the loss function is 20.41291878966458\n",
      "At iteration 500, the gradient is [[-0.295993744447983, 0.11676885975569062, 0.20640833318838267, 0.31875446082904946]] \n",
      " the MSE is 19.519816527240074, and the loss function is 19.531212274050485\n",
      "At iteration 600, the gradient is [[-0.16417121874150578, 0.01446109520944117, 0.11258750648720155, 0.2450329286037086]] \n",
      " the MSE is 19.201553482371835, and the loss function is 19.21337263542677\n",
      "At iteration 700, the gradient is [[-0.09218560062918293, -0.03968434953002637, 0.061165497601582706, 0.20266653401050488]] \n",
      " the MSE is 19.054734623208073, and the loss function is 19.066800008887494\n",
      "At iteration 800, the gradient is [[-0.05284797051699749, -0.06760784176123538, 0.03288627313955344, 0.17747393842883435]] \n",
      " the MSE is 18.96220462155362, and the loss function is 18.974417705508614\n",
      "At iteration 900, the gradient is [[-0.03132438108170822, -0.08127797179316658, 0.017243632844728067, 0.1617158870770274]] \n",
      " the MSE is 18.888868821674766, and the loss function is 18.90117578919931\n",
      "At iteration 1000, the gradient is [[-0.019521586178181748, -0.08722292501847012, 0.008505175259951425, 0.15116790303509825]] \n",
      " the MSE is 18.824081394386205, and the loss function is 18.836453305938385\n",
      "At iteration 1100, the gradient is [[-0.013023973265679101, -0.08900273311139176, 0.0035430287804811555, 0.14352318803595787]] \n",
      " the MSE is 18.76449113785605, and the loss function is 18.776912708798154\n",
      "At iteration 1200, the gradient is [[-0.009422396673553336, -0.08855809783093939, 0.0006503285616158664, 0.1375212654705739]] \n",
      " the MSE is 18.70893213035813, and the loss function is 18.72139549689962\n",
      "At iteration 1300, the gradient is [[-0.007402412433195958, -0.08694628865911463, -0.0011044974560974262, 0.13247257822687675]] \n",
      " the MSE is 18.65690084849642, and the loss function is 18.66940213734392\n",
      "At iteration 1400, the gradient is [[-0.006246831949375444, -0.08474263657970131, -0.0022300214022226077, 0.12799911399340966]] \n",
      " the MSE is 18.60810149073901, and the loss function is 18.620638942903568\n",
      "At iteration 1500, the gradient is [[-0.005564322216840662, -0.08225958185636849, -0.0030040218745037814, 0.12389289217868209]] \n",
      " the MSE is 18.562309635387145, and the loss function is 18.57488258492632\n",
      "At iteration 1600, the gradient is [[-0.005141333862160232, -0.07966618188468141, -0.0035783494011563427, 0.12003875453389416]] \n",
      " the MSE is 18.519330872074057, and the loss function is 18.531939193096587\n",
      "At iteration 1700, the gradient is [[-0.004861315117071221, -0.07705331150619443, -0.004036139902795828, 0.11637223903243307]] \n",
      " the MSE is 18.47898783672777, and the loss function is 18.491631646171687\n",
      "At iteration 1800, the gradient is [[-0.004660636053504382, -0.07446923393994277, -0.004423029880957232, 0.11285659473824522]] \n",
      " the MSE is 18.441115787954946, and the loss function is 18.45379528777012\n",
      "At iteration 1900, the gradient is [[-0.00450454142347992, -0.07193900638475623, -0.00476418730292894, 0.10947023978381275]] \n",
      " the MSE is 18.405560786362948, and the loss function is 18.41827618149372\n",
      "At iteration 2000, the gradient is [[-0.004374030826030906, -0.06947506608954483, -0.005073603719368591, 0.10619991701509301]] \n",
      " the MSE is 18.372178693989945, and the loss function is 18.384930151436283\n",
      "At iteration 2100, the gradient is [[-0.004258700671826076, -0.06708300465407109, -0.005359164209315385, 0.1030369582547559]] \n",
      " the MSE is 18.340834454114297, and the loss function is 18.353622084135356\n",
      "At iteration 2200, the gradient is [[-0.004152838821952065, -0.06476471713760817, -0.005625413757465582, 0.09997524463361375]] \n",
      " the MSE is 18.31140148512522, and the loss function is 18.324225334268792\n",
      "At iteration 2300, the gradient is [[-0.004053293841418759, -0.0625201189402813, -0.005875066828034193, 0.0970100923209872]] \n",
      " the MSE is 18.28376113517939, and the loss function is 18.2966211855248\n",
      "At iteration 2400, the gradient is [[-0.0039583124586919, -0.060348081320568944, -0.006109831235377966, 0.094137643186205]] \n",
      " the MSE is 18.257802179092153, and the loss function is 18.270698350660275\n",
      "At iteration 2500, the gradient is [[-0.003866905265832136, -0.05824694064962719, -0.00633085789585048, 0.09135453098849593]] \n",
      " the MSE is 18.233420349874066, and the loss function is 18.24635250455739\n",
      "At iteration 2600, the gradient is [[-0.0037785006207561866, -0.05621477513939686, -0.006538986457471036, 0.08865769793466496]] \n",
      " the MSE is 18.210517900966842, and the loss function is 18.223485847094945\n",
      "At iteration 2700, the gradient is [[-0.00369275578890015, -0.05424955474296014, -0.006734879555026778, 0.08604429331779134]] \n",
      " the MSE is 18.189003196558456, and the loss function is 18.20200669362944\n",
      "At iteration 2800, the gradient is [[-0.00360945387420024, -0.052349221894338945, -0.006919096293220122, 0.0835116169786481]] \n",
      " the MSE is 18.16879032791779, and the loss function is 18.181829091251743\n",
      "At iteration 2900, the gradient is [[-0.003528447556118924, -0.050511734548453646, -0.007092132565734554, 0.0810570872614565]] \n",
      " the MSE is 18.14979875397644, and the loss function is 18.1628724591694\n",
      "At iteration 3000, the gradient is [[-0.0034496283644779554, -0.0487350886849216, -0.007254443273118874, 0.07867822237160944]] \n",
      " the MSE is 18.13195296456622, and the loss function is 18.145061251689004\n",
      "At iteration 3100, the gradient is [[-0.0033729098880363783, -0.047017329638865527, -0.007406454657764851, 0.07637262908238125]] \n",
      " the MSE is 18.11518216485352, and the loss function is 18.128324642375134\n",
      "At iteration 3200, the gradient is [[-0.0032982185864960933, -0.045356557366397494, -0.007548571240188571, 0.0741379954868244]] \n",
      " the MSE is 18.09941997961966, and the loss function is 18.112596228053565\n",
      "At iteration 3300, the gradient is [[-0.0032254887511789566, -0.043750928429987485, -0.007681179802966528, 0.07197208599154811]] \n",
      " the MSE is 18.084604176129986, and the loss function is 18.097813751411042\n",
      "At iteration 3400, the gradient is [[-0.003154659730356902, -0.04219865622282836, -0.007804651757633709, 0.0698727375670829]] \n",
      " the MSE is 18.070676404417696, and the loss function is 18.08391884102224\n",
      "At iteration 3500, the gradient is [[-0.003085674390664193, -0.04069801025981794, -0.00791934462301775, 0.0678378567163994]] \n",
      " the MSE is 18.057581953884828, and the loss function is 18.070856767708296\n",
      "At iteration 3600, the gradient is [[-0.003018478253984062, -0.03924731498596015, -0.008025603012804035, 0.06586541686664904]] \n",
      " the MSE is 18.045269525193177, and the loss function is 18.05857621620031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8732/639760672.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.cat((ones_column, torch.tensor(self.X)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 3700, the gradient is [[-0.002953019003376409, -0.037844948347150686, -0.008123759349421665, 0.06395345602234681]] \n",
      " the MSE is 18.033691016482937, and the loss function is 18.047029071145403\n",
      "At iteration 3800, the gradient is [[-0.002889246191199088, -0.03648934025630864, -0.008214134423015133, 0.062100074590628905]] \n",
      " the MSE is 18.022801323018214, and the loss function is 18.036170216553963\n",
      "At iteration 3900, the gradient is [[-0.002827111058182099, -0.03517897102650909, -0.00829703786038292, 0.06030343332896713]] \n",
      " the MSE is 18.012558149414872, and the loss function is 18.025957347842894\n",
      "At iteration 4000, the gradient is [[-0.0027665664138110235, -0.03391236980949264, -0.00837276853950698, 0.058561751387322114]] \n",
      " the MSE is 18.002921833659887, and the loss function is 18.016350795683195\n",
      "At iteration 4100, the gradient is [[-0.002707566550801889, -0.0326881130597272, -0.008441614969274334, 0.05687330442856046]] \n",
      " the MSE is 17.99385518218084, and the loss function is 18.007313360909773\n",
      "At iteration 4200, the gradient is [[-0.0026500671787114028, -0.03150482303422548, -0.008503855645189285, 0.05523642281752563]] \n",
      " the MSE is 17.98532331527101, and the loss function is 17.99881015979818\n",
      "At iteration 4300, the gradient is [[-0.0025940253686547797, -0.03036116633312101, -0.008559759387219715, 0.05364948987259993]] \n",
      " the MSE is 17.97729352221913, and the loss function is 17.99080847905659\n",
      "At iteration 4400, the gradient is [[-0.0025393995049527687, -0.029255852483066554, -0.008609585663284776, 0.052110940175566986]] \n",
      " the MSE is 17.969735125533894, and the loss function is 17.983277639922424\n",
      "At iteration 4500, the gradient is [[-0.0024861492406158497, -0.028187632563640534, -0.008653584900273452, 0.05061925793694019]] \n",
      " the MSE is 17.962619353691522, and the loss function is 17.976188870791262\n",
      "At iteration 4600, the gradient is [[-0.002434235456018342, -0.02715529787666632, -0.008691998784050957, 0.049172975414127844]] \n",
      " the MSE is 17.95591922187084, and the loss function is 17.9695151878419\n",
      "At iteration 4700, the gradient is [[-0.0023836202195157354, -0.026157678657392613, -0.008725060549118844, 0.047770671380560244]] \n",
      " the MSE is 17.949609420173818, and the loss function is 17.963231283154894\n",
      "At iteration 4800, the gradient is [[-0.002334266749772138, -0.02519364282659775, -0.008752995258621802, 0.04641096964387494]] \n",
      " the MSE is 17.943666208861124, and the loss function is 17.957313419853634\n",
      "At iteration 4900, the gradient is [[-0.002286139379479423, -0.024262094782390477, -0.00877602007506417, 0.04509253761156522]] \n",
      " the MSE is 17.93806732016206, and the loss function is 17.951739333826783\n",
      "At iteration 5000, the gradient is [[-0.0022392035205358683, -0.02336197423070311, -0.008794344522256267, 0.0438140849023688]] \n",
      " the MSE is 17.932791866245406, and the loss function is 17.946488141618172\n",
      "At iteration 5100, the gradient is [[-0.002193425630176624, -0.02249225505297931, -0.008808170738589857, 0.042574362002140015]] \n",
      " the MSE is 17.927820252964473, and the loss function is 17.941540254096935\n",
      "At iteration 5200, the gradient is [[-0.0021487731782468137, -0.021651944210173437, -0.008817693722150997, 0.04137215896256738]] \n",
      " the MSE is 17.92313409901316, and the loss function is 17.936877295544367\n",
      "At iteration 5300, the gradient is [[-0.0021052146158256346, -0.02084008068188101, -0.008823101567902735, 0.04020630414136706]] \n",
      " the MSE is 17.918716160153306, and the loss function is 17.932482027817315\n",
      "At iteration 5400, the gradient is [[-0.002062719344288627, -0.020055734439189322, -0.008824575697018137, 0.039075662982863454]] \n",
      " the MSE is 17.914550258194552, and the loss function is 17.928338279269024\n",
      "At iteration 5500, the gradient is [[-0.0020212576859291826, -0.0192980054506496, -0.008822291078924738, 0.03797913683729012]] \n",
      " the MSE is 17.91062121442827, and the loss function is 17.9244308781286\n",
      "At iteration 5600, the gradient is [[-0.001980800855213757, -0.018566022719921412, -0.008816416446047103, 0.03691566181787375]] \n",
      " the MSE is 17.906914787235614, and the loss function is 17.92074559005889\n",
      "At iteration 5700, the gradient is [[-0.0019413209309908745, -0.017858943354272253, -0.008807114501602108, 0.03588420769435602]] \n",
      " the MSE is 17.903417613607573, and the loss function is 17.917269059630303\n",
      "At iteration 5800, the gradient is [[-0.0019027908294820366, -0.017175951662793648, -0.008794542120581684, 0.034883776821882054]] \n",
      " the MSE is 17.90011715433125, and the loss function is 17.913988755464565\n",
      "At iteration 5900, the gradient is [[-0.0018651842785043977, -0.016516258283731888, -0.008778850544355492, 0.033913403103897545]] \n",
      " the MSE is 17.89700164261203, and the loss function is 17.9108929188178\n",
      "At iteration 6000, the gradient is [[-0.0018284757919459424, -0.015879099339412173, -0.008760185568728106, 0.03297215098836998]] \n",
      " the MSE is 17.894060035915995, and the loss function is 17.907970515387046\n",
      "At iteration 6100, the gradient is [[-0.0017926406455353355, -0.015263735618529223, -0.008738687726089567, 0.03205911449581736]] \n",
      " the MSE is 17.891281970830125, and the loss function is 17.905211190137514\n",
      "At iteration 6200, the gradient is [[-0.0017576548530714954, -0.014669451784366499, -0.008714492461453411, 0.031173416278521528]] \n",
      " the MSE is 17.888657720750906, and the loss function is 17.902605224961047\n",
      "At iteration 6300, the gradient is [[-0.0017234951436350576, -0.014095555608627254, -0.008687730302968674, 0.03031420670956141]] \n",
      " the MSE is 17.886178156223632, and the loss function is 17.90014349898781\n",
      "At iteration 6400, the gradient is [[-0.0016901389393115507, -0.01354137722951036, -0.008658527026662213, 0.029480663001122086]] \n",
      " the MSE is 17.883834707766113, and the loss function is 17.897817451384775\n",
      "At iteration 6500, the gradient is [[-0.0016575643336514932, -0.013006268433727092, -0.008627003815958571, 0.02867198835078608]] \n",
      " the MSE is 17.881619331020534, and the loss function is 17.895619046484573\n",
      "At iteration 6600, the gradient is [[-0.0016257500709936376, -0.012489601961489412, -0.008593277415981676, 0.027887411115070454]] \n",
      " the MSE is 17.879524474087525, and the loss function is 17.893540741098608\n",
      "At iteration 6700, the gradient is [[-0.0015946755263271726, -0.011990770833733655, -0.008557460282810351, 0.02712618400933646]] \n",
      " the MSE is 17.877543046905338, and the loss function is 17.891575453877177\n",
      "At iteration 6800, the gradient is [[-0.0015643206855564902, -0.01150918770076955, -0.008519660727797406, 0.026387583333315666]] \n",
      " the MSE is 17.87566839254572, and the loss function is 17.889716536588043\n",
      "At iteration 6900, the gradient is [[-0.001534666126892713, -0.01104428421205257, -0.008479983057352225, 0.02567090822112796]] \n",
      " the MSE is 17.87389426030626, and the loss function is 17.887957747193152\n",
      "At iteration 7000, the gradient is [[-0.0015056930023334629, -0.010595510405838482, -0.008438527707957817, 0.02497547991544452]] \n",
      " the MSE is 17.87221478048632, and the loss function is 17.886293224610416\n",
      "At iteration 7100, the gradient is [[-0.0014773830200280548, -0.010162334118609083, -0.008395391376940925, 0.024300641064638126]] \n",
      " the MSE is 17.87062444074101, and the loss function is 17.884717465054937\n",
      "At iteration 7200, the gradient is [[-0.0014497184270276207, -0.009744240413284163, -0.008350667148869083, 0.023645755042439806]] \n",
      " the MSE is 17.869118063914136, and the loss function is 17.88322529986054\n",
      "At iteration 7300, the gradient is [[-0.0014226819928779338, -0.009340731025859882, -0.008304444617908184, 0.02301020528919318]] \n",
      " the MSE is 17.867690787257207, and the loss function is 17.881811874688545\n",
      "At iteration 7400, the gradient is [[-0.0013962569931568526, -0.00895132382960105, -0.008256810006060142, 0.022393394674265075]] \n",
      " the MSE is 17.866338042947753, and the loss function is 17.880472630036945\n",
      "At iteration 7500, the gradient is [[-0.001370427194242717, -0.00857555231659352, -0.008207846277664261, 0.0217947448786105]] \n",
      " the MSE is 17.865055539825182, and the loss function is 17.879203282968174\n",
      "At iteration 7600, the gradient is [[-0.001345176838016635, -0.008212965095750894, -0.008157633250047557, 0.021213695797165442]] \n",
      " the MSE is 17.86383924626802, and the loss function is 17.877999809979183\n",
      "At iteration 7700, the gradient is [[-0.001320490627147797, -0.007863125406929065, -0.008106247700583503, 0.020649704960248316]] \n",
      " the MSE is 17.86268537414078, and the loss function is 17.87685843094204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 7800, the gradient is [[-0.0012963537111144055, -0.007525610650710688, -0.008053763470299726, 0.020102246973347893]] \n",
      " the MSE is 17.86159036374348, and the loss function is 17.875775594047955\n",
      "At iteration 7900, the gradient is [[-0.00127275167239436, -0.007200011933116821, -0.00800025156400548, 0.01957081297485951]] \n",
      " the MSE is 17.86055086970081, and the loss function is 17.874747961691728\n",
      "At iteration 8000, the gradient is [[-0.0012496705131687263, -0.006885933624998703, -0.0079457802472019, 0.019054910111045913]] \n",
      " the MSE is 17.85956374773218, and the loss function is 17.873772397237737\n",
      "At iteration 8100, the gradient is [[-0.0012270966424556625, -0.0065829929354813445, -0.00789041513976498, 0.018554061027774623]] \n",
      " the MSE is 17.858626042247337, and the loss function is 17.872845952612096\n",
      "At iteration 8200, the gradient is [[-0.0012050168635617444, -0.00629081949909102, -0.007834219306567797, 0.01806780337845452]] \n",
      " the MSE is 17.857734974715694, and the loss function is 17.871965856669178\n",
      "At iteration 8300, the gradient is [[-0.0011834183622020803, -0.006009054976211511, -0.007777253345192533, 0.0175956893475714]] \n",
      " the MSE is 17.856887932761154, and the loss function is 17.87112950428408\n",
      "At iteration 8400, the gradient is [[-0.0011622886945313509, -0.005737352666107242, -0.007719575470604283, 0.01713728518959085]] \n",
      " the MSE is 17.856082459936587, and the loss function is 17.87033444612526\n",
      "At iteration 8500, the gradient is [[-0.0011416157760418635, -0.005475377132656326, -0.007661241597243824, 0.01669217078232408]] \n",
      " the MSE is 17.855316246135768, and the loss function is 17.86957837906507\n",
      "At iteration 8600, the gradient is [[-0.001121387870446256, -0.005222803841798986, -0.00760230541824506, 0.016259939194730795]] \n",
      " the MSE is 17.85458711860245, and the loss function is 17.868859137187847\n",
      "At iteration 8700, the gradient is [[-0.0011015935791638585, -0.0049793188107938886, -0.0075428184821526145, 0.01584019626835963]] \n",
      " the MSE is 17.853893033499507, and the loss function is 17.868174683358404\n",
      "At iteration 8800, the gradient is [[-0.0010822218308441846, -0.004744618268566156, -0.007482830267030752, 0.015432560212238832]] \n",
      " the MSE is 17.85323206800275, and the loss function is 17.867523101315555\n",
      "At iteration 8900, the gradient is [[-0.0010632618714709688, -0.004518408327073923, -0.007422388252216106, 0.015036661210563438]] \n",
      " the MSE is 17.852602412886895, and the loss function is 17.866902588258096\n",
      "At iteration 9000, the gradient is [[-0.0010447032548883673, -0.004300404663211281, -0.007361537987689469, 0.014652141042898012]] \n",
      " the MSE is 17.852002365572513, and the loss function is 17.866311447892034\n",
      "At iteration 9100, the gradient is [[-0.0010265358330924122, -0.004090332210717452, -0.007300323161054965, 0.014278652716604832]] \n",
      " the MSE is 17.851430323605424, and the loss function is 17.86574808391053\n",
      "At iteration 9200, the gradient is [[-0.0010087497474278494, -0.0038879248622027437, -0.007238785662453601, 0.013915860110813215]] \n",
      " the MSE is 17.850884778541182, and the loss function is 17.86521099387912\n",
      "At iteration 9300, the gradient is [[-0.0009913354196621226, -0.003692925180574123, -0.0071769656472348365, 0.013563437631839067]] \n",
      " the MSE is 17.85036431020958, and the loss function is 17.864698763501202\n",
      "At iteration 9400, the gradient is [[-0.0009742835435942661, -0.003505084119774716, -0.007114901596595982, 0.01322106987953257]] \n",
      " the MSE is 17.849867581335147, and the loss function is 17.864210061239667\n",
      "At iteration 9500, the gradient is [[-0.0009575850766176421, -0.003324160754429695, -0.00705263037618733, 0.012888451324296354]] \n",
      " the MSE is 17.849393332491488, and the loss function is 17.86374363327257\n",
      "At iteration 9600, the gradient is [[-0.0009412312319472536, -0.003149922018241254, -0.006990187292826372, 0.012565285994320923]] \n",
      " the MSE is 17.84894037736868, and the loss function is 17.86329829876195\n",
      "At iteration 9700, the gradient is [[-0.0009252134707818662, -0.002982142450710555, -0.00692760614929499, 0.01225128717283116]] \n",
      " the MSE is 17.848507598333914, and the loss function is 17.862872945416076\n",
      "At iteration 9800, the gradient is [[-0.000909523494846909, -0.002820603951985774, -0.00686491929732114, 0.011946177104968175]] \n",
      " the MSE is 17.848093942267216, and the loss function is 17.86246652532686\n",
      "At iteration 9900, the gradient is [[-0.0008941532389995563, -0.0026650955455600972, -0.006802157688798111, 0.011649686714017778]] \n",
      " the MSE is 17.847698416655085, and the loss function is 17.862078051065286\n",
      "The Cross-validated Mean Squared Error for Locally Weighted Regression is : 16.466324743149237\n",
      "The Cross-validated Mean Squared Error for Ridge-Regression is : 18.560100843353318\n"
     ]
    }
   ],
   "source": [
    "#Compared to the ridge-regression class we wrote earlier, we see an improvement!\n",
    "#Here, we use k-fold validation to compare how this class does compared to the custom ridge regression we wrote\n",
    "from sklearn.model_selection import KFold\n",
    "mse_lwr = []\n",
    "mse_rf = []\n",
    "kf = KFold(n_splits=10,shuffle=True,random_state=1234)\n",
    "model_rf = CustomRidgeModel()\n",
    "model_lw = Lowess(kernel= Epanechnikov,tau=0.14)\n",
    "\n",
    "data = pd.read_csv('../content/01intro/cars.csv')\n",
    "y = data['MPG'].values\n",
    "x = data.drop(['MPG'],axis=1).values\n",
    "scale = MinMaxScaler()\n",
    "\n",
    "for idxtrain, idxtest in kf.split(x):\n",
    "  xtrain = x[idxtrain]\n",
    "  ytrain = y[idxtrain]\n",
    "  ytest = y[idxtest]\n",
    "  xtest = x[idxtest]\n",
    "  xtrain = scale.fit_transform(xtrain)\n",
    "  xtest = scale.transform(xtest)\n",
    "\n",
    "  model_lw.fit(xtrain,ytrain)\n",
    "  yhat_lw = model_lw.predict(xtest)\n",
    "\n",
    "  model_rf.fit(xtrain,ytrain)\n",
    "  yhat_rf = model_rf.predict(xtest,scale=False)\n",
    "\n",
    "  mse_lwr.append(mse(ytest,yhat_lw))\n",
    "  mse_rf.append(mse(ytest,yhat_rf))\n",
    "print('The Cross-validated Mean Squared Error for Locally Weighted Regression is : '+str(np.mean(mse_lwr)))\n",
    "print('The Cross-validated Mean Squared Error for Ridge-Regression is : '+str(np.mean(mse_rf)))\n",
    "#As we can see, Lowess beats ridge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
