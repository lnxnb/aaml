{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2917afe6",
   "metadata": {},
   "source": [
    "## Advanced Applied Machine Learning HW 4\n",
    "\n",
    "For this assignment you will use a gridsearch algorithm, such as the particle swarm or CSO to tune hyperparameters for a Pytorch neural network design, such as Alex Net, to create a data application for the CiFAR10  data set and yield good accuracy on the test set. For CiFAR10, good accuracy on the test set is over 84%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8b5231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Much code taken from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "#and https://pytorch.org/hub/pytorch_vision_alexnet/\n",
    "#and class file: \"Example AlexNet Design Pytorch.ipynb\"\n",
    "#and class file: \"Grid_Search_Algorithms.ipynb\"\n",
    "#First, I'm just going to make sure the model runs and we get fairly good accuracy with default parameters.\n",
    "import torch\n",
    "import torchvision \n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "839b4880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get gpu, mps or cpu device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1049c1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def get_train_valid_loader(data_dir,\n",
    "                           batch_size,\n",
    "                           augment,\n",
    "                           random_seed,\n",
    "                           valid_size=0.1,\n",
    "                           shuffle=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    valid_transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "    if augment:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    # load the dataset\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=train_transform,\n",
    "    )\n",
    "\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=valid_transform,\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "def get_test_loader(data_dir,\n",
    "                    batch_size,\n",
    "                    shuffle=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "\n",
    "    # define transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((227,227)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=False,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "# CIFAR10 dataset\n",
    "train_loader, valid_loader = get_train_valid_loader(data_dir = './data',                                      batch_size = 64,\n",
    "                       augment = False,                             \t\t     random_seed = 1)\n",
    "\n",
    "test_loader = get_test_loader(data_dir = './data',\n",
    "                              batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74b586ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e652b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 40\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = AlexNet(num_classes).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d2f9dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [704/704], Loss: 0.8856\n",
      "Accuracy of the network on the 5000 validation images: 56.9 %\n",
      "Epoch [2/20], Step [704/704], Loss: 0.3926\n",
      "Accuracy of the network on the 5000 validation images: 66.74 %\n",
      "Epoch [3/20], Step [704/704], Loss: 0.9596\n",
      "Accuracy of the network on the 5000 validation images: 71.76 %\n",
      "Epoch [4/20], Step [704/704], Loss: 0.8214\n",
      "Accuracy of the network on the 5000 validation images: 73.3 %\n",
      "Epoch [5/20], Step [704/704], Loss: 0.4678\n",
      "Accuracy of the network on the 5000 validation images: 76.66 %\n",
      "Epoch [6/20], Step [704/704], Loss: 0.5510\n",
      "Accuracy of the network on the 5000 validation images: 76.7 %\n",
      "Epoch [7/20], Step [704/704], Loss: 1.9526\n",
      "Accuracy of the network on the 5000 validation images: 79.1 %\n",
      "Epoch [8/20], Step [704/704], Loss: 0.3695\n",
      "Accuracy of the network on the 5000 validation images: 79.58 %\n",
      "Epoch [9/20], Step [704/704], Loss: 0.5063\n",
      "Accuracy of the network on the 5000 validation images: 79.12 %\n",
      "Epoch [10/20], Step [704/704], Loss: 0.6946\n",
      "Accuracy of the network on the 5000 validation images: 81.34 %\n",
      "Epoch [11/20], Step [704/704], Loss: 0.6316\n",
      "Accuracy of the network on the 5000 validation images: 81.66 %\n",
      "Epoch [12/20], Step [704/704], Loss: 0.2220\n",
      "Accuracy of the network on the 5000 validation images: 79.92 %\n",
      "Epoch [13/20], Step [704/704], Loss: 0.6737\n",
      "Accuracy of the network on the 5000 validation images: 81.2 %\n",
      "Epoch [14/20], Step [704/704], Loss: 0.5543\n",
      "Accuracy of the network on the 5000 validation images: 82.3 %\n",
      "Epoch [15/20], Step [704/704], Loss: 0.2511\n",
      "Accuracy of the network on the 5000 validation images: 81.26 %\n",
      "Epoch [16/20], Step [704/704], Loss: 1.3132\n",
      "Accuracy of the network on the 5000 validation images: 81.6 %\n",
      "Epoch [17/20], Step [704/704], Loss: 0.8785\n",
      "Accuracy of the network on the 5000 validation images: 83.5 %\n",
      "Epoch [18/20], Step [704/704], Loss: 0.2836\n",
      "Accuracy of the network on the 5000 validation images: 82.76 %\n",
      "Epoch [19/20], Step [704/704], Loss: 0.3451\n",
      "Accuracy of the network on the 5000 validation images: 83.56 %\n",
      "Epoch [20/20], Step [704/704], Loss: 0.3002\n",
      "Accuracy of the network on the 5000 validation images: 80.62 %\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "\n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bc80a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 80.28 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2214867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Success! Now let's functionalize this process\n",
    "def runNetWithHypers(train_loader,valid_loader,test_loader,num_classes,num_epochs,batch_size,learning_rate,weight_decay,momentum):\n",
    "    model = AlexNet(num_classes).to(device)\n",
    "\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = weight_decay, momentum = momentum)\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "\n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))\n",
    "    return correct/total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e4a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's implement PSO to determine good hyperparameters that give us > 84% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c656c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [704/704], Loss: 1.2881\n",
      "Epoch [2/30], Step [704/704], Loss: 1.0304\n",
      "Epoch [3/30], Step [704/704], Loss: 0.9337\n",
      "Epoch [4/30], Step [704/704], Loss: 1.6509\n",
      "Epoch [5/30], Step [704/704], Loss: 0.5721\n",
      "Epoch [6/30], Step [704/704], Loss: 1.0838\n",
      "Epoch [7/30], Step [704/704], Loss: 1.0250\n",
      "Epoch [8/30], Step [704/704], Loss: 0.3377\n",
      "Epoch [9/30], Step [704/704], Loss: 0.1971\n",
      "Epoch [10/30], Step [704/704], Loss: 0.0226\n",
      "Epoch [11/30], Step [704/704], Loss: 0.3743\n",
      "Epoch [12/30], Step [704/704], Loss: 0.4387\n",
      "Epoch [13/30], Step [704/704], Loss: 0.6110\n",
      "Epoch [14/30], Step [704/704], Loss: 0.2393\n",
      "Epoch [15/30], Step [704/704], Loss: 0.3760\n",
      "Epoch [16/30], Step [704/704], Loss: 0.2641\n",
      "Epoch [17/30], Step [704/704], Loss: 0.3431\n",
      "Epoch [18/30], Step [704/704], Loss: 0.2858\n",
      "Epoch [19/30], Step [704/704], Loss: 0.0271\n",
      "Epoch [20/30], Step [704/704], Loss: 0.4657\n",
      "Epoch [21/30], Step [704/704], Loss: 0.5817\n",
      "Epoch [22/30], Step [704/704], Loss: 0.7168\n",
      "Epoch [23/30], Step [704/704], Loss: 0.0124\n",
      "Epoch [24/30], Step [704/704], Loss: 0.0434\n",
      "Epoch [25/30], Step [704/704], Loss: 0.2870\n",
      "Epoch [26/30], Step [704/704], Loss: 0.2501\n",
      "Epoch [27/30], Step [704/704], Loss: 0.1224\n",
      "Epoch [28/30], Step [704/704], Loss: 1.1645\n",
      "Epoch [29/30], Step [704/704], Loss: 0.8002\n",
      "Epoch [30/30], Step [704/704], Loss: 0.0230\n",
      "Accuracy of the network on the 5000 validation images: 84.66 %\n",
      "Accuracy of the network on the 10000 test images: 84.15 %\n",
      "[0.0116776892303859, 0.00018272633009454856, 0.4384863796183392] 0.8415\n",
      "Epoch [1/30], Step [704/704], Loss: 1.2337\n",
      "Epoch [2/30], Step [704/704], Loss: 1.0291\n",
      "Epoch [3/30], Step [704/704], Loss: 0.4874\n",
      "Epoch [4/30], Step [704/704], Loss: 0.6720\n",
      "Epoch [5/30], Step [704/704], Loss: 0.9341\n",
      "Epoch [6/30], Step [704/704], Loss: 0.5712\n"
     ]
    }
   ],
   "source": [
    "num_classes=10\n",
    "num_epochs=30\n",
    "batch_size=40\n",
    "learning_rate=0.005\n",
    "weight_decay = 0.005\n",
    "momentum = 0.9\n",
    "accuracy = 0\n",
    "\n",
    "# PSO algorithm. For now we're going to try just optimizing along learning rate. If that proves insufficient, we may expand\n",
    "# to momentum, weight_decay, and batch_size\n",
    "\n",
    "def particle_swarm_optimization(num_dimensions, num_particles, max_iter,i_min=-10,i_max=10,bounds=None,w=0.5,c1=0.25,c2=0.75):\n",
    "    # Initialize the particles\n",
    "    # This creates a data structure such as a dictionary\n",
    "    if bounds is None:\n",
    "        particles = [({'position': [np.random.uniform(i_min, i_max) for _ in range(num_dimensions)],\n",
    "                    'velocity': [np.random.uniform(-1, 1) for _ in range(num_dimensions)],\n",
    "                    'pbest': float('inf'),\n",
    "                    'pbest_position': None})\n",
    "                    for _ in range(num_particles)]\n",
    "    else:\n",
    "        particles = [({'position': [np.random.uniform(bounds[i][0], bounds[i][1]) for i in range(num_dimensions)],\n",
    "                    'velocity': [np.random.uniform(-1, 1) for _ in range(num_dimensions)],\n",
    "                    'pbest': float('inf'),\n",
    "                    'pbest_position': None})\n",
    "                    for _ in range(num_particles)]\n",
    "\n",
    "    # Initialize global best\n",
    "    gbest_value = float('inf')\n",
    "    gbest_position = None\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        for particle in particles:\n",
    "            position = particle['position']\n",
    "            velocity = particle['velocity']\n",
    "            # Calculate the current value\n",
    "            current_value = runNetWithHypers(train_loader,valid_loader,test_loader,num_classes=num_classes,num_epochs=num_epochs,batch_size=batch_size,\n",
    "                 learning_rate=position[0],weight_decay=position[1],momentum=position[2])\n",
    "            print(position, current_value)\n",
    "\n",
    "            # Update personal best\n",
    "            if 1-current_value < particle['pbest']:\n",
    "                particle['pbest'] = current_value\n",
    "                particle['pbest_position'] = position.copy()\n",
    "\n",
    "            # Update global best\n",
    "            if 1-current_value < gbest_value:\n",
    "                gbest_value = current_value\n",
    "                gbest_position = position.copy()\n",
    "\n",
    "            # Update particle's velocity and position\n",
    "            for i in range(num_dimensions):\n",
    "                r1, r2 = np.random.uniform(), np.random.uniform()\n",
    "                velocity[i] = w * velocity[i] + c1*r1 * (particle['pbest_position'][i] - position[i]) + c2*r2 * (gbest_position[i] - position[i])\n",
    "                position[i] += velocity[i]\n",
    "                # legalize the values to the provided bounds\n",
    "                if bounds is not None:\n",
    "                    position[i] = np.clip(position[i],bounds[i][0],bounds[i][1])\n",
    "\n",
    "    return gbest_position, gbest_value\n",
    "\n",
    "\n",
    "particle_swarm_optimization(num_dimensions=3, num_particles=30, max_iter=10,i_min=-0.001,i_max=0.001,bounds=[(0.0004,0.04),(0.0001,0.0002),(0.3,0.5)],w=0.5,c1=0.25,c2=0.75)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4a649",
   "metadata": {},
   "source": [
    "Success! With 30 training epochs in AlexNet, a learning rate of 0.011677, a weight decay of 0.000182, and a momentum of 0.438486, we have achieved > 84% accuracy on our test data! This took a little experimentation and fine-tuning of the PSO hyperparameters (increments, bounds, #of particles, etc), but it worked!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
