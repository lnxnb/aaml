{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ecd9388",
   "metadata": {},
   "source": [
    "# Advanced Applied Machine Learning - HW3\n",
    "\n",
    "## Q1\n",
    "\n",
    "####  (5 points) Create your own PyTorch class that implements the method of SCAD regularization and variable selection (smoothly clipped absolute deviations) for linear models. Test your method one a real data set, and determine a variable selection based on features importance according to SCAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b54ba844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split as tts, KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from scipy.linalg import toeplitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "933ba121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code copied from SCAD paper\n",
    "def scad_penalty(beta_hat, lambda_val, a_val):\n",
    "    is_linear = (np.abs(beta_hat) <= lambda_val)\n",
    "    is_quadratic = np.logical_and(lambda_val < np.abs(beta_hat), np.abs(beta_hat) <= a_val * lambda_val)\n",
    "    is_constant = (a_val * lambda_val) < np.abs(beta_hat)\n",
    "    \n",
    "    linear_part = lambda_val * np.abs(beta_hat) * is_linear\n",
    "    quadratic_part = (2 * a_val * lambda_val * np.abs(beta_hat) - beta_hat**2 - lambda_val**2) / (2 * (a_val - 1)) * is_quadratic\n",
    "    constant_part = (lambda_val**2 * (a + 1)) / 2 * is_constant\n",
    "    return linear_part + quadratic_part + constant_part\n",
    "    \n",
    "def scad_derivative(beta_hat, lambda_val, a_val):\n",
    "    return lambda_val * ((beta_hat <= lambda_val) + (a_val * lambda_val - beta_hat)*((a_val * lambda_val - beta_hat) > 0) / ((a_val - 1) * lambda_val) * (beta_hat > lambda_val))\n",
    "\n",
    "#SCAD paper serving as basis for this class: https://andrewcharlesjones.github.io/journal/scad.html\n",
    "class SCAD(nn.Module):\n",
    "    def __init__(self,lam=0.01, alpha=0.01):\n",
    "        self.lam = lam\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        #I'm not going to include a scaler here since, as an nn Module we should assume that scaling happens in an earlier part of the network\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    #Code for initialBetasMag was copied from: https://stackoverflow.com/questions/9171158/how-do-you-get-the-magnitude-of-a-vector-in-numpy\n",
    "    def predict(self, newX):\n",
    "        n = self.X.shape[0]\n",
    "        p = self.X.shape[1]\n",
    "        I = np.eye(p)\n",
    "        initialBetas = np.random.rand(p)\n",
    "        initialBetasMag = np.sqrt(initialBetas.dot(initialBetas))\n",
    "        betas = (np.linalg.inv(self.X.T@self.X + (scad_derivative(initialBetasMag,self.lam,self.alpha)/2*initialBetasMag)*I)@(self.X.T@self.y))\n",
    "        if len(newX.shape) == 1:\n",
    "            ypred = betas@newX.reshape(-1,1)\n",
    "            return ypred, betas\n",
    "        else:\n",
    "            ypreds = np.ndarray((newX.shape[0],),dtype=np.float64)\n",
    "            for i,row in enumerate(newX):\n",
    "                ypred = betas.reshape(1,-1)@row.reshape(-1,1)\n",
    "                ypreds[i] = ypred\n",
    "            return ypreds, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab3a50d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCAD MSE is: 54.512056947421954 and the average of the betas selected was: \n",
      "[ 31.70538639  14.28174821   1.6087059  -13.43878333   6.8976607\n",
      "   1.05353074  -2.39082051  35.93715007]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../content/01intro/concrete.csv')\n",
    "X = data.drop(['strength'],axis=1).values\n",
    "y = data['strength'].values\n",
    "\n",
    "#I tested the different scalers and got the best results with QuantileTransformer. \n",
    "#For some reason StandardScaler was awful in this application. \n",
    "#Tried with different n_quantiles and none gave a noticeably different MSE.\n",
    "scaler = QuantileTransformer()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "SCADmodel=SCAD()\n",
    "\n",
    "SCADMSEs = []\n",
    "nsplits = 10\n",
    "\n",
    "#Running our tts\n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#Creating an array that we'll store our betas in for each iteration of the tts\n",
    "outputBetas = np.ndarray((X_train.shape[1],))\n",
    "\n",
    "kf = KFold(n_splits=nsplits, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[val_index]\n",
    "    SCADmodel.fit(X_train_fold, y_train_fold)\n",
    "    SCADyhats = SCADmodel.predict(X_test_fold)[0]\n",
    "    outputBetas += SCADmodel.predict(X_test_fold)[1]\n",
    "    SCADMSEs.append(mse(y_test_fold,SCADyhats))\n",
    "\n",
    "#To get the average values of the betas, we simply add them for each iteration of the tts, and then divide each element by the number of splits that we ran\n",
    "print(\"SCAD MSE is: \" + str(np.mean(SCADMSEs)) + \" and the average of the betas selected was: \\n\" + str(outputBetas/nsplits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c458af",
   "metadata": {},
   "source": [
    "As we can see, the most important variables here are the features in the 0, 1, 3, and 7 positions. In our data these correspond to the cement content, the slag content, the water content, and the age of the cement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87d464",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "#### (4 points) Based on the simulation design explained in class, generate 500 data sets where the input features have a strong correlation structure (you may consider a 0.9) and apply ElasticNet, SqrtLasso and SCAD to check which method produces the best approximation of an ideal solution, such as a \"betastar\" you design with a sparsity pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "21794dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create other models (copied from class code)\n",
    "class ElasticNet(nn.Module):\n",
    "    def __init__(self, input_size, alpha=1.0, l1_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the ElasticNet regression model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of input features.\n",
    "            alpha (float): Regularization strength. Higher values of alpha\n",
    "                emphasize L1 regularization, while lower values emphasize L2 regularization.\n",
    "            l1_ratio (float): The ratio of L1 regularization to the total\n",
    "                regularization (L1 + L2). It should be between 0 and 1.\n",
    "\n",
    "        \"\"\"\n",
    "        super(ElasticNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "\n",
    "        # Define the linear regression layer\n",
    "        self.linear = nn.Linear(input_size, 1,bias=False,device=device,dtype=dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the ElasticNet model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input data with shape (batch_size, input_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted values with shape (batch_size, 1).\n",
    "\n",
    "        \"\"\"\n",
    "        return self.linear(x)\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the ElasticNet loss function.\n",
    "\n",
    "        Args:\n",
    "            y_pred (Tensor): Predicted values with shape (batch_size, 1).\n",
    "            y_true (Tensor): True target values with shape (batch_size, 1).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The ElasticNet loss.\n",
    "\n",
    "        \"\"\"\n",
    "        mse_loss = nn.MSELoss()(y_pred, y_true)\n",
    "        l1_reg = torch.norm(self.linear.weight, p=1)\n",
    "        l2_reg = torch.norm(self.linear.weight, p=2)\n",
    "\n",
    "        objective = (mse_loss/2) + self.alpha * (\n",
    "            self.l1_ratio * l1_reg + (1 - self.l1_ratio) * ((l2_reg**2)/2)\n",
    "        )\n",
    "\n",
    "        return objective\n",
    "\n",
    "    def fit(self, X, y, num_epochs=100, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Fit the ElasticNet model to the training data.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Input data with shape (num_samples, input_size).\n",
    "            y (Tensor): Target values with shape (num_samples, 1).\n",
    "            num_epochs (int): Number of training epochs.\n",
    "            learning_rate (float): Learning rate for optimization.\n",
    "\n",
    "        \"\"\"\n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = self(X)\n",
    "            loss = self.loss(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for input data.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Input data with shape (num_samples, input_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted values with shape (num_samples, 1).\n",
    "\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(X)\n",
    "        return y_pred\n",
    "    def get_coefficients(self):\n",
    "        \"\"\"\n",
    "        Get the coefficients (weights) of the linear regression layer.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Coefficients with shape (output_size, input_size).\n",
    "\n",
    "        \"\"\"\n",
    "        return self.linear.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "656864cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqrtLasso(nn.Module):\n",
    "    def __init__(self, input_size, alpha=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the  regression model.\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        super(SqrtLasso, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "        # Define the linear regression layer\n",
    "        self.linear = nn.Linear(input_size, 1,bias=False,device=device,dtype=dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input data with shape (batch_size, input_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted values with shape (batch_size, 1).\n",
    "\n",
    "        \"\"\"\n",
    "        return self.linear(x)\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the loss function.\n",
    "\n",
    "        Args:\n",
    "            y_pred (Tensor): Predicted values with shape (batch_size, 1).\n",
    "            y_true (Tensor): True target values with shape (batch_size, 1).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The loss.\n",
    "\n",
    "        \"\"\"\n",
    "        mse_loss = nn.MSELoss(reduction='mean')(y_pred, y_true)\n",
    "        l1_reg = torch.norm(self.linear.weight, p=1,dtype=torch.float64)\n",
    "        # l2_reg = torch.norm(self.linear.weight, p=2,dtype=torch.float64)\n",
    "\n",
    "        loss = torch.sqrt(mse_loss) + self.alpha * (l1_reg)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X, y, num_epochs=200, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Fit the model to the training data.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Input data with shape (num_samples, input_size).\n",
    "            y (Tensor): Target values with shape (num_samples, 1).\n",
    "            num_epochs (int): Number of training epochs.\n",
    "            learning_rate (float): Learning rate for optimization.\n",
    "\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = self(X)\n",
    "            loss = self.loss(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for input data.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Input data with shape (num_samples, input_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted values with shape (num_samples, 1).\n",
    "\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(X)\n",
    "        return y_pred\n",
    "    def get_coefficients(self):\n",
    "        \"\"\"\n",
    "        Get the coefficients (weights) of the linear regression layer.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Coefficients with shape (output_size, input_size).\n",
    "\n",
    "        \"\"\"\n",
    "        return self.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2e2f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our simulated data (copied from class code)\n",
    "def make_correlated_features(num_samples,p,rho):\n",
    "  vcor = [] \n",
    "  for i in range(p):\n",
    "    vcor.append(rho**i)\n",
    "  r = toeplitz(vcor)\n",
    "  mu = np.repeat(0,p)\n",
    "  x = np.random.multivariate_normal(mu, r, size=num_samples)\n",
    "  return x\n",
    "\n",
    "rho =0.9\n",
    "p = 8\n",
    "n = 500\n",
    "vcor = []\n",
    "for i in range(p):\n",
    "  vcor.append(rho**i)\n",
    "\n",
    "x = make_correlated_features(n,p,rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b12e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create betastar (code copied from class)\n",
    "beta =np.array([-1,2,3,0])\n",
    "beta = beta.reshape(-1,1)\n",
    "betastar = np.concatenate([beta,np.repeat(0,p-len(beta)).reshape(-1,1)],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07c12032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate response (copied from class code)\n",
    "y = x@betastar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "651a1edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCAD MSE is: 9.662230800377289\n"
     ]
    }
   ],
   "source": [
    "#run SCAD model\n",
    "scaler = QuantileTransformer(n_quantiles=100)\n",
    "X = scaler.fit_transform(x)\n",
    "\n",
    "SCADmodel=SCAD()\n",
    "\n",
    "SCADMSEs = []\n",
    "nsplits = 10\n",
    "\n",
    "#Running our tts\n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=nsplits, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[val_index]\n",
    "    SCADmodel.fit(X_train_fold, y_train_fold)\n",
    "    SCADyhats = SCADmodel.predict(X_test_fold)[0]\n",
    "    SCADMSEs.append(mse(y_test_fold,SCADyhats))\n",
    "\n",
    "#To get the average values of the betas, we simply add them for each iteration of the tts, and then divide each element by the number of splits that we ran\n",
    "print(\"SCAD MSE is: \" + str(np.mean(SCADMSEs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0bb3df35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/200], Loss: 4.021856479677776\n",
      "Epoch [200/200], Loss: 3.9865258426679753\n",
      "Epoch [100/200], Loss: 3.841953362294562\n",
      "Epoch [200/200], Loss: 3.825837576964542\n",
      "Epoch [100/200], Loss: 3.8747989499669733\n",
      "Epoch [200/200], Loss: 3.874532245499158\n",
      "Epoch [100/200], Loss: 3.9527022544955983\n",
      "Epoch [200/200], Loss: 3.952664956325557\n",
      "Epoch [100/200], Loss: 3.884766987544616\n",
      "Epoch [200/200], Loss: 3.8842494579652924\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "dtype = torch.float64\n",
    "\n",
    "#run SqrtLasso model\n",
    "scaler = QuantileTransformer(n_quantiles=100)\n",
    "X = scaler.fit_transform(x)\n",
    "\n",
    "SqrtLassomodel=SqrtLasso(input_size=X.shape[1])\n",
    "\n",
    "sqrtLassoMSEs = []\n",
    "nsplits = 5\n",
    "\n",
    "#Running our tts\n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "kf = KFold(n_splits=nsplits, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[val_index]\n",
    "    X_train_fold = torch.tensor(X_train_fold)\n",
    "    X_test_fold = torch.tensor(X_test_fold)\n",
    "    y_train_fold = torch.tensor(y_train_fold)\n",
    "    y_test_fold = torch.tensor(y_test_fold)\n",
    "    \n",
    "    SqrtLassomodel.fit(X_train_fold, y_train_fold)\n",
    "    sqrtLassoyhats = SqrtLassomodel.predict(X_test_fold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "88052f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 9.012733220954003\n",
      "Epoch [20/100], Loss: 8.783893695649336\n",
      "Epoch [30/100], Loss: 8.719479072858805\n",
      "Epoch [40/100], Loss: 8.680081270039974\n",
      "Epoch [50/100], Loss: 8.654736323164396\n",
      "Epoch [60/100], Loss: 8.637435238517899\n",
      "Epoch [70/100], Loss: 8.624868695387377\n",
      "Epoch [80/100], Loss: 8.615199924535814\n",
      "Epoch [90/100], Loss: 8.608231585770843\n",
      "Epoch [100/100], Loss: 8.602627067742606\n",
      "Epoch [10/100], Loss: 8.244540035605645\n",
      "Epoch [20/100], Loss: 8.237804680728082\n",
      "Epoch [30/100], Loss: 8.2327682261921\n",
      "Epoch [40/100], Loss: 8.22812517636053\n",
      "Epoch [50/100], Loss: 8.22460001670946\n",
      "Epoch [60/100], Loss: 8.221748231338097\n",
      "Epoch [70/100], Loss: 8.219258253468661\n",
      "Epoch [80/100], Loss: 8.217056358240034\n",
      "Epoch [90/100], Loss: 8.21508760842788\n",
      "Epoch [100/100], Loss: 8.213311157760813\n",
      "Epoch [10/100], Loss: 8.215950765919938\n",
      "Epoch [20/100], Loss: 8.21481553954689\n",
      "Epoch [30/100], Loss: 8.214075360098285\n",
      "Epoch [40/100], Loss: 8.213217496225242\n",
      "Epoch [50/100], Loss: 8.212284461682568\n",
      "Epoch [60/100], Loss: 8.21131129080413\n",
      "Epoch [70/100], Loss: 8.210479593420057\n",
      "Epoch [80/100], Loss: 8.209961245299388\n",
      "Epoch [90/100], Loss: 8.209503605296714\n",
      "Epoch [100/100], Loss: 8.209105702339595\n",
      "Epoch [10/100], Loss: 8.174003959870396\n",
      "Epoch [20/100], Loss: 8.173522325361409\n",
      "Epoch [30/100], Loss: 8.173130991951352\n",
      "Epoch [40/100], Loss: 8.172814721554884\n",
      "Epoch [50/100], Loss: 8.17254856161499\n",
      "Epoch [60/100], Loss: 8.172330569573063\n",
      "Epoch [70/100], Loss: 8.172152352793603\n",
      "Epoch [80/100], Loss: 8.172016005465885\n",
      "Epoch [90/100], Loss: 8.171907663521553\n",
      "Epoch [100/100], Loss: 8.171247651454005\n",
      "Epoch [10/100], Loss: 8.259904886885021\n",
      "Epoch [20/100], Loss: 8.260120697016813\n",
      "Epoch [30/100], Loss: 8.25978804714368\n",
      "Epoch [40/100], Loss: 8.259549972912767\n",
      "Epoch [50/100], Loss: 8.259374769978008\n",
      "Epoch [60/100], Loss: 8.259266552729418\n",
      "Epoch [70/100], Loss: 8.259299570336614\n",
      "Epoch [80/100], Loss: 8.259169367288301\n",
      "Epoch [90/100], Loss: 8.258921289576977\n",
      "Epoch [100/100], Loss: 8.25846474673805\n",
      "Epoch [10/100], Loss: 8.205536551271008\n",
      "Epoch [20/100], Loss: 8.204215546933897\n",
      "Epoch [30/100], Loss: 8.20408073343367\n",
      "Epoch [40/100], Loss: 8.203458890079641\n",
      "Epoch [50/100], Loss: 8.202575159448333\n",
      "Epoch [60/100], Loss: 8.20354109694727\n",
      "Epoch [70/100], Loss: 8.202860622035747\n",
      "Epoch [80/100], Loss: 8.202717007188784\n",
      "Epoch [90/100], Loss: 8.202324263776216\n",
      "Epoch [100/100], Loss: 8.203355506401973\n",
      "Epoch [10/100], Loss: 8.38676180964685\n",
      "Epoch [20/100], Loss: 8.386018700705232\n",
      "Epoch [30/100], Loss: 8.385643163249117\n",
      "Epoch [40/100], Loss: 8.385083790793546\n",
      "Epoch [50/100], Loss: 8.384552648100128\n",
      "Epoch [60/100], Loss: 8.384507065320662\n",
      "Epoch [70/100], Loss: 8.384501670092972\n",
      "Epoch [80/100], Loss: 8.384515845865446\n",
      "Epoch [90/100], Loss: 8.384547905341812\n",
      "Epoch [100/100], Loss: 8.384587113585846\n",
      "Epoch [10/100], Loss: 8.376251111012971\n",
      "Epoch [20/100], Loss: 8.375965907181998\n",
      "Epoch [30/100], Loss: 8.375562832051166\n",
      "Epoch [40/100], Loss: 8.375994026488165\n",
      "Epoch [50/100], Loss: 8.3763582627198\n",
      "Epoch [60/100], Loss: 8.376009127929368\n",
      "Epoch [70/100], Loss: 8.3760171451406\n",
      "Epoch [80/100], Loss: 8.375669542527342\n",
      "Epoch [90/100], Loss: 8.375367851890308\n",
      "Epoch [100/100], Loss: 8.376469221270769\n",
      "Epoch [10/100], Loss: 8.235828971580233\n",
      "Epoch [20/100], Loss: 8.23436303985099\n",
      "Epoch [30/100], Loss: 8.233960874533906\n",
      "Epoch [40/100], Loss: 8.233748760767309\n",
      "Epoch [50/100], Loss: 8.23366336350557\n",
      "Epoch [60/100], Loss: 8.233661962807604\n",
      "Epoch [70/100], Loss: 8.232963124383106\n",
      "Epoch [80/100], Loss: 8.23303549608129\n",
      "Epoch [90/100], Loss: 8.23315189663491\n",
      "Epoch [100/100], Loss: 8.232581123534612\n",
      "Epoch [10/100], Loss: 8.259454448730958\n",
      "Epoch [20/100], Loss: 8.258470272775302\n",
      "Epoch [30/100], Loss: 8.257905945859559\n",
      "Epoch [40/100], Loss: 8.257394783271574\n",
      "Epoch [50/100], Loss: 8.256492329367916\n",
      "Epoch [60/100], Loss: 8.256200571559999\n",
      "Epoch [70/100], Loss: 8.25605602600033\n",
      "Epoch [80/100], Loss: 8.255932813556953\n",
      "Epoch [90/100], Loss: 8.255822266154084\n",
      "Epoch [100/100], Loss: 8.255926683719837\n"
     ]
    }
   ],
   "source": [
    "#Run ElasticNet model\n",
    "\n",
    "#run SCAD model\n",
    "scaler = QuantileTransformer(n_quantiles=100)\n",
    "X = scaler.fit_transform(x)\n",
    "\n",
    "ENmodel=ElasticNet(input_size=X.shape[1])\n",
    "\n",
    "eNMSEs = []\n",
    "nsplits = 10\n",
    "\n",
    "#Running our tts\n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=nsplits, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[val_index]\n",
    "    X_train_fold = torch.tensor(X_train_fold)\n",
    "    X_test_fold = torch.tensor(X_test_fold)\n",
    "    y_train_fold = torch.tensor(y_train_fold)\n",
    "    y_test_fold = torch.tensor(y_test_fold)\n",
    "    ENmodel.fit(X_train_fold, y_train_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a23794",
   "metadata": {},
   "source": [
    "As we can see, the model with the lowest MSE is SqrtLasso (which is the loss function for this model). ElasticNet does better than this, which does better than the SCAD model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
